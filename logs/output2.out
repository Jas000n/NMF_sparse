nohup: ignoring input
in the movie-100k datasets, there are 943 users and 1682 items !
[[5 3 0 ... 0 0 0]
 [4 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
type2= [0, 4, 5, 6, 9, 10, 12, 15, 17, 21, 22, 41, 42, 43, 55, 57, 58, 59, 61, 63, 69, 71, 81, 82, 84, 86, 89, 90, 91, 93, 94, 101, 108, 118, 124, 127, 129, 143, 144, 150, 157, 173, 176, 177, 183, 187, 188, 193, 197, 199, 200, 206, 209, 212, 213, 214, 215, 220, 221, 229, 232, 233, 235, 238, 243, 245, 248, 249, 252, 253, 255, 261, 262, 263, 266, 267, 268, 269, 270, 275, 278, 279, 285, 289, 290, 291, 292, 294, 295, 296, 297, 298, 300, 302, 304, 306, 307, 310, 311, 312, 313, 317, 319, 320, 324, 325, 326, 327, 329, 331, 333, 338, 341, 342, 343, 344, 345, 346, 353, 360, 362, 372, 373, 377, 378, 379, 384, 386, 388, 390, 392, 393, 397, 398, 400, 404, 405, 406, 408, 415, 416, 424, 428, 434, 436, 441, 446, 449, 451, 452, 453, 454, 455, 456, 457, 467, 471, 473, 478, 483, 486, 487, 492, 494, 496, 497, 499, 502, 503, 505, 507, 513, 520, 523, 526, 531, 532, 534, 535, 536, 541, 542, 544, 550, 553, 560, 565, 566, 576, 585, 587, 591, 592, 600, 605, 607, 617, 620, 621, 626, 628, 631, 641, 642, 644, 647, 649, 652, 653, 654, 658, 659, 660, 663, 664, 665, 681, 692, 693, 708, 710, 711, 714, 715, 726, 737, 746, 748, 750, 756, 757, 765, 785, 787, 789, 795, 803, 804, 806, 814, 822, 832, 839, 845, 863, 879, 880, 888, 895]
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	3.000000000000002
  (0, 1680)	3.000000000000002
  (0, 1679)	2.999699639999998
  (0, 1678)	3.000000000000002
  (0, 1677)	2.9993992799999982
  (0, 1676)	3.000000000000002
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	3.000000000000002
  (0, 1671)	2.999699700000002
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9997012900000004
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9997012900000004
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9993998199999985
  (0, 1662)	2.9997012900000004
  (0, 1661)	2.9996984699999967
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000032100000014
  :	:
  (942, 24)	3.033440810882999
  (942, 23)	3.015690172884004
  (942, 22)	3.030190000794003
  (942, 21)	3.0398424218730016
  (942, 20)	3.0051247016160016
  (942, 19)	3.010854740678999
  (942, 18)	3.0186904485240027
  (942, 17)	3.0072314535299975
  (942, 16)	3.010813130358003
  (942, 15)	3.0102521281559995
  (942, 14)	3.0419146038239986
  (942, 13)	3.0341051017560057
  (942, 12)	3.0156775664309983
  (942, 11)	3.041956785798
  (942, 10)	3.0235077382679996
  (942, 9)	3.0150669808530033
  (942, 8)	3.0379737784770033
  (942, 7)	3.027152507535002
  (942, 6)	3.0449392499340036
  (942, 5)	3.011755725981002
  (942, 4)	3.0105208652400015
  (942, 3)	3.012948314486998
  (942, 2)	3.006007995762004
  (942, 1)	3.0075020860949992
  (942, 0)	3.0564314910630035
this is the 1 epoch
rmse loss on training set is 1.2533368236305205
rmse loss on test set is 1.2395623053537288
for this epoch using 109.98483085632324 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9999961889319042
  (0, 1680)	3.000000000000002
  (0, 1679)	2.999397943151449
  (0, 1678)	2.999998917462941
  (0, 1677)	2.9987969688399567
  (0, 1676)	2.999996828702364
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999993427810988
  (0, 1671)	2.9993983090659464
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.99940775192688
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.99940775192688
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9987993933878894
  (0, 1662)	2.99940775192688
  (0, 1661)	2.999399121246299
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000007230859804
  :	:
  (942, 24)	3.0657499419507444
  (942, 23)	3.0310578348812904
  (942, 22)	3.05979203243858
  (942, 21)	3.078647044715538
  (942, 20)	3.010048074882275
  (942, 19)	3.021527089965081
  (942, 18)	3.037151455096156
  (942, 17)	3.014306959327628
  (942, 16)	3.021346851072798
  (942, 15)	3.020327886326624
  (942, 14)	3.082183155559613
  (942, 13)	3.0673842909662974
  (942, 12)	3.030931714619249
  (942, 11)	3.0829233586919287
  (942, 10)	3.0464681073364432
  (942, 9)	3.029905179193237
  (942, 8)	3.074773518287575
  (942, 7)	3.0536369249933655
  (942, 6)	3.08773399878608
  (942, 5)	3.02335417813729
  (942, 4)	3.0207830737321624
  (942, 3)	3.0255352515956417
  (942, 2)	3.0117972829106114
  (942, 1)	3.014751971741797
  (942, 0)	3.109741408712761
this is the 2 epoch
rmse loss on training set is 1.2349653293736425
rmse loss on test set is 1.2242213754720845
for this epoch using 111.86322498321533 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9999887037131208
  (0, 1680)	3.000000000000002
  (0, 1679)	2.99909496876644
  (0, 1678)	2.9999967962801137
  (0, 1677)	2.9981931412527603
  (0, 1676)	2.999990685050806
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9999803558998566
  (0, 1671)	2.9990959076984254
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9991191793834844
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9991191793834844
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9981987781541024
  (0, 1662)	2.9991191793834844
  (0, 1661)	2.9991018587482783
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000120598310693
  :	:
  (942, 24)	3.09694307809046
  (942, 23)	3.0460928457008967
  (942, 22)	3.0888031576187847
  (942, 21)	3.116418105533898
  (942, 20)	3.014761324672162
  (942, 19)	3.032006063948928
  (942, 18)	3.0553695659774487
  (942, 17)	3.021215524791481
  (942, 16)	3.0316042919250763
  (942, 15)	3.0302162770608527
  (942, 14)	3.1208388383481114
  (942, 13)	3.0998310827579147
  (942, 12)	3.0457573579487196
  (942, 11)	3.1229003689655648
  (942, 10)	3.068883394656609
  (942, 9)	3.044504282756615
  (942, 8)	3.110411168453807
  (942, 7)	3.0794551849813905
  (942, 6)	3.1284543374877125
  (942, 5)	3.0347838621664955
  (942, 4)	3.0307892208469687
  (942, 3)	3.0377647968771835
  (942, 2)	3.0173613679921996
  (942, 1)	3.0217534050120007
  (942, 0)	3.1600546960222244
this is the 3 epoch
rmse loss on training set is 1.2181589407804805
rmse loss on test set is 1.2099978116264427
for this epoch using 110.79682946205139 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999977681182832
  (0, 1680)	3.000000000000002
  (0, 1679)	2.998790775073673
  (0, 1678)	2.999993679661282
  (0, 1677)	2.997587870486068
  (0, 1676)	2.9999817646484113
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999960862253343
  (0, 1671)	2.998792574352161
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9988353743726455
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9988353743726455
  (0, 1664)	3.000000000000002
  (0, 1663)	2.997598030917949
  (0, 1662)	2.9988353743726455
  (0, 1661)	2.9988065931934256
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000017701989033
  :	:
  (942, 24)	3.1270390128599623
  (942, 23)	3.0607885572753246
  (942, 22)	3.117221966878471
  (942, 21)	3.1531621462496275
  (942, 20)	3.0192589428954104
  (942, 19)	3.0422839131275596
  (942, 18)	3.073334641060988
  (942, 17)	3.0279493174568755
  (942, 16)	3.0415891851565804
  (942, 15)	3.03990952482387
  (942, 14)	3.157918393502018
  (942, 13)	3.1314432173196125
  (942, 12)	3.060152825713508
  (942, 11)	3.161890711982294
  (942, 10)	3.090757078744312
  (942, 9)	3.0588570687391345
  (942, 8)	3.1449020581780065
  (942, 7)	3.104610744754164
  (942, 6)	3.1671724313489653
  (942, 5)	3.0460364535305002
  (942, 4)	3.0405424756179866
  (942, 3)	3.0496417331398846
  (942, 2)	3.022696804147575
  (942, 1)	3.0285106618538986
  (942, 0)	3.207497154562477
this is the 4 epoch
rmse loss on training set is 1.2027505607059634
rmse loss on test set is 1.196801730459254
for this epoch using 112.4816586971283 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99996325750756
  (0, 1680)	3.000000000000002
  (0, 1679)	2.99848541901111
  (0, 1678)	2.999989609963662
  (0, 1677)	2.9969812280585577
  (0, 1676)	2.999970258924585
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999935029732391
  (0, 1671)	2.9984883852773203
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.998556146790528
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.998556146790528
  (0, 1664)	3.000000000000002
  (0, 1663)	2.996997206614283
  (0, 1662)	2.998556146790528
  (0, 1661)	2.998513240480999
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000241686853673
  :	:
  (942, 24)	3.1560590331725145
  (942, 23)	3.0751411618154494
  (942, 22)	3.145048456842697
  (942, 21)	3.1888878137384506
  (942, 20)	3.023538051466135
  (942, 19)	3.0523554967822215
  (942, 18)	3.0910392528788795
  (942, 17)	3.034503048168502
  (942, 16)	3.0513057744921026
  (942, 15)	3.049402454009353
  (942, 14)	3.193461386515212
  (942, 13)	3.1622219955417914
  (942, 12)	3.074119201815482
  (942, 11)	3.199899351260821
  (942, 10)	3.112093704839131
  (942, 9)	3.0729588074782
  (942, 8)	3.178264285146631
  (942, 7)	3.1291084433233762
  (942, 6)	3.203961761481568
  (942, 5)	3.0571061873800214
  (942, 4)	3.050046494966941
  (942, 3)	3.061171529903678
  (942, 2)	3.0278025577436583
  (942, 1)	3.035028457659352
  (942, 0)	3.252194552027498
this is the 5 epoch
rmse loss on training set is 1.1885945753635596
rmse loss on test set is 1.1845509529302833
for this epoch using 109.26979517936707 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999945567660228
  (0, 1680)	3.000000000000002
  (0, 1679)	2.998178956064514
  (0, 1678)	2.999984628556818
  (0, 1677)	2.996373283572214
  (0, 1676)	2.999956354433731
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9999029454703225
  (0, 1671)	2.9981834143960886
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.998281313992403
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.998281313992403
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9963963582547994
  (0, 1662)	2.998281313992403
  (0, 1661)	2.9982217213159172
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000314762162525
  :	:
  (942, 24)	3.1840264031205003
  (942, 23)	3.0891491604477626
  (942, 22)	3.1722839120261974
  (942, 21)	3.2236056633572434
  (942, 20)	3.0275978720712153
  (942, 19)	3.0622177774412447
  (942, 18)	3.1084781982242027
  (942, 17)	3.0408734741111725
  (942, 16)	3.060758732782294
  (942, 15)	3.0586919862969113
  (942, 14)	3.2275095751627623
  (942, 13)	3.1921717179759876
  (942, 12)	3.087659784883784
  (942, 11)	3.236933143558013
  (942, 10)	3.1328987672631867
  (942, 9)	3.0868067809224953
  (942, 8)	3.2105181987254876
  (942, 7)	3.1529543642349345
  (942, 6)	3.238896474014459
  (942, 5)	3.067989365830132
  (942, 4)	3.0593053461684288
  (942, 3)	3.0723602418867015
  (942, 2)	3.032679498963285
  (942, 1)	3.041311867796213
  (942, 0)	3.2942718442158663
this is the 6 epoch
rmse loss on training set is 1.1755636802954135
rmse loss on test set is 1.173170275558814
for this epoch using 108.9389591217041 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9999247449675748
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9978714401450284
  (0, 1678)	2.999978775718006
  (0, 1677)	2.9957641045720513
  (0, 1676)	2.9999402322754403
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999864700314185
  (0, 1671)	2.9978777332101876
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.998010700416187
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.998010700416187
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9957955367525977
  (0, 1662)	2.998010700416187
  (0, 1661)	2.9979319608521475
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000039644678482
  :	:
  (942, 24)	3.210965929802236
  (942, 23)	3.1028129288692403
  (942, 22)	3.198930790877208
  (942, 21)	3.257327967695839
  (942, 20)	3.031439297306358
  (942, 19)	3.071869411361994
  (942, 18)	3.1256481000281724
  (942, 17)	3.047058997424228
  (942, 16)	3.069953087730502
  (942, 15)	3.067776733415611
  (942, 14)	3.260106365257569
  (942, 13)	3.221299208884498
  (942, 12)	3.100779646299399
  (942, 11)	3.273000667614913
  (942, 10)	3.1531785987134087
  (942, 9)	3.1003998928569825
  (942, 8)	3.241685961935768
  (942, 7)	3.1761557046014572
  (942, 6)	3.272050824244153
  (942, 5)	3.0786839590045694
  (942, 4)	3.06832343701753
  (942, 3)	3.0832144161695236
  (942, 2)	3.037329991405891
  (942, 1)	3.0473662564856268
  (942, 0)	3.3338525037785955
this is the 7 epoch
rmse loss on training set is 1.1635462124647542
rmse loss on test set is 1.1625908292877318
for this epoch using 115.14791083335876 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9999009207208
  (0, 1680)	3.000000000000002
  (0, 1679)	2.997562923499732
  (0, 1678)	2.999972090553626
  (0, 1677)	2.9951537564458492
  (0, 1676)	2.9999220676259433
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9998203883078833
  (0, 1671)	2.997571410735614
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.997744137251757
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.997744137251757
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9951947907998657
  (0, 1662)	2.997744137251757
  (0, 1661)	2.9976438883772665
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000486969899094
  :	:
  (942, 24)	3.236903597259744
  (942, 23)	3.1161343613023518
  (942, 22)	3.224992616566208
  (942, 21)	3.2900685319573633
  (942, 20)	3.0350645430688035
  (942, 19)	3.081310415991682
  (942, 18)	3.142547081773099
  (942, 17)	3.053059340235335
  (942, 16)	3.0788941551713505
  (942, 15)	3.0766566663812607
  (942, 14)	3.291296341962922
  (942, 13)	3.2496134107251775
  (942, 12)	3.113485267419525
  (942, 11)	3.308112057921673
  (942, 10)	3.172940266607061
  (942, 9)	3.113738352647943
  (942, 8)	3.2717911791190497
  (942, 7)	3.1987206508193418
  (942, 6)	3.303498704379804
  (942, 5)	3.0891892812004844
  (942, 4)	3.0771054531735067
  (942, 3)	3.093741007649668
  (942, 2)	3.0417575609655194
  (942, 1)	3.053197213418324
  (942, 0)	3.3710579476564435
this is the 8 epoch
rmse loss on training set is 1.152443906314508
rmse loss on test set is 1.1527495106325765
for this epoch using 118.2461633682251 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9998742238441953
  (0, 1680)	3.000000000000002
  (0, 1679)	2.997253456650088
  (0, 1678)	2.9999646109426297
  (0, 1677)	2.9945423023575386
  (0, 1676)	2.9999020293704395
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999770106215688
  (0, 1671)	2.997264513461049
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9974814621467725
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9974814621467725
  (0, 1664)	3.000000000000002
  (0, 1663)	2.994594166790045
  (0, 1662)	2.9974814621467725
  (0, 1661)	2.997357437031149
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000058658053423
  :	:
  (942, 24)	3.2618662572275356
  (942, 23)	3.1291165778391794
  (942, 22)	3.250473872900945
  (942, 21)	3.321842517067058
  (942, 20)	3.038476866260614
  (942, 19)	3.0905418993038825
  (942, 18)	3.1591745003637812
  (942, 17)	3.0588752809460007
  (942, 16)	3.087587479376164
  (942, 15)	3.0853328461952527
  (942, 14)	3.3211248664646758
  (942, 13)	3.2771250372448057
  (942, 12)	3.125784241079516
  (942, 11)	3.3422788445816463
  (942, 10)	3.192191476499635
  (942, 9)	3.1268234180366714
  (942, 8)	3.300858578617688
  (942, 7)	3.220658261259524
  (942, 6)	3.3333132448694123
  (942, 5)	3.0995057273894826
  (942, 4)	3.0856563021639523
  (942, 3)	3.1039473023721498
  (942, 2)	3.045966628336227
  (942, 1)	3.0588104975201476
  (942, 0)	3.4060070549697223
this is the 9 epoch
rmse loss on training set is 1.1421700047307335
rmse loss on test set is 1.1435884732838029
for this epoch using 119.62567663192749 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999844780616699
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9969430883536115
  (0, 1678)	2.999956373498576
  (0, 1677)	2.9939298032086508
  (0, 1676)	2.999880279826293
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999713953084145
  (0, 1671)	2.9969571053269544
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9972225189414554
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9972225189414554
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9939937087767783
  (0, 1662)	2.9972225189414554
  (0, 1661)	2.99707254355339
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0000695540464735
  :	:
  (942, 24)	3.2858813674717653
  (942, 23)	3.141763683263688
  (942, 22)	3.2753799056334914
  (942, 21)	3.352666271337679
  (942, 20)	3.0416803351268396
  (942, 19)	3.099565839002155
  (942, 18)	3.175530726247389
  (942, 17)	3.064508439737053
  (942, 16)	3.096038779870065
  (942, 15)	3.093807204073189
  (942, 14)	3.349637729369624
  (942, 13)	3.3038462756009475
  (942, 12)	3.1376850254344317
  (942, 11)	3.3755138001076306
  (942, 10)	3.2109404825076955
  (942, 9)	3.139657185474116
  (942, 8)	3.3289137417409136
  (942, 7)	3.241978356096304
  (942, 6)	3.361566480454687
  (942, 5)	3.109634558306929
  (942, 4)	3.0939810635471314
  (942, 3)	3.113840848302092
  (942, 2)	3.049962292704506
  (942, 1)	3.064211987296182
  (942, 0)	3.438815767195314
this is the 10 epoch
rmse loss on training set is 1.1326476668404528
rmse loss on test set is 1.1350546709701423
for this epoch using 116.90054988861084 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999812714441458
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9966318655850905
  (0, 1678)	2.9999474135471664
  (0, 1677)	2.9933163176230075
  (0, 1676)	2.9998569745473587
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9996520298405365
  (0, 1671)	2.9966492477223534
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9969671574265218
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9969671574265218
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9933934584631285
  (0, 1662)	2.9969671574265218
  (0, 1661)	2.9967891480548996
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000081411820095
  :	:
  (942, 24)	3.308976770170584
  (942, 23)	3.1540805678133457
  (942, 22)	3.299716829322846
  (942, 21)	3.3825571712792675
  (942, 20)	3.044679642129207
  (942, 19)	3.1083849020216543
  (942, 18)	3.191616961836585
  (942, 17)	3.0699611037299444
  (942, 16)	3.1042539042591684
  (942, 15)	3.1020823616983555
  (942, 14)	3.376880853485547
  (942, 13)	3.329790529722288
  (942, 12)	3.14919674056487
  (942, 11)	3.407830793763162
  (942, 10)	3.229196004596389
  (942, 9)	3.152242418826989
  (942, 8)	3.3559828708537927
  (942, 7)	3.262691414330882
  (942, 6)	3.3883290731209006
  (942, 5)	3.1195777247747345
  (942, 4)	3.1020849447461103
  (942, 3)	3.123429393092381
  (942, 2)	3.0537501567178422
  (942, 1)	3.0694076372043573
  (942, 0)	3.469596762697019
this is the 11 epoch
rmse loss on training set is 1.1238086238750935
rmse loss on test set is 1.1270994443788753
for this epoch using 113.24966025352478 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9997781456586887
  (0, 1680)	3.000000000000002
  (0, 1679)	2.996319833533678
  (0, 1678)	2.9999377651167785
  (0, 1677)	2.992701901950578
  (0, 1676)	2.9998322622002638
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9995844389256567
  (0, 1671)	2.9963409994967796
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.996715233119647
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.996715233119647
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9927934552153697
  (0, 1662)	2.996715233119647
  (0, 1661)	2.9965071938100665
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000094258393678
  :	:
  (942, 24)	3.3311805041330804
  (942, 23)	3.1660727422211976
  (942, 22)	3.3234914398281075
  (942, 21)	3.4115334719316386
  (942, 20)	3.0474799512896893
  (942, 19)	3.1170022966870783
  (942, 18)	3.2074350910826994
  (942, 17)	3.075236084185607
  (942, 16)	3.1122387865825605
  (942, 15)	3.1101614839120324
  (942, 14)	3.402900039704087
  (942, 13)	3.3549721985472565
  (942, 12)	3.1603290001452624
  (942, 11)	3.439244653869517
  (942, 10)	3.246967152532403
  (942, 9)	3.1645824091358254
  (942, 8)	3.3820925906901937
  (942, 7)	3.2828084779738473
  (942, 6)	3.413670085001662
  (942, 5)	3.1293377237921205
  (942, 4)	3.1099732420863275
  (942, 3)	3.1327208283934653
  (942, 2)	3.0573361848168963
  (942, 1)	3.0744034395334188
  (942, 0)	3.4984591980147313
this is the 12 epoch
rmse loss on training set is 1.1155920422242216
rmse loss on test set is 1.1196781464251604
for this epoch using 113.6136155128479 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9997411913972964
  (0, 1680)	3.000000000000002
  (0, 1679)	2.996007035613341
  (0, 1678)	2.9999274609397104
  (0, 1677)	2.9920866102869743
  (0, 1676)	2.99980628450387
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999511283958967
  (0, 1671)	2.9960324169852446
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9964666070568375
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9964666070568375
  (0, 1664)	3.000000000000002
  (0, 1663)	2.992193736096536
  (0, 1662)	2.9964666070568375
  (0, 1661)	2.996226627066577
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0001081205330324
  :	:
  (942, 24)	3.3525206457407144
  (942, 23)	3.177746200872899
  (942, 22)	3.346711132431307
  (942, 21)	3.4396141669184925
  (942, 20)	3.0500867735471666
  (942, 19)	3.1254216514112074
  (942, 18)	3.222987554466528
  (942, 17)	3.080336599659759
  (942, 16)	3.1199994107229454
  (942, 15)	3.118048157769232
  (942, 14)	3.4277407505921578
  (942, 13)	3.3794064839251177
  (942, 12)	3.1710917719627734
  (942, 11)	3.469771038332881
  (942, 10)	3.264263356255986
  (942, 9)	3.1766808595711464
  (942, 8)	3.407269778013156
  (942, 7)	3.3023410632742225
  (942, 6)	3.437656795083927
  (942, 5)	3.1389174804247184
  (942, 4)	3.1176513065888822
  (942, 3)	3.141723140252451
  (942, 2)	3.060726588600675
  (942, 1)	3.079205391286004
  (942, 0)	3.525508508726572
this is the 13 epoch
rmse loss on training set is 1.1079435595020781
rmse loss on test set is 1.1127498012885788
for this epoch using 111.95750951766968 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999701965461125
  (0, 1680)	3.000000000000002
  (0, 1679)	2.995693513483875
  (0, 1678)	2.999916532462194
  (0, 1677)	2.9914704945055495
  (0, 1676)	2.9997791762237886
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999432669433743
  (0, 1671)	2.9957235540439093
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.996221145595894
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.996221145595894
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9915943359155355
  (0, 1662)	2.996221145595894
  (0, 1661)	2.995947396870406
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000123024401156
  :	:
  (942, 24)	3.3730251743729682
  (942, 23)	3.1891073081018444
  (942, 22)	3.3693838255261683
  (942, 21)	3.4668188582697392
  (942, 20)	3.052505864945869
  (942, 19)	3.1336469150280206
  (942, 18)	3.2382772448024326
  (942, 17)	3.085266180249796
  (942, 16)	3.127541778431287
  (942, 15)	3.1257462930907716
  (942, 14)	3.451447927035329
  (942, 13)	3.4031092238816
  (942, 12)	3.1814952622617185
  (942, 11)	3.4994263135046673
  (942, 10)	3.2810943023902954
  (942, 9)	3.188541790896182
  (942, 8)	3.4315414155735864
  (942, 7)	3.32130107881965
  (942, 6)	3.4603545542502063
  (942, 5)	3.1483202507069996
  (942, 4)	3.125124514093909
  (942, 3)	3.150444365154126
  (942, 2)	3.063927734148055
  (942, 1)	3.083819465595416
  (942, 0)	3.550846263147565
this is the 14 epoch
rmse loss on training set is 1.1008144651168632
rmse loss on test set is 1.1062767934827484
for this epoch using 111.76104426383972 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9996605782459445
  (0, 1680)	3.000000000000002
  (0, 1679)	2.995379307080557
  (0, 1678)	2.9999050098615436
  (0, 1677)	2.990853604299561
  (0, 1676)	2.9997510652144785
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9993487004403168
  (0, 1671)	2.99541446209491
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9959787202298376
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9959787202298376
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9909952872883614
  (0, 1662)	2.9959787202298376
  (0, 1661)	2.9956694549041556
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000138995272154
  :	:
  (942, 24)	3.3927218587901002
  (942, 23)	3.2001627035926465
  (942, 22)	3.3915178897566216
  (942, 21)	3.4931676359321244
  (942, 20)	3.0547431434864003
  (942, 19)	3.14168227481546
  (942, 18)	3.2533074201481766
  (942, 17)	3.0900285890324075
  (942, 16)	3.1348718815438725
  (942, 15)	3.1332600405967743
  (942, 14)	3.4740658338931536
  (942, 13)	3.4260967476889945
  (942, 12)	3.191549819834921
  (942, 11)	3.5282274413694483
  (942, 10)	3.2974698765804007
  (942, 9)	3.200169463668605
  (942, 8)	3.454934466988463
  (942, 7)	3.3397007502826277
  (942, 6)	3.481826673803718
  (942, 5)	3.1575495417139883
  (942, 4)	3.132398239311669
  (942, 3)	3.1588925512665718
  (942, 2)	3.066946067215455
  (942, 1)	3.0882515872330205
  (942, 0)	3.5745700625986236
this is the 15 epoch
rmse loss on training set is 1.0941610016119345
rmse loss on test set is 1.1002245838635687
for this epoch using 109.79887366294861 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999617136683509
  (0, 1680)	3.000000000000002
  (0, 1679)	2.995064454650602
  (0, 1678)	2.999892922069008
  (0, 1677)	2.9902359872321886
  (0, 1676)	2.999722072501922
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999259482415277
  (0, 1671)	2.9951051901785615
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9957392074086524
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9957392074086524
  (0, 1664)	3.000000000000002
  (0, 1663)	2.990396620708412
  (0, 1662)	2.9957392074086524
  (0, 1661)	2.995392755337061
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0001560572999737
  :	:
  (942, 24)	3.4116381615221254
  (942, 23)	3.2109192236206954
  (942, 22)	3.413122082447689
  (942, 21)	3.5186809667847356
  (942, 20)	3.056804621275659
  (942, 19)	3.1495320890269967
  (942, 18)	3.268081630827285
  (942, 17)	3.094627757555803
  (942, 16)	3.1419956779946534
  (942, 15)	3.1405937244654627
  (942, 14)	3.4956379311458057
  (942, 13)	3.448385749774994
  (942, 12)	3.201265856540718
  (942, 11)	3.5561918749568595
  (942, 10)	3.313400111337827
  (942, 9)	3.2115683141480886
  (942, 8)	3.4774757697052787
  (942, 7)	3.3575525515488818
  (942, 6)	3.5021343431613565
  (942, 5)	3.166609045705537
  (942, 4)	3.139477833423737
  (942, 3)	3.167075724467971
  (942, 2)	3.069788053022406
  (942, 1)	3.0925076117924726
  (942, 0)	3.596773482455545
this is the 16 epoch
rmse loss on training set is 1.0879437670614969
rmse loss on test set is 1.0945614499662575
for this epoch using 108.29244494438171 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9995717442093786
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9947489927948614
  (0, 1678)	2.9998802967971177
  (0, 1677)	2.989617688792616
  (0, 1676)	2.9996923124006996
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9991651209146712
  (0, 1671)	2.994795785011511
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9955024883679573
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9955024883679573
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9897983646234363
  (0, 1662)	2.9955024883679573
  (0, 1661)	2.9951172546852805
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0001742333346666
  :	:
  (942, 24)	3.42980115878287
  (942, 23)	3.2213838354607134
  (942, 22)	3.434205487139013
  (942, 21)	3.543379592893086
  (942, 20)	3.058696349253391
  (942, 19)	3.1572008313603077
  (942, 18)	3.2826036581396547
  (942, 17)	3.0990677328584573
  (942, 16)	3.1489190712507833
  (942, 15)	3.1477517867657405
  (942, 14)	3.5162067674496087
  (942, 13)	3.469993179984752
  (942, 12)	3.21065378153061
  (942, 11)	3.583337461794755
  (942, 10)	3.328895139056348
  (942, 9)	3.222742901461784
  (942, 8)	3.499191943662711
  (942, 7)	3.3748691419358785
  (942, 6)	3.521336572875966
  (942, 5)	3.1755025858421004
  (942, 4)	3.146368604881065
  (942, 3)	3.1750018587480544
  (942, 2)	3.072460127968653
  (942, 1)	3.0965933081663195
  (942, 0)	3.617546048658825
this is the 17 epoch
rmse loss on training set is 1.0821272021597794
rmse loss on test set is 1.0892582484316233
for this epoch using 112.33279633522034 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9995245007515163
  (0, 1680)	3.000000000000002
  (0, 1679)	2.994432956513554
  (0, 1678)	2.9998671605705467
  (0, 1677)	2.988998752456557
  (0, 1676)	2.9996618926595944
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.999065721409355
  (0, 1671)	2.9944862910496735
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9952684489638575
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9952684489638575
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9892005455170794
  (0, 1662)	2.9952684489638575
  (0, 1661)	2.9948429116815483
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000193544779695
  :	:
  (942, 24)	3.44723747381041
  (942, 23)	3.231563582783537
  (942, 22)	3.4547774580065544
  (942, 21)	3.5672844386706943
  (942, 20)	3.0604243722856843
  (942, 19)	3.164693045277201
  (942, 18)	3.296877462791083
  (942, 17)	3.1033526339703177
  (942, 16)	3.155647892824631
  (942, 15)	3.1547387416938504
  (942, 14)	3.535813893392186
  (942, 13)	3.490936148100276
  (942, 12)	3.2197239469596624
  (942, 11)	3.6096823551580406
  (942, 10)	3.343965149862151
  (942, 9)	3.233697864046187
  (942, 8)	3.520109313620255
  (942, 7)	3.3916633091896804
  (942, 6)	3.5394901595731274
  (942, 5)	3.184234071445241
  (942, 4)	3.153075803070898
  (942, 3)	3.182678850596571
  (942, 2)	3.0749686611302467
  (942, 1)	3.1005143439592886
  (942, 0)	3.636973244824276
this is the 18 epoch
rmse loss on training set is 1.0766791484387659
rmse loss on test set is 1.0842881975679055
for this epoch using 110.87616777420044 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99947550273687
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9941163792548404
  (0, 1678)	2.9998535387595986
  (0, 1677)	2.988379219750082
  (0, 1676)	2.999630914630741
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998961389100689
  (0, 1671)	2.9941767505548063
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.995036979513051
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.995036979513051
  (0, 1664)	3.000000000000002
  (0, 1663)	2.988603187993225
  (0, 1662)	2.995036979513051
  (0, 1661)	2.9945696871530965
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0002140114846703
  :	:
  (942, 24)	3.463973221849142
  (942, 23)	3.2414655402506485
  (942, 22)	3.4748475689414366
  (942, 21)	3.590416526568441
  (942, 20)	3.0619946928253245
  (942, 19)	3.1720133064777394
  (942, 18)	3.310907141435997
  (942, 17)	3.107486616238543
  (942, 16)	3.1621878875408727
  (942, 15)	3.161559137930083
  (942, 14)	3.5544997920572863
  (942, 13)	3.5112318408377967
  (942, 12)	3.2284866033435606
  (942, 11)	3.635244932819299
  (942, 10)	3.358620353962992
  (942, 9)	3.2444378837569214
  (942, 8)	3.5402538434268154
  (942, 7)	3.407947917936222
  (942, 6)	3.556649669765006
  (942, 5)	3.192807461155624
  (942, 4)	3.159604604548894
  (942, 3)	3.190114497012157
  (942, 2)	3.077319923785504
  (942, 1)	3.104276273511109
  (942, 0)	3.655136545536092
this is the 19 epoch
rmse loss on training set is 1.0715704663705379
rmse loss on test set is 1.0796266783190094
for this epoch using 108.68602347373962 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9994248431134625
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9937992929654804
  (0, 1678)	2.9998394556155215
  (0, 1677)	2.9877591303154367
  (0, 1676)	2.999599473457663
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998852228754971
  (0, 1671)	2.9938672036637475
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9948079746378315
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9948079746378315
  (0, 1664)	3.000000000000002
  (0, 1663)	2.988006314861826
  (0, 1662)	2.9948079746378315
  (0, 1661)	2.994297543907193
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000235651668469
  :	:
  (942, 24)	3.4800339652469807
  (942, 23)	3.251096775831131
  (942, 22)	3.494425567042931
  (942, 21)	3.612796900876886
  (942, 20)	3.0634132416679676
  (942, 19)	3.179166192142822
  (942, 18)	3.3246968900189082
  (942, 17)	3.1114738421283077
  (942, 16)	3.1685447012618817
  (942, 15)	3.1682175277404356
  (942, 14)	3.572303824783901
  (942, 13)	3.530897449803222
  (942, 12)	3.2369518630430454
  (942, 11)	3.6600437229714844
  (942, 10)	3.3728709481671117
  (942, 9)	3.2549676563359946
  (942, 8)	3.559651080743092
  (942, 7)	3.4237358632572366
  (942, 6)	3.5728674398403713
  (942, 5)	3.2012267326448307
  (942, 4)	3.1659601015567933
  (942, 3)	3.1973164767862996
  (942, 2)	3.079520065544972
  (942, 1)	3.107884528228927
  (942, 0)	3.6721134718247908
this is the 20 epoch
rmse loss on training set is 1.0667747040317817
rmse loss on test set is 1.0752510520896696
for this epoch using 109.43683958053589 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999372611385741
  (0, 1680)	3.000000000000002
  (0, 1679)	2.993481728142689
  (0, 1678)	2.9998249343072216
  (0, 1677)	2.9871385219781605
  (0, 1676)	2.999567658278107
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998738344555018
  (0, 1671)	2.9935576884594877
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.994581333115504
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.994581333115504
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9874099472250673
  (0, 1662)	2.994581333115504
  (0, 1661)	2.9940264466235935
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000258481868416
  :	:
  (942, 24)	3.495444677354696
  (942, 23)	3.260464319620089
  (942, 22)	3.513521330276935
  (942, 21)	3.6344465592034365
  (942, 20)	3.064685854597554
  (942, 19)	3.186156255809915
  (942, 18)	3.338250972835
  (942, 17)	3.115318457396587
  (942, 16)	3.1747238707989633
  (942, 15)	3.1747184416959073
  (942, 14)	3.589264190242863
  (942, 13)	3.549950109098752
  (942, 12)	3.2451296706125903
  (942, 11)	3.6840973369681778
  (942, 10)	3.386727086252162
  (942, 9)	3.2652918671656153
  (942, 8)	3.578326110935387
  (942, 7)	3.439040029060531
  (942, 6)	3.588193589831174
  (942, 5)	3.2094958577810324
  (942, 4)	3.1721472925694343
  (942, 3)	3.2042923347397503
  (942, 2)	3.081575095919576
  (942, 1)	3.1113444089557505
  (942, 0)	3.6879776652301524
this is the 21 epoch
rmse loss on training set is 1.0622678085939046
rmse loss on test set is 1.071140494025472
for this epoch using 111.49119687080383 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9993188936612
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9931637138866867
  (0, 1678)	2.9998099969586787
  (0, 1677)	2.9865174308146907
  (0, 1676)	2.999535552438132
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998619839967453
  (0, 1671)	2.9932482410434273
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9943569577319877
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9943569577319877
  (0, 1664)	3.000000000000002
  (0, 1663)	2.986814104562846
  (0, 1662)	2.9943569577319877
  (0, 1661)	2.993756361753341
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0002825169116605
  :	:
  (942, 24)	3.5102297140924272
  (942, 23)	3.2695751381442695
  (942, 22)	3.5321448290490394
  (942, 21)	3.655386391172667
  (942, 20)	3.065818253927643
  (942, 19)	3.1929880069481897
  (942, 18)	3.3515736964181926
  (942, 17)	3.1190245717355847
  (942, 16)	3.1807308157599583
  (942, 15)	3.1810663680824187
  (942, 14)	3.6054178951619993
  (942, 13)	3.568406841449969
  (942, 12)	3.2530297789589326
  (942, 11)	3.707424408510574
  (942, 10)	3.400198852876079
  (942, 9)	3.2754151714297723
  (942, 8)	3.5963035190275936
  (942, 7)	3.453873250917911
  (942, 6)	3.602676048826002
  (942, 5)	3.2176187823448115
  (942, 4)	3.178171074636848
  (942, 3)	3.2110494686106157
  (942, 2)	3.0834908703704316
  (942, 1)	3.114661080126208
  (942, 0)	3.7027989772227046
this is the 22 epoch
rmse loss on training set is 1.0580278742100502
rmse loss on test set is 1.0672758404693319
for this epoch using 107.64757561683655 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999263772706449
  (0, 1680)	3.000000000000002
  (0, 1679)	2.992845277953293
  (0, 1678)	2.999794664686754
  (0, 1677)	2.985895891219831
  (0, 1676)	2.9995032337142495
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998496817624357
  (0, 1671)	2.992938895608099
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9941347551394264
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9941347551394264
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9862188048169918
  (0, 1662)	2.9941347551394264
  (0, 1661)	2.9934872574234554
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000307769905392
  :	:
  (942, 24)	3.524412792197305
  (942, 23)	3.278436113308631
  (942, 22)	3.55030609144278
  (942, 21)	3.6756371238931
  (942, 20)	3.0668160341182475
  (942, 19)	3.1996658944621075
  (942, 18)	3.3646693875183593
  (942, 17)	3.1225962431422674
  (942, 16)	3.1865708321058523
  (942, 15)	3.1872657362353713
  (942, 14)	3.620800735213203
  (942, 13)	3.5862845118697027
  (942, 12)	3.2606617304249426
  (942, 11)	3.7300435389022777
  (942, 10)	3.413296240735588
  (942, 9)	3.2853421779601706
  (942, 8)	3.6136073587395283
  (942, 7)	3.4682482830510293
  (942, 6)	3.616360590143029
  (942, 5)	3.2255994095496123
  (942, 4)	3.1840362373084945
  (942, 3)	3.217595118315948
  (942, 2)	3.0852730800533466
  (942, 1)	3.1178395654845756
  (942, 0)	3.7166435711050556
this is the 23 epoch
rmse loss on training set is 1.0540349209466136
rmse loss on test set is 1.06363944942291
for this epoch using 106.4794545173645 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9992073280111704
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9925264468063224
  (0, 1678)	2.9997789576390326
  (0, 1677)	2.985273935973615
  (0, 1676)	2.999470774540858
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9983693792180905
  (0, 1671)	2.992629684509944
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9939146357175668
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9939146357175668
  (0, 1664)	3.000000000000002
  (0, 1663)	2.985624064473558
  (0, 1662)	2.9939146357175668
  (0, 1661)	2.9932191033471094
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000334252242916
  :	:
  (942, 24)	3.538016973291636
  (942, 23)	3.2870540252752423
  (942, 22)	3.568015171877976
  (942, 21)	3.6952192737348857
  (942, 20)	3.0676846507877786
  (942, 19)	3.2061942934849212
  (942, 18)	3.377542374552412
  (942, 17)	3.126037465400151
  (942, 16)	3.192249087210046
  (942, 15)	3.193320903164171
  (942, 14)	3.6354472847363026
  (942, 13)	3.603599787997718
  (942, 12)	3.268034842054359
  (942, 11)	3.751973247990857
  (942, 10)	3.4260291306930974
  (942, 9)	3.2950774361682864
  (942, 8)	3.630261127759888
  (942, 7)	3.4821777691553595
  (942, 6)	3.6292908745928334
  (942, 5)	3.2334415867497186
  (942, 4)	3.1897474579466008
  (942, 3)	3.223936357329842
  (942, 2)	3.0869272446082783
  (942, 1)	3.120884745161922
  (942, 0)	3.7295740338377517
this is the 24 epoch
rmse loss on training set is 1.0502707002926843
rmse loss on test set is 1.0602150729350883
for this epoch using 106.6728310585022 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999149635858516
  (0, 1680)	3.000000000000002
  (0, 1679)	2.992207245669303
  (0, 1678)	2.9997628950314046
  (0, 1677)	2.9846515963072013
  (0, 1676)	2.999438242240691
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998237625408079
  (0, 1671)	2.9923206383415897
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9936965134390077
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9936965134390077
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9850298986427974
  (0, 1662)	2.9936965134390077
  (0, 1661)	2.9929518707388096
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000361973623034
  :	:
  (942, 24)	3.551064653018043
  (942, 23)	3.2954355386781997
  (942, 22)	3.5852821229503666
  (942, 21)	3.7141531039699025
  (942, 20)	3.068429412554258
  (942, 19)	3.212577494930706
  (942, 18)	3.3901969720152074
  (942, 17)	3.1293521581642403
  (942, 16)	3.197770616233277
  (942, 15)	3.1992361429383487
  (942, 14)	3.649390894117183
  (942, 13)	3.6203691063597883
  (942, 12)	3.275158194407224
  (942, 11)	3.7732319304179804
  (942, 10)	3.4384072746082515
  (942, 9)	3.304625425567153
  (942, 8)	3.6462877485034135
  (942, 7)	3.4956742167641504
  (942, 6)	3.6415085003571606
  (942, 5)	3.241149094822642
  (942, 4)	3.195309298254068
  (942, 3)	3.230080085942009
  (942, 2)	3.088458707455893
  (942, 1)	3.123801353929795
  (942, 0)	3.741649495532042
this is the 25 epoch
rmse loss on training set is 1.0467185235097987
rmse loss on test set is 1.0569877404243861
for this epoch using 108.64725422859192 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9990907694007314
  (0, 1680)	3.000000000000002
  (0, 1679)	2.991887698576294
  (0, 1678)	2.9997464951851445
  (0, 1677)	2.9840289019674353
  (0, 1676)	2.9994056992560583
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.998101655738574
  (0, 1671)	2.992011786003383
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.993480305738005
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.993480305738005
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9844363211364464
  (0, 1662)	2.993480305738005
  (0, 1661)	2.9926855322344332
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0003909420804593
  :	:
  (942, 24)	3.56357755457884
  (942, 23)	3.303587191671942
  (942, 22)	3.6021169702219695
  (942, 21)	3.7324585878370664
  (942, 20)	3.069055475233507
  (942, 19)	3.2188196973616248
  (942, 18)	3.402637467418041
  (942, 17)	3.132544159225688
  (942, 16)	3.2031403196459083
  (942, 15)	3.20501563839406
  (942, 14)	3.662663693765262
  (942, 13)	3.636608643877849
  (942, 12)	3.282040623391642
  (942, 11)	3.7938378168073768
  (942, 10)	3.4504402806269545
  (942, 9)	3.3139905474695026
  (942, 8)	3.661709553689391
  (942, 7)	3.508749974867271
  (942, 6)	3.6530530581857303
  (942, 5)	3.248725639798167
  (942, 4)	3.2007262018600473
  (942, 3)	3.2360330261810737
  (942, 2)	3.089872633154617
  (942, 1)	3.126593980467046
  (942, 0)	3.752925754625847
this is the 26 epoch
rmse loss on training set is 1.0433631096845735
rmse loss on test set is 1.0539436520188594
for this epoch using 106.85768246650696 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.999030798738887
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9915678284216423
  (0, 1678)	2.9997297755633383
  (0, 1677)	2.9834058812799418
  (0, 1676)	2.9993732033793292
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9979615685663927
  (0, 1671)	2.9917031547737
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9932659333830265
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9932659333830265
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9838433445422035
  (0, 1662)	2.9932659333830265
  (0, 1661)	2.9924200618155914
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0004211640252794
  :	:
  (942, 24)	3.57557672609583
  (942, 23)	3.3115153873864376
  (942, 22)	3.618529689740907
  (942, 21)	3.7501553766098557
  (942, 20)	3.069567837998861
  (942, 19)	3.2249250007990486
  (942, 18)	3.414868110390534
  (942, 17)	3.1356172186024707
  (942, 16)	3.2083629617458027
  (942, 15)	3.210663474791196
  (942, 14)	3.6752966037490804
  (942, 13)	3.652334294039306
  (942, 12)	3.28869071465623
  (942, 11)	3.813808939529766
  (942, 10)	3.462137600696493
  (942, 9)	3.3231771185164853
  (942, 8)	3.676548276154
  (942, 7)	3.521417214514446
  (942, 6)	3.663962190771587
  (942, 5)	3.2561748463756386
  (942, 4)	3.2060024928221313
  (942, 3)	3.241801718205339
  (942, 2)	3.091174006446475
  (942, 1)	3.1292670674936134
  (942, 0)	3.7634554070093134
this is the 27 epoch
rmse loss on training set is 1.0401904508419721
rmse loss on test set is 1.051070081068076
for this epoch using 110.83937954902649 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9989697910057194
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9912476570084596
  (0, 1678)	2.999712752806549
  (0, 1677)	2.982782561210364
  (0, 1676)	2.999340807981032
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9978174609977497
  (0, 1671)	2.991394770377997
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9930533203528618
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9930533203528618
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9832509802951117
  (0, 1662)	2.9930533203528618
  (0, 1661)	2.9921554347382755
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00045264428981
  :	:
  (942, 24)	3.587082541274334
  (942, 23)	3.319226387427536
  (942, 22)	3.6345301880794376
  (942, 21)	3.7672627722603615
  (942, 20)	3.069971341170554
  (942, 19)	3.230897402166283
  (942, 18)	3.4268931036370427
  (942, 17)	3.1385749941598085
  (942, 16)	3.2134431700362005
  (942, 15)	3.2161836351100037
  (942, 14)	3.6873193482513673
  (942, 13)	3.6675616471989243
  (942, 12)	3.295116800153936
  (942, 11)	3.8331631026970294
  (942, 10)	3.4735085200919187
  (942, 9)	3.3321893657468915
  (942, 8)	3.6908250423738846
  (942, 7)	3.5336879121467675
  (942, 6)	3.674271655307509
  (942, 5)	3.263500253028617
  (942, 4)	3.2111423749192185
  (942, 3)	3.247392517981984
  (942, 2)	3.0923676326816416
  (942, 1)	3.1318249126413833
  (942, 0)	3.7732879775944643
this is the 28 epoch
rmse loss on training set is 1.037187691885451
rmse loss on test set is 1.0483552850474909
for this epoch using 108.69087624549866 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9989078104508677
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9909272050956908
  (0, 1678)	2.99969544276746
  (0, 1677)	2.982158967423924
  (0, 1676)	2.9993085622344333
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.99766942883346
  (0, 1671)	2.9910866570561927
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9928423937163164
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9928423937163164
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9826592387458613
  (0, 1662)	2.9928423937163164
  (0, 1661)	2.991891627465376
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0004853861812513
  :	:
  (942, 24)	3.598114702915178
  (942, 23)	3.3267263071136655
  (942, 22)	3.650128284689959
  (942, 21)	3.783799704332712
  (942, 20)	3.0702706653553884
  (942, 19)	3.236740792100173
  (942, 18)	3.4387165954851566
  (942, 17)	3.1414210485114658
  (942, 16)	3.218385435342114
  (942, 15)	3.221579996725026
  (942, 14)	3.698760474096225
  (942, 13)	3.682305974544264
  (942, 12)	3.301326956543615
  (942, 11)	3.8519178560521183
  (942, 10)	3.4845621487542537
  (942, 9)	3.3410314229626974
  (942, 8)	3.7045603692363707
  (942, 7)	3.5455738354157824
  (942, 6)	3.68401538835388
  (942, 5)	3.2707053084432665
  (942, 4)	3.216149931622338
  (942, 3)	3.2528115960924957
  (942, 2)	3.0934581393624803
  (942, 1)	3.134271669946796
  (942, 0)	3.7824700530296913
this is the 29 epoch
rmse loss on training set is 1.0343430234689348
rmse loss on test set is 1.045788424136909
for this epoch using 107.10624718666077 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998844918527575
  (0, 1680)	3.000000000000002
  (0, 1679)	2.990606492443765
  (0, 1678)	2.9996778605446752
  (0, 1677)	2.9815351243428507
  (0, 1676)	2.999276511335551
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9975175665215996
  (0, 1671)	2.99077883762837
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.992633083515478
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.992633083515478
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9820681292259246
  (0, 1662)	2.992633083515478
  (0, 1661)	2.991628617602891
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0005193915389143
  :	:
  (942, 24)	3.608692248869703
  (942, 23)	3.334021112185475
  (942, 22)	3.6653336963898338
  (942, 21)	3.7997847106587375
  (942, 20)	3.070470331701165
  (942, 19)	3.242458952909065
  (942, 18)	3.4503426738021923
  (942, 17)	3.144158846991979
  (942, 16)	3.2231941125571306
  (942, 15)	3.2268563292343337
  (942, 14)	3.7096473726844215
  (942, 13)	3.6965822153053938
  (942, 12)	3.307329005142561
  (942, 11)	3.8700904724373166
  (942, 10)	3.495307414256477
  (942, 9)	3.349707328184692
  (942, 8)	3.717774163641318
  (942, 7)	3.5570865312645137
  (942, 6)	3.6932255722629646
  (942, 5)	3.2777933690762966
  (942, 4)	3.2210291266436823
  (942, 3)	3.258064937517485
  (942, 2)	3.0944499785900965
  (942, 1)	3.1366113518632717
  (942, 0)	3.791045414446327
this is the 30 epoch
rmse loss on training set is 1.0316455861864036
rmse loss on test set is 1.043359486812043
for this epoch using 109.60267448425293 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9987811739804022
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9902855378586497
  (0, 1678)	2.999660020515325
  (0, 1677)	2.9809110552019753
  (0, 1676)	2.9992446967177453
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9973619671171585
  (0, 1671)	2.9904713335585416
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.992425322652485
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.992425322652485
  (0, 1664)	3.000000000000002
  (0, 1663)	2.981477660109551
  (0, 1662)	2.992425322652485
  (0, 1661)	2.991366383839655
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000554660794918
  :	:
  (942, 24)	3.618833560079327
  (942, 23)	3.341116616762217
  (942, 22)	3.6801560237966116
  (942, 21)	3.815235921569472
  (942, 20)	3.070574703066907
  (942, 19)	3.2480555574888994
  (942, 18)	3.4617753610878546
  (942, 17)	3.14679175652272
  (942, 16)	3.227873421924527
  (942, 15)	3.2320162932560526
  (942, 14)	3.7200063047468093
  (942, 13)	3.7104049668338783
  (942, 12)	3.3131305131832542
  (942, 11)	3.8876979285397075
  (942, 10)	3.505753056227801
  (942, 9)	3.358221022023568
  (942, 8)	3.730485724564563
  (942, 7)	3.568237316059753
  (942, 6)	3.7019327025067326
  (942, 5)	3.284767697650715
  (942, 4)	3.2257838049748524
  (942, 3)	3.2631583422689725
  (942, 2)	3.0953474302315276
  (942, 1)	3.138847831703528
  (942, 0)	3.7990551692921044
this is the 31 epoch
rmse loss on training set is 1.029085384698575
rmse loss on test set is 1.0410592218414463
for this epoch using 112.16927123069763 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9987166329333346
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9899643592343987
  (0, 1678)	2.999641936366659
  (0, 1677)	2.980286782102143
  (0, 1676)	2.999213156260307
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.997202722247934
  (0, 1671)	2.990164165016547
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.992219046779798
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.992219046779798
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9808878388726328
  (0, 1662)	2.992219046779798
  (0, 1661)	2.99110490589034
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000591193037305
  :	:
  (942, 24)	3.628556370381347
  (942, 23)	3.3480184823515953
  (942, 22)	3.694604739547103
  (942, 21)	3.830171047276808
  (942, 20)	3.070587985940375
  (942, 19)	3.253534169037729
  (942, 18)	3.4730186105781597
  (942, 17)	3.14932304522158
  (942, 16)	3.232427450767644
  (942, 15)	3.2370634400322658
  (942, 14)	3.729862427393528
  (942, 13)	3.7237884772152006
  (942, 12)	3.3187387961613855
  (942, 11)	3.904756888629918
  (942, 10)	3.5159076220805603
  (942, 9)	3.3665763468182934
  (942, 8)	3.742713747252452
  (942, 7)	3.5790372675795328
  (942, 6)	3.710165655347542
  (942, 5)	3.2916314624354963
  (942, 4)	3.2304176943360736
  (942, 3)	3.268097426751221
  (942, 2)	3.0961546056550646
  (942, 1)	3.140984846432813
  (942, 0)	3.806537881456246
this is the 32 epoch
rmse loss on training set is 1.0266532106119886
rmse loss on test set is 1.0388790761312405
for this epoch using 107.36226725578308 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998651348977749
  (0, 1680)	3.000000000000002
  (0, 1679)	2.989642973594098
  (0, 1678)	2.999623621126529
  (0, 1677)	2.9796623260616677
  (0, 1676)	2.9991819244904456
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9970399220861106
  (0, 1671)	2.989857350937863
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9920141941939646
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9920141941939646
  (0, 1664)	3.000000000000002
  (0, 1663)	2.980298672148514
  (0, 1662)	2.9920141941939646
  (0, 1661)	2.9908441644415458
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0006289860748554
  :	:
  (942, 24)	3.6378777777986984
  (942, 23)	3.3547322177461973
  (942, 22)	3.7086891781438913
  (942, 21)	3.8446073681206463
  (942, 20)	3.0705142329605346
  (942, 19)	3.2588982414325898
  (942, 18)	3.4840763032186746
  (942, 17)	3.1517558826292142
  (942, 16)	3.2368601555941967
  (942, 15)	3.2420012117041996
  (942, 14)	3.73923982299702
  (942, 13)	3.7367466401134255
  (942, 12)	3.3241609210913565
  (942, 11)	3.921283691027488
  (942, 10)	3.52577946389727
  (942, 9)	3.3747770464154843
  (942, 8)	3.7544763292518826
  (942, 7)	3.589497218673982
  (942, 6)	3.7179517553722836
  (942, 5)	3.2983877371778485
  (942, 4)	3.2349344069670884
  (942, 3)	3.2728876257440316
  (942, 2)	3.096875451905689
  (942, 1)	3.14302599974397
  (942, 0)	3.813529699024687
this is the 33 epoch
rmse loss on training set is 1.0243405730890567
rmse loss on test set is 1.036811137906352
for this epoch using 109.80649614334106 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998585373259966
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9893213971292036
  (0, 1678)	2.999605087192704
  (0, 1677)	2.979037707065711
  (0, 1676)	2.9991510327783084
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.996873655325076
  (0, 1671)	2.9895509090813306
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.991810705732754
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.991810705732754
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9797101657808405
  (0, 1662)	2.991810705732754
  (0, 1661)	2.9905841411008276
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000668036502792
  :	:
  (942, 24)	3.646814257063792
  (942, 23)	3.3612631796634305
  (942, 22)	3.7224185272841486
  (942, 21)	3.8585617273975634
  (942, 20)	3.0703573459240023
  (942, 19)	3.2641511201531848
  (942, 18)	3.4949522453845643
  (942, 17)	3.1540933404435254
  (942, 16)	3.2411753645084524
  (942, 15)	3.2468329421429636
  (942, 14)	3.7481615295018798
  (942, 13)	3.749292991577929
  (942, 12)	3.3294037105105585
  (942, 11)	3.93729433704289
  (942, 10)	3.53537673634776
  (942, 9)	3.382826766482605
  (942, 8)	3.7657909780123777
  (942, 7)	3.59962775243125
  (942, 6)	3.7253168424836063
  (942, 5)	3.3050395015765837
  (942, 4)	3.239337441699014
  (942, 3)	3.2775341949132724
  (942, 2)	3.097513756213453
  (942, 1)	3.1449747653539206
  (942, 0)	3.8200644791224034
this is the 34 epoch
rmse loss on training set is 1.022139636306001
rmse loss on test set is 1.0348480847601216
for this epoch using 108.24945187568665 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99851875456793
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9889996452373615
  (0, 1678)	2.99958634636109
  (0, 1677)	2.97841294411363
  (0, 1676)	2.9991205095248383
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.996704009160937
  (0, 1671)	2.98924485608487
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9916085246756983
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9916085246756983
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9791223248734875
  (0, 1662)	2.9916085246756983
  (0, 1661)	2.9903248183485274
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0007083397688277
  :	:
  (942, 24)	3.655381673154596
  (942, 23)	3.3676165740056003
  (942, 22)	3.7358018205354275
  (942, 21)	3.872050526506765
  (942, 20)	3.070121079173332
  (942, 19)	3.2692960436537066
  (942, 18)	3.5056501672413805
  (942, 17)	3.156338393670731
  (942, 16)	3.2453767798729767
  (942, 15)	3.2515618582366224
  (942, 14)	3.7566495718035053
  (942, 13)	3.7614407085700576
  (942, 12)	3.3344737470951475
  (942, 11)	3.9528044821632196
  (942, 10)	3.544707395517665
  (942, 9)	3.390729055263037
  (942, 8)	3.7766746198243824
  (942, 7)	3.6094391986934076
  (942, 6)	3.732285338006491
  (942, 5)	3.3115896422013296
  (942, 4)	3.243630186253814
  (942, 3)	3.2820422137640466
  (942, 2)	3.0980731507448027
  (942, 1)	3.14683449046913
  (942, 0)	3.826173909404946
this is the 35 epoch
rmse loss on training set is 1.0200431629917257
rmse loss on test set is 1.0329831361442843
for this epoch using 109.6562430858612 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998451539416804
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9886777325586142
  (0, 1678)	2.9995674098528253
  (0, 1677)	2.977788055264406
  (0, 1676)	2.9990903803421416
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9965310692783946
  (0, 1671)	2.9889392075189996
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9914075966479774
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9914075966479774
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9785351538377474
  (0, 1662)	2.9914075966479774
  (0, 1661)	2.990066179492135
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0007498902390255
  :	:
  (942, 24)	3.6635952956472644
  (942, 23)	3.37379745763386
  (942, 22)	3.7488479312332026
  (942, 21)	3.8850897221689387
  (942, 20)	3.0698090432803418
  (942, 19)	3.2743361450988377
  (942, 18)	3.516173721654668
  (942, 17)	3.1584939221146624
  (942, 16)	3.2494679811691647
  (942, 15)	3.256191081549485
  (942, 14)	3.764724993882279
  (942, 13)	3.773202608992366
  (942, 12)	3.33937737876872
  (942, 11)	3.9678294292647673
  (942, 10)	3.5537791985406293
  (942, 9)	3.3984873646949025
  (942, 8)	3.7871436098840134
  (942, 7)	3.618941631779292
  (942, 6)	3.738880309625459
  (942, 5)	3.318040953776324
  (942, 4)	3.247815919724873
  (942, 3)	3.2864165889613997
  (942, 2)	3.0985571175218105
  (942, 1)	3.148608399374624
  (942, 0)	3.831887625853274
this is the 36 epoch
rmse loss on training set is 1.018044463379689
rmse loss on test set is 1.0312100099081787
for this epoch using 108.05715942382812 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998383772133211
  (0, 1680)	3.000000000000002
  (0, 1679)	2.98835567301014
  (0, 1678)	2.9995482883402587
  (0, 1677)	2.977163057680017
  (0, 1676)	2.999060668226416
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.996354919840442
  (0, 1671)	2.9886339779383757
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9912078695275497
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9912078695275497
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9779486564368365
  (0, 1662)	2.9912078695275497
  (0, 1661)	2.989808208623218
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0007926812630377
  :	:
  (942, 24)	3.671469813712291
  (942, 23)	3.379810740564399
  (942, 22)	3.761565567484321
  (942, 21)	3.8976948254920094
  (942, 20)	3.069424708951353
  (942, 19)	3.279274454392223
  (942, 18)	3.526526483568776
  (942, 17)	3.160562712137937
  (942, 16)	3.253452428012051
  (942, 15)	3.2607236302813876
  (942, 14)	3.7724078914195216
  (942, 13)	3.784591153025475
  (942, 12)	3.3441207242014044
  (942, 11)	3.9823841236513275
  (942, 10)	3.562599703936285
  (942, 9)	3.4061050518265916
  (942, 8)	3.797213743296934
  (942, 7)	3.628144869283178
  (942, 6)	3.7451235349185983
  (942, 5)	3.324396140759517
  (942, 4)	3.2518978151981686
  (942, 3)	3.290662057951656
  (942, 2)	3.0989689934463955
  (942, 1)	3.1502995971074204
  (942, 0)	3.837233326606793
this is the 37 epoch
rmse loss on training set is 1.0161373489889265
rmse loss on test set is 1.029522882530304
for this epoch using 110.9410650730133 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998315494937877
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9880334798194856
  (0, 1678)	2.9995289919719044
  (0, 1677)	2.976537967667074
  (0, 1676)	2.999031393723263
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.996175643481691
  (0, 1671)	2.98832918093115
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9910092933554937
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9910092933554937
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9773628358278184
  (0, 1662)	2.9910092933554937
  (0, 1661)	2.9895508905765973
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0008367052383838
  :	:
  (942, 24)	3.679019351601536
  (942, 23)	3.3856611885082772
  (942, 22)	3.773963268169078
  (942, 21)	3.909880902675683
  (942, 20)	3.0689714110923014
  (942, 19)	3.2841139004363913
  (942, 18)	3.536711949785552
  (942, 17)	3.162547458638348
  (942, 16)	3.257333463280771
  (942, 15)	3.26516242146555
  (942, 14)	3.779717444657658
  (942, 13)	3.7956184455982203
  (942, 12)	3.3487096786111166
  (942, 11)	3.996483149732201
  (942, 10)	3.5711762725654954
  (942, 9)	3.413585380471911
  (942, 8)	3.806900266854846
  (942, 7)	3.6370584718289787
  (942, 6)	3.751035563298656
  (942, 5)	3.3306578191578455
  (942, 4)	3.2558789424788075
  (942, 3)	3.2947831928257436
  (942, 2)	3.099311975377626
  (942, 1)	3.1519110731806976
  (942, 0)	3.8422368816400048
this is the 38 epoch
rmse loss on training set is 1.0143160907221274
rmse loss on test set is 1.0279163527166428
for this epoch using 111.57013750076294 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998246748026651
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9877111655563597
  (0, 1678)	2.9995095303962565
  (0, 1677)	2.975912800716452
  (0, 1676)	2.999002575085578
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.995993321304869
  (0, 1671)	2.988024829166403
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.990811820249539
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.990811820249539
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9767776946011177
  (0, 1662)	2.990811820249539
  (0, 1661)	2.989294210891796
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0008819536734124
  :	:
  (942, 24)	3.6862574844923395
  (942, 23)	3.3913534256867344
  (942, 22)	3.7860493998436167
  (942, 21)	3.921662577163559
  (942, 20)	3.0684523529816317
  (942, 19)	3.2888573135720893
  (942, 18)	3.5467335390826964
  (942, 17)	3.164450767192466
  (942, 16)	3.261114316331293
  (942, 15)	3.269510273352499
  (942, 14)	3.786671951299026
  (942, 13)	3.8062962398346154
  (942, 12)	3.3531499197906482
  (942, 11)	4.010140729168104
  (942, 10)	3.579516069122874
  (942, 9)	3.4209315230559163
  (942, 8)	3.8162178914366702
  (942, 7)	3.645691743670015
  (942, 6)	3.756635776211009
  (942, 5)	3.336828518528256
  (942, 4)	3.259762270892626
  (942, 3)	3.2987844043723675
  (942, 2)	3.0995891252186607
  (942, 1)	3.153445705330005
  (942, 0)	3.8469224381488574
this is the 39 epoch
rmse loss on training set is 1.012575380830897
rmse loss on test set is 1.0263854080690973
for this epoch using 108.91368222236633 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9981775696495574
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9873887421629983
  (0, 1678)	2.999489912784677
  (0, 1677)	2.975287571541309
  (0, 1676)	2.9989742284238856
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9958080328802668
  (0, 1671)	2.987720934439488
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9906154043206463
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9906154043206463
  (0, 1664)	3.000000000000002
  (0, 1663)	2.976193234817709
  (0, 1662)	2.9906154043206463
  (0, 1661)	2.9890381557765138
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000928417248831
  :	:
  (942, 24)	3.693197254570847
  (942, 23)	3.396891937863899
  (942, 22)	3.7978321544516414
  (942, 21)	3.9330540330676063
  (942, 20)	3.067870610507129
  (942, 19)	3.293507428152888
  (942, 18)	3.5565945926195726
  (942, 17)	3.1662751563256086
  (942, 16)	3.2647981062624782
  (942, 15)	3.2737699079355758
  (942, 14)	3.7932888592664353
  (942, 13)	3.8166359413378714
  (942, 12)	3.357446914295246
  (942, 11)	4.02337072032679
  (942, 10)	3.587626064094285
  (942, 9)	3.428146562609789
  (942, 8)	3.825180804903144
  (942, 7)	3.6540537340340076
  (942, 6)	3.7619424454719486
  (942, 5)	3.3429106841214864
  (942, 4)	3.263550672136638
  (942, 3)	3.3026699462757207
  (942, 2)	3.0998033749779723
  (942, 1)	3.1549062632571605
  (942, 0)	3.851312521565474
this is the 40 epoch
rmse loss on training set is 1.0109102983509153
rmse loss on test set is 1.0249253945538135
for this epoch using 109.3957417011261 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.998107996187989
  (0, 1680)	3.000000000000002
  (0, 1679)	2.987066220983238
  (0, 1678)	2.9994701478531947
  (0, 1677)	2.9746622941132888
  (0, 1676)	2.998946367849477
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9956198562478167
  (0, 1671)	2.9874175077154947
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.990420001592626
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.990420001592626
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9756094580441133
  (0, 1662)	2.990420001592626
  (0, 1661)	2.988782712072118
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.000976085877395
  :	:
  (942, 24)	3.699851187252133
  (942, 23)	3.402281075546649
  (942, 22)	3.809319547762269
  (942, 21)	3.9440690197047448
  (942, 20)	3.067229136429916
  (942, 19)	3.29806688521747
  (942, 18)	3.5662983745848593
  (942, 17)	3.1680230598736476
  (942, 16)	3.268387845210714
  (942, 15)	3.2779439535798534
  (942, 14)	3.7995847991744145
  (942, 13)	3.8266486131866397
  (942, 12)	3.361605923734401
  (942, 11)	4.036186618903176
  (942, 10)	3.5955130361145176
  (942, 9)	3.4352334948790415
  (942, 8)	3.8338026853687928
  (942, 7)	3.662153239121818
  (942, 6)	3.766972789661055
  (942, 5)	3.348906679132196
  (942, 4)	3.267246923155918
  (942, 3)	3.3064439194176325
  (942, 2)	3.099957531775506
  (942, 1)	3.15629541235128
  (942, 0)	3.8554281321649744
this is the 41 epoch
rmse loss on training set is 1.0093162776562652
rmse loss on test set is 1.0235319885232887
for this epoch using 109.7616765499115 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9980380622298393
  (0, 1680)	3.000000000000002
  (0, 1679)	2.986743612790233
  (0, 1678)	2.9994502438833934
  (0, 1677)	2.974036981697072
  (0, 1676)	2.9989190056102606
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9954288679215684
  (0, 1671)	2.987114559170749
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.990225569924669
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.990225569924669
  (0, 1664)	3.000000000000002
  (0, 1663)	2.975026365385249
  (0, 1662)	2.990225569924669
  (0, 1661)	2.98852786722094
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0010249487618808
  :	:
  (942, 24)	3.7062313074472675
  (942, 23)	3.407525057308815
  (942, 22)	3.8205194184575357
  (942, 21)	3.9547208570994923
  (942, 20)	3.0665307646450928
  (942, 19)	3.30253823522772
  (942, 18)	3.57584807304636
  (942, 17)	3.16969682940736
  (942, 16)	3.271886441651935
  (942, 15)	3.2820349477224133
  (942, 14)	3.805575616383106
  (942, 13)	3.83634498153192
  (942, 12)	3.3656320111198177
  (942, 11)	4.0486015595705585
  (942, 10)	3.603183574666556
  (942, 9)	3.442195230514988
  (942, 8)	3.842096714748795
  (942, 7)	3.6699988046766605
  (942, 6)	3.7717430285066462
  (942, 5)	3.3548187870243704
  (942, 4)	3.2708537090278824
  (942, 3)	3.310110276249494
  (942, 2)	3.100054282770402
  (942, 1)	3.1576157173697443
  (942, 0)	3.859288837265916
this is the 42 epoch
rmse loss on training set is 1.0077890798216784
rmse loss on test set is 1.022201171068393
for this epoch using 106.76237106323242 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9979678006424977
  (0, 1680)	3.000000000000002
  (0, 1679)	2.986420927812915
  (0, 1678)	2.9994302087423814
  (0, 1677)	2.9734116468834486
  (0, 1676)	2.998892152219667
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9952351428963464
  (0, 1671)	2.986812098232511
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9900320689367614
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9900320689367614
  (0, 1664)	3.000000000000002
  (0, 1663)	2.974443957515412
  (0, 1662)	2.9900320689367614
  (0, 1661)	2.9882736092353706
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001074994450918
  :	:
  (942, 24)	3.7123491558000317
  (942, 23)	3.4126279732029468
  (942, 22)	3.8314394277996224
  (942, 21)	3.965022442319472
  (942, 20)	3.0657782144136654
  (942, 19)	3.306923940846041
  (942, 18)	3.5852468009688168
  (942, 17)	3.1712987366947565
  (942, 16)	3.2752967036929697
  (942, 15)	3.286045339616715
  (942, 14)	3.811276402526859
  (942, 13)	3.845735441695436
  (942, 12)	3.3695300472286025
  (942, 11)	4.060628318540972
  (942, 10)	3.6106440830702407
  (942, 9)	3.4490345973235685
  (942, 8)	3.8500755924903984
  (942, 7)	3.6775987290485586
  (942, 6)	3.7762684352259868
  (942, 5)	3.3606492139057926
  (942, 4)	3.27437362583769
  (942, 3)	3.3136728252037244
  (942, 2)	3.1000961999911705
  (942, 1)	3.1588696460647543
  (942, 0)	3.8629128590582473
this is the 43 epoch
rmse loss on training set is 1.0063247665166009
rmse loss on test set is 1.0209292044964826
for this epoch using 108.33658051490784 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997897242643803
  (0, 1680)	3.000000000000002
  (0, 1679)	2.986098175761269
  (0, 1678)	2.9994100499018366
  (0, 1677)	2.9727863016206997
  (0, 1676)	2.9988658165787703
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.99503875465632
  (0, 1671)	2.9865101336168816
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.989839459937917
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.989839459937917
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9738622347072656
  (0, 1662)	2.989839459937917
  (0, 1661)	2.98801992666861
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001126210892881
  :	:
  (942, 24)	3.718215804825992
  (942, 23)	3.4175937882283693
  (942, 22)	3.8420870598138355
  (942, 21)	3.9749862565233287
  (942, 20)	3.0649740945448274
  (942, 19)	3.3112263797290247
  (942, 18)	3.5944975973696223
  (942, 17)	3.172830976180641
  (942, 16)	3.278621342337074
  (942, 15)	3.2899774930980117
  (942, 14)	3.8167015264280137
  (942, 13)	3.854830064680975
  (942, 12)	3.3733047169469725
  (942, 11)	4.072279316923395
  (942, 10)	3.617900781713424
  (942, 9)	3.4557543425498323
  (942, 8)	3.8577515494099046
  (942, 7)	3.6849610666854344
  (942, 6)	3.780563386800989
  (942, 5)	3.3664000909295715
  (942, 4)	3.277809183531206
  (942, 3)	3.3171352351187195
  (942, 2)	3.100085745053577
  (942, 1)	3.1600595727439242
  (942, 0)	3.8663171581194424
this is the 44 epoch
rmse loss on training set is 1.0049196761848453
rmse loss on test set is 1.0197126107501673
for this epoch using 110.48176097869873 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997826417870747
  (0, 1680)	3.000000000000002
  (0, 1679)	2.985775365850451
  (0, 1678)	2.999389774456211
  (0, 1677)	2.9721609572446908
  (0, 1676)	2.9988400060917635
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9948397751853757
  (0, 1671)	2.986208673364928
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9896477058570627
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9896477058570627
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9732811968591646
  (0, 1662)	2.9896477058570627
  (0, 1661)	2.9877668085869282
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0011785854875868
  :	:
  (942, 24)	3.7238418748966047
  (942, 23)	3.422426345829039
  (942, 22)	3.8524696219288446
  (942, 21)	3.984624372611377
  (942, 20)	3.064120907511781
  (942, 19)	3.315447847318744
  (942, 18)	3.603603428586499
  (942, 17)	3.1742956674661493
  (942, 16)	3.281862974710953
  (942, 15)	3.2938336893507394
  (942, 14)	3.821864664322145
  (942, 13)	3.863638604020543
  (942, 12)	3.376960525565152
  (942, 11)	4.083566624777977
  (942, 10)	3.6249597114836436
  (942, 9)	3.4623571351797495
  (942, 8)	3.865136361565784
  (942, 7)	3.6920936319890143
  (942, 6)	3.784641412186223
  (942, 5)	3.3720734767040725
  (942, 4)	3.281162808734119
  (942, 3)	3.3205010396546273
  (942, 2)	3.1000252737544196
  (942, 1)	3.1611877817553746
  (942, 0)	3.869517512701343
this is the 45 epoch
rmse loss on training set is 1.0035704022907097
rmse loss on test set is 1.0185481515981143
for this epoch using 107.78911280632019 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9977553544460838
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9854525068237674
  (0, 1678)	2.999369389140086
  (0, 1677)	2.971535624507456
  (0, 1676)	2.9988147267751395
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9946382749790272
  (0, 1671)	2.985907724877212
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9894567711765925
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9894567711765925
  (0, 1664)	3.000000000000002
  (0, 1663)	2.972700843520755
  (0, 1662)	2.9894567711765925
  (0, 1661)	2.9875142445434895
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001232105135811
  :	:
  (942, 24)	3.729237550019125
  (942, 23)	3.427129371398618
  (942, 22)	3.862594246021084
  (942, 21)	3.9939484633803484
  (942, 20)	3.0632210534871303
  (942, 19)	3.319590559615994
  (942, 18)	3.61256718963483
  (942, 17)	3.1756948577738764
  (942, 16)	3.285024127242525
  (942, 15)	3.2976161296615447
  (942, 14)	3.826778829335377
  (942, 13)	3.8721705028854236
  (942, 12)	3.3805018049988242
  (942, 11)	4.094501965774118
  (942, 10)	3.6318267373631135
  (942, 9)	3.468845568243763
  (942, 8)	3.8722413641077105
  (942, 7)	3.699004003479739
  (942, 6)	3.788515238460311
  (942, 5)	3.3776713596957313
  (942, 4)	3.2844368475277887
  (942, 3)	3.3237736416809662
  (942, 2)	3.0999170405327554
  (942, 1)	3.1622564708897225
  (942, 0)	3.872528593888375
this is the 46 epoch
rmse loss on training set is 1.0022737734352911
rmse loss on test set is 1.0174328104443504
for this epoch using 109.10217690467834 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9976840790427044
  (0, 1680)	3.000000000000002
  (0, 1679)	2.985129606974629
  (0, 1678)	2.9993489003447666
  (0, 1677)	2.970910313604487
  (0, 1676)	2.9987899833606506
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.994434323057772
  (0, 1671)	2.985607294946621
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9892666218684933
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9892666218684933
  (0, 1664)	3.000000000000002
  (0, 1663)	2.972121173917051
  (0, 1662)	2.9892666218684933
  (0, 1661)	2.9872622245535014
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001286756286658
  :	:
  (942, 24)	3.734412593371078
  (942, 23)	3.431706475773821
  (942, 22)	3.872467889814614
  (942, 21)	4.002969810093044
  (942, 20)	3.0622768342872213
  (942, 19)	3.3236566559222465
  (942, 18)	3.6213917056349847
  (942, 17)	3.1770305243867316
  (942, 16)	3.2881072387809853
  (942, 15)	3.3013269381448733
  (942, 14)	3.831456400166816
  (942, 13)	3.8804349014011033
  (942, 12)	3.3839327199168663
  (942, 11)	4.105096722368227
  (942, 10)	3.638507552153802
  (942, 9)	3.4752221611093757
  (942, 8)	3.8790774650491464
  (942, 7)	3.7056995282203906
  (942, 6)	3.7921968349429407
  (942, 5)	3.383195660611999
  (942, 4)	3.2876335681741424
  (942, 3)	3.32695631761953
  (942, 2)	3.099763202792022
  (942, 1)	3.1632677546933485
  (942, 0)	3.8753640367420474
this is the 47 epoch
rmse loss on training set is 1.0010268351675593
rmse loss on test set is 1.0163637756164847
for this epoch using 106.50563526153564 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99761261694591
  (0, 1680)	3.000000000000002
  (0, 1679)	2.984806674167429
  (0, 1678)	2.999328314134118
  (0, 1677)	2.970285034200743
  (0, 1676)	2.9987657793924853
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.994227986981701
  (0, 1671)	2.985307389789769
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.989077225332897
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.989077225332897
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9715421869709773
  (0, 1662)	2.989077225332897
  (0, 1661)	2.987010739070722
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001342524982697
  :	:
  (942, 24)	3.7393763625541685
  (942, 23)	3.4361611587003296
  (942, 22)	3.882097338592549
  (942, 21)	4.0116993113826345
  (942, 20)	3.0612904572168516
  (942, 19)	3.3276482015399065
  (942, 18)	3.630079733292924
  (942, 17)	3.178304577050796
  (942, 16)	3.2911146636518858
  (942, 15)	3.3049681644300226
  (942, 14)	3.835909148939875
  (942, 13)	3.888440644111402
  (942, 12)	3.3872572737583644
  (942, 11)	4.115361941425095
  (942, 10)	3.645007680303008
  (942, 9)	3.481489361751817
  (942, 8)	3.8856551589183264
  (942, 7)	3.712187326453037
  (942, 6)	3.795697455309903
  (942, 5)	3.3886482347536955
  (942, 4)	3.2907551637834387
  (942, 3)	3.330052221728931
  (942, 2)	3.099565825079021
  (942, 1)	3.1642236676882995
  (942, 0)	3.878036507557753
this is the 48 epoch
rmse loss on training set is 0.9998268333328589
rmse loss on test set is 1.01533842500565
for this epoch using 106.11734199523926 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9975409921135285
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9844837158575337
  (0, 1678)	2.9993076362596245
  (0, 1677)	2.9696597954554464
  (0, 1676)	2.998742117318683
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.994019332866204
  (0, 1671)	2.985008015076758
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.98888855033906
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.98888855033906
  (0, 1664)	3.000000000000002
  (0, 1663)	2.970963881324627
  (0, 1662)	2.98888855033906
  (0, 1661)	2.9867597789651863
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001399396902989
  :	:
  (942, 24)	3.7441378245390786
  (942, 23)	3.4404968122578867
  (942, 22)	3.891489207179677
  (942, 21)	4.020147492419899
  (942, 20)	3.060264038807821
  (942, 19)	3.3315671904221302
  (942, 18)	3.63863396241957
  (942, 17)	3.1795188603343854
  (942, 16)	3.2940486746418913
  (942, 15)	3.308541786300689
  (942, 14)	3.840148268196314
  (942, 13)	3.896196287544491
  (942, 12)	3.390479314625288
  (942, 11)	4.125308340213989
  (942, 10)	3.6513324818035158
  (942, 9)	3.4876495489942534
  (942, 8)	3.8919845402490165
  (942, 7)	3.7184742964088633
  (942, 6)	3.7990276777461123
  (942, 5)	3.394030874328175
  (942, 4)	3.2938037549198778
  (942, 3)	3.3330643903190693
  (942, 2)	3.09932688311698
  (942, 1)	3.1651261674959144
  (942, 0)	3.8805577673682214
this is the 49 epoch
rmse loss on training set is 0.9986711988174386
rmse loss on test set is 1.0143543119426375
for this epoch using 106.68025016784668 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997469227233889
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9841607391102736
  (0, 1678)	2.9992868721748414
  (0, 1677)	2.9690346060456956
  (0, 1676)	2.998718998577266
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.993808425398729
  (0, 1671)	2.984709175959683
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.988700566968678
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.988700566968678
  (0, 1664)	3.000000000000002
  (0, 1663)	2.970386255359079
  (0, 1662)	2.988700566968678
  (0, 1661)	2.986509335502137
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0014573574038996
  :	:
  (942, 24)	3.74870557027745
  (942, 23)	3.444716724233943
  (942, 22)	3.900649942160368
  (942, 21)	4.028324514278808
  (942, 20)	3.0591996084469537
  (942, 19)	3.335415547765241
  (942, 18)	3.6470570174764947
  (942, 17)	3.180675155936961
  (942, 16)	3.2969114659085825
  (942, 15)	3.3120497122800012
  (942, 14)	3.8441843970146783
  (942, 13)	3.9037101078385645
  (942, 12)	3.393602541040019
  (942, 11)	4.134946312717287
  (942, 10)	3.657487156145111
  (942, 9)	3.4937050347100205
  (942, 8)	3.8980753168779403
  (942, 7)	3.724567119254368
  (942, 6)	3.802197443183354
  (942, 5)	3.399345310716306
  (942, 4)	3.2967813921414
  (942, 3)	3.335995745886205
  (942, 2)	3.0990482676917193
  (942, 1)	3.1659771378619026
  (942, 0)	3.882938731834468
this is the 50 epoch
rmse loss on training set is 0.9975575335619411
rmse loss on test set is 1.0134091522047828
for this epoch using 110.22133445739746 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9973973437817496
  (0, 1680)	3.000000000000002
  (0, 1679)	2.983837750619071
  (0, 1678)	2.999266027049103
  (0, 1677)	2.968409474189036
  (0, 1676)	2.9986964236770923
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9935953278563705
  (0, 1671)	2.984410877099621
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.988513246561397
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.988513246561397
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9698093072131098
  (0, 1662)	2.988513246561397
  (0, 1661)	2.9862594003220595
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0015163915578364
  :	:
  (942, 24)	3.7530878289623155
  (942, 23)	3.448824081436854
  (942, 22)	3.9095858242983894
  (942, 21)	4.036240183443351
  (942, 20)	3.058099111889991
  (942, 19)	3.3391951325384692
  (942, 18)	3.65535145913718
  (942, 17)	3.1817751849430187
  (942, 16)	3.2997051558123287
  (942, 15)	3.315493784155234
  (942, 14)	3.848027646242391
  (942, 13)	3.91099010839091
  (942, 12)	3.396630507558872
  (942, 11)	4.1442859361953595
  (942, 10)	3.663476746297255
  (942, 9)	3.4996580659813863
  (942, 8)	3.9039368230211715
  (942, 7)	3.7304722641413655
  (942, 6)	3.8052160916746574
  (942, 5)	3.4045932166877786
  (942, 4)	3.2996900584707274
  (942, 3)	3.338849101160623
  (942, 2)	3.0987317883909733
  (942, 1)	3.166778391581866
  (942, 0)	3.88518952766817
this is the 51 epoch
rmse loss on training set is 0.9964835977294229
rmse loss on test set is 1.0125008120577608
for this epoch using 115.77545094490051 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9973253620721767
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9835147567227454
  (0, 1678)	2.999245105780644
  (0, 1677)	2.9677844076648534
  (0, 1676)	2.9986743922738373
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9933801021242807
  (0, 1671)	2.984113122692407
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.988326561662517
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.988326561662517
  (0, 1664)	3.000000000000002
  (0, 1663)	2.969233034800684
  (0, 1662)	2.988326561662517
  (0, 1661)	2.9860099654217303
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0015764841899046
  :	:
  (942, 24)	3.7572924819220685
  (942, 23)	3.452821972941609
  (942, 22)	3.918302971129223
  (942, 21)	4.043903961404259
  (942, 20)	3.056964414659857
  (942, 19)	3.3429077399467704
  (942, 18)	3.663519785854863
  (942, 17)	3.1828206100172656
  (942, 16)	3.3024317896676205
  (942, 15)	3.3188757794379016
  (942, 14)	3.8516876228369137
  (942, 13)	3.9180440274982296
  (942, 12)	3.3995666302353715
  (942, 11)	4.153336977957587
  (942, 10)	3.6693061427050853
  (942, 9)	3.505510827210088
  (942, 8)	3.909578032106192
  (942, 7)	3.7361959933320756
  (942, 6)	3.8080923969608222
  (942, 5)	3.409776208560438
  (942, 4)	3.3025316717957667
  (942, 3)	3.3416271630608625
  (942, 2)	3.0983791771977245
  (942, 1)	3.1675316733268404
  (942, 0)	3.88731954573291
this is the 52 epoch
rmse loss on training set is 0.9954472979246435
rmse loss on test set is 1.0116272972448694
for this epoch using 108.08334732055664 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997253301312382
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9831917634219955
  (0, 1678)	2.9992241130090953
  (0, 1677)	2.967159413834904
  (0, 1676)	2.9986529032413207
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.993162808714673
  (0, 1671)	2.983815916493126
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.988140485972777
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.988140485972777
  (0, 1664)	3.000000000000002
  (0, 1663)	2.968657435827395
  (0, 1662)	2.988140485972777
  (0, 1661)	2.9857610231363076
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0016376199125285
  :	:
  (942, 24)	3.761327076137039
  (942, 23)	3.456713393262429
  (942, 22)	3.926807339697723
  (942, 21)	4.051324974300546
  (942, 20)	3.055797305327974
  (942, 19)	3.346555103823661
  (942, 18)	3.671564435428992
  (942, 17)	3.1838130375381577
  (942, 16)	3.3050933424124835
  (942, 15)	3.3221974137558767
  (942, 14)	3.8551734533167457
  (942, 13)	3.9248793459607287
  (942, 12)	3.4024141919283317
  (942, 11)	4.162108902293799
  (942, 10)	3.67498008728311
  (942, 9)	3.5112654421760574
  (942, 8)	3.915007569340355
  (942, 7)	3.7417443673731126
  (942, 6)	3.8108345992882833
  (942, 5)	3.4148958483003318
  (942, 4)	3.3053080871979352
  (942, 3)	3.344332536549415
  (942, 2)	3.0979920919396333
  (942, 1)	3.1682386623690095
  (942, 0)	3.8893374909711356
this is the 53 epoch
rmse loss on training set is 0.994446676371468
rmse loss on test set is 1.0107867428442054
for this epoch using 114.04027032852173 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9971811796516716
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9828687763951094
  (0, 1678)	2.999203053127436
  (0, 1677)	2.9665344996627834
  (0, 1676)	2.998631954738283
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.992943506786532
  (0, 1671)	2.9835192618393953
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.987954994300164
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.987954994300164
  (0, 1664)	3.000000000000002
  (0, 1663)	2.968082507805869
  (0, 1662)	2.987954994300164
  (0, 1661)	2.9855125661223463
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0016997831581698
  :	:
  (942, 24)	3.765198837370743
  (942, 23)	3.460501245448038
  (942, 22)	3.9351047294170227
  (942, 21)	4.058512022565884
  (942, 20)	3.054599498678973
  (942, 19)	3.350138898951998
  (942, 18)	3.679487786563905
  (942, 17)	3.18475401966804
  (942, 16)	3.3076917211950705
  (942, 15)	3.325460343175313
  (942, 14)	3.8584938063278327
  (942, 13)	3.9315032946259483
  (942, 12)	3.40517634745164
  (942, 11)	4.1706108775259505
  (942, 10)	3.6805031773931
  (942, 9)	3.5169239760418396
  (942, 8)	3.920233724000206
  (942, 7)	3.747123250295985
  (942, 6)	3.8134504365392434
  (942, 5)	3.419953645560276
  (942, 4)	3.308021099207899
  (942, 3)	3.3469677283862747
  (942, 2)	3.097572119596848
  (942, 1)	3.1689009752085435
  (942, 0)	3.8912514293045777
this is the 54 epoch
rmse loss on training set is 0.9934799009642007
rmse loss on test set is 1.0099774039210914
for this epoch using 112.57275080680847 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997109014229403
  (0, 1680)	3.000000000000002
  (0, 1679)	2.982545801012938
  (0, 1678)	2.99918193029336
  (0, 1677)	2.9659096717325197
  (0, 1676)	2.998611544271013
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.992722254165686
  (0, 1671)	2.983223161673543
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9877700625136745
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9877700625136745
  (0, 1664)	3.000000000000002
  (0, 1663)	2.96750824807021
  (0, 1662)	2.9877700625136745
  (0, 1661)	2.9852645873416734
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.001762958210064
  :	:
  (942, 24)	3.768914682910474
  (942, 23)	3.4641883440962324
  (942, 22)	3.9432007850267627
  (942, 21)	4.065473590544482
  (942, 20)	3.053372638759267
  (942, 19)	3.3536607433113583
  (942, 18)	3.6872921604141227
  (942, 17)	3.185645056358528
  (942, 16)	3.3102287678771227
  (942, 15)	3.3286661664507724
  (942, 14)	3.8616569143344868
  (942, 13)	3.937922861852018
  (942, 12)	3.407856128563764
  (942, 11)	4.178851783143591
  (942, 10)	3.685879869794189
  (942, 9)	3.522488437300631
  (942, 8)	3.925264461429356
  (942, 7)	3.7523383148234664
  (942, 6)	3.815947173736317
  (942, 5)	3.424951059655174
  (942, 4)	3.3106724439883712
  (942, 3)	3.349535150777743
  (942, 2)	3.097120779471308
  (942, 1)	3.1695201681027396
  (942, 0)	3.8930688316533235
this is the 55 epoch
rmse loss on training set is 0.9925452561165986
rmse loss on test set is 1.0091976469096047
for this epoch using 114.34692597389221 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.997036821221106
  (0, 1680)	3.000000000000002
  (0, 1679)	2.982222842353216
  (0, 1678)	2.9991607484401346
  (0, 1677)	2.965284936266295
  (0, 1676)	2.998591668751917
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9924991073654112
  (0, 1671)	2.982927618563684
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.987585667498889
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.987585667498889
  (0, 1664)	3.000000000000002
  (0, 1663)	2.966934653789595
  (0, 1662)	2.987585667498889
  (0, 1661)	2.9850170800461036
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00182712923116
  :	:
  (942, 24)	3.772481233914848
  (942, 23)	3.467777418285628
  (942, 22)	3.951100999631012
  (942, 21)	4.072217856045947
  (942, 20)	3.052118301810958
  (942, 19)	3.357122200251215
  (942, 18)	3.6949798221116157
  (942, 17)	3.1864875972906344
  (942, 16)	3.312706261454551
  (942, 15)	3.331816427202667
  (942, 14)	3.864670594447728
  (942, 13)	3.9441448008727233
  (942, 12)	3.4104564487962077
  (942, 11)	4.186840216990893
  (942, 10)	3.691114484555374
  (942, 9)	3.527960779666756
  (942, 8)	3.9301074347351856
  (942, 7)	3.7573950475643603
  (942, 6)	3.8183316309853224
  (942, 5)	3.429889501473353
  (942, 4)	3.313263801444395
  (942, 3)	3.3520371249185335
  (942, 2)	3.0966395262209288
  (942, 1)	3.1700977394990946
  (942, 0)	3.8947966152170608
this is the 56 epoch
rmse loss on training set is 0.9916411343395457
rmse loss on test set is 1.0084459416627427
for this epoch using 117.23574709892273 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996964615882835
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9818999052141644
  (0, 1678)	2.999139511286964
  (0, 1677)	2.964660299141368
  (0, 1676)	2.998572324554287
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9922741216073394
  (0, 1671)	2.982632634723716
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.987401787115444
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.987401787115444
  (0, 1664)	3.000000000000002
  (0, 1663)	2.966361721980993
  (0, 1662)	2.987401787115444
  (0, 1661)	2.984770037762992
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00189228029127
  :	:
  (942, 24)	3.7759048273674938
  (942, 23)	3.471271114423024
  (942, 22)	3.9588107177982885
  (942, 21)	4.078752699811971
  (942, 20)	3.050837999092782
  (942, 19)	3.3605247805899237
  (942, 18)	3.7025529822712007
  (942, 17)	3.1872830437493715
  (942, 16)	3.31512592039569
  (942, 15)	3.334912616021806
  (942, 14)	3.8675422684062446
  (942, 13)	3.9501756370495795
  (942, 12)	3.4129801081211215
  (942, 11)	4.194584502476545
  (942, 10)	3.696211208921228
  (942, 9)	3.5333429039077875
  (942, 8)	3.934769996177368
  (942, 7)	3.7622987541808866
  (942, 6)	3.820610209919378
  (942, 5)	3.4347703353233556
  (942, 4)	3.315796797261659
  (942, 3)	3.3544758844262184
  (942, 2)	3.096129752762366
  (942, 1)	3.17063513237424
  (942, 0)	3.8964411821586844
this is the 57 epoch
rmse loss on training set is 0.9907660284849209
rmse loss on test set is 1.007720854116152
for this epoch using 112.51977944374084 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996892412593681
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9815769941274963
  (0, 1678)	2.9991182223488617
  (0, 1677)	2.964035765906137
  (0, 1676)	2.998553507563441
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.992047350842619
  (0, 1671)	2.9823382120324187
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9872184001561384
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9872184001561384
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9657894495210817
  (0, 1662)	2.9872184001561384
  (0, 1661)	2.9845234542814905
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0019583953925473
  :	:
  (942, 24)	3.779191527638239
  (942, 23)	3.4746719990057797
  (942, 22)	3.966335138707805
  (942, 21)	4.0850857148718
  (942, 20)	3.049533179590403
  (942, 19)	3.3638699446397795
  (942, 18)	3.7100137984710178
  (942, 17)	3.1880327504332455
  (942, 16)	3.317489404898295
  (942, 15)	3.337956172501146
  (942, 14)	3.870278981727675
  (942, 13)	3.956021674998694
  (942, 12)	3.4154297974589705
  (942, 11)	4.202092695781262
  (942, 10)	3.7011741011237604
  (942, 9)	3.5386366596181333
  (942, 8)	3.93925920824343
  (942, 7)	3.767054564515076
  (942, 6)	3.8227889187078365
  (942, 5)	3.439594880716457
  (942, 4)	3.3182730048737
  (942, 3)	3.3568535786677733
  (942, 2)	3.095592793046387
  (942, 1)	3.1711337364809533
  (942, 0)	3.8980084558268833
this is the 58 epoch
rmse loss on training set is 0.989918524598635
rmse loss on test set is 1.0070210395149832
for this epoch using 118.32842826843262 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9968202248966085
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9812541133708232
  (0, 1678)	2.9990968849460846
  (0, 1677)	2.9634113417955574
  (0, 1676)	2.9985352132245433
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.991818847773397
  (0, 1671)	2.9820443520514925
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.987035486307825
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.987035486307825
  (0, 1664)	3.000000000000002
  (0, 1663)	2.965217833157456
  (0, 1662)	2.987035486307825
  (0, 1661)	2.984277323639504
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0020254584932546
  :	:
  (942, 24)	3.782347137654569
  (942, 23)	3.4779825612988295
  (942, 22)	3.9736793193279483
  (942, 21)	4.091224215766415
  (942, 20)	3.0482052326184834
  (942, 19)	3.367159104158856
  (942, 18)	3.7173643767053783
  (942, 17)	3.1887380271992947
  (942, 16)	3.3197983190664857
  (942, 15)	3.3409484871954613
  (942, 14)	3.8728874220498035
  (942, 13)	3.9616890055819582
  (942, 12)	3.417808103028008
  (942, 11)	4.209372593040235
  (942, 10)	3.706007094133995
  (942, 9)	3.543843846934182
  (942, 8)	3.9435818544082406
  (942, 7)	3.771667437662335
  (942, 6)	3.8248733956925243
  (942, 5)	3.4443644140851553
  (942, 4)	3.320693947359345
  (942, 3)	3.3591722759782563
  (942, 2)	3.0950299247099866
  (942, 1)	3.171594890505513
  (942, 0)	3.899503914650461
this is the 59 epoch
rmse loss on training set is 0.9890972953316145
rmse loss on test set is 1.0063452361579932
for this epoch using 112.5801911354065 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9967480655376746
  (0, 1680)	3.000000000000002
  (0, 1679)	2.980931266979448
  (0, 1678)	2.9990755022131514
  (0, 1677)	2.9627870317457434
  (0, 1676)	2.9985174365870724
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9915886638743836
  (0, 1671)	2.981751056042812
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.986853026113789
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.986853026113789
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9646468695191457
  (0, 1662)	2.986853026113789
  (0, 1661)	2.9840316401113403
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0020934535299975
  :	:
  (942, 24)	3.7853772096874954
  (942, 23)	3.481205215926721
  (942, 22)	3.9808481776143214
  (942, 21)	4.097175247624275
  (942, 20)	3.046855490317379
  (942, 19)	3.370393624230677
  (942, 18)	3.7246067728081083
  (942, 17)	3.189400140744404
  (942, 16)	3.322054213009078
  (942, 15)	3.3438909035096662
  (942, 14)	3.875373936682508
  (942, 13)	3.9671835127542017
  (942, 12)	3.420117510537613
  (942, 11)	4.216431737481113
  (942, 10)	3.710713999347905
  (942, 9)	3.54896621819138
  (942, 8)	3.9477444495761738
  (942, 7)	3.7761421669818946
  (942, 6)	3.8268689317134466
  (942, 5)	3.449080170438503
  (942, 4)	3.323061099271585
  (942, 3)	3.361433966772471
  (942, 2)	3.0944423716093854
  (942, 1)	3.1720198841379794
  (942, 0)	3.900932623832559
this is the 60 epoch
rmse loss on training set is 0.9883010938614817
rmse loss on test set is 1.0056922596167557
for this epoch using 130.99165177345276 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9966759465035406
  (0, 1680)	3.000000000000002
  (0, 1679)	2.980608458757687
  (0, 1678)	2.9990540771074086
  (0, 1677)	2.962162840407963
  (0, 1676)	2.998500172346395
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9913568494145766
  (0, 1671)	2.9814583249847475
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.986671000937769
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.986671000937769
  (0, 1664)	3.000000000000002
  (0, 1663)	2.96407655512647
  (0, 1662)	2.986671000937769
  (0, 1661)	2.9837863981960107
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0021623644384894
  :	:
  (942, 24)	3.7882870557572432
  (942, 23)	3.4843423053815497
  (942, 22)	3.987846495716191
  (942, 21)	4.102945595074246
  (942, 20)	3.045485230047394
  (942, 19)	3.3735748250730553
  (942, 18)	3.7317429938447315
  (942, 17)	3.19002031622435
  (942, 16)	3.324258584861052
  (942, 15)	3.3467847195171503
  (942, 14)	3.8777445493926646
  (942, 13)	3.972510880259777
  (942, 12)	3.4223604092284106
  (942, 11)	4.223277426500102
  (942, 10)	3.7152985102026084
  (942, 9)	3.554005479524027
  (942, 8)	3.951753250206204
  (942, 7)	3.7804833850353385
  (942, 6)	3.8287804911842236
  (942, 5)	3.453743344955362
  (942, 4)	3.325375888399475
  (942, 3)	3.363640566550691
  (942, 2)	3.09383130623837
  (942, 1)	3.1724099600580296
  (942, 0)	3.902299264968389
this is the 61 epoch
rmse loss on training set is 0.9875287482825236
rmse loss on test set is 1.0050609973916318
for this epoch using 113.13542151451111 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996603879057535
  (0, 1680)	3.000000000000002
  (0, 1679)	2.980285692289593
  (0, 1678)	2.9990326124172144
  (0, 1677)	2.9615387721619717
  (0, 1676)	2.998483414882351
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9911234534790556
  (0, 1671)	2.981166159587717
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9864893929294185
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9864893929294185
  (0, 1664)	3.000000000000002
  (0, 1663)	2.963506886400237
  (0, 1662)	2.9864893929294185
  (0, 1661)	2.9835415926060773
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002232175172814
  :	:
  (942, 24)	3.79108175766487
  (942, 23)	3.4873961024474824
  (942, 22)	3.99467892318135
  (942, 21)	4.1085417909835265
  (942, 20)	3.0440956766836504
  (942, 19)	3.376703983777539
  (942, 18)	3.738774999472222
  (942, 17)	3.1905997388115943
  (942, 16)	3.326412882730006
  (942, 15)	3.349631189709407
  (942, 14)	3.8800049764449605
  (942, 13)	3.9776765981730566
  (942, 12)	3.4245390957619963
  (942, 11)	4.229916718661246
  (942, 10)	3.719764205718978
  (942, 9)	3.5589632924087473
  (942, 8)	3.9556142641212984
  (942, 7)	3.784695568445805
  (942, 6)	3.8306127319766508
  (942, 5)	3.458355094516635
  (942, 4)	3.3276396974647686
  (942, 3)	3.3657939187998087
  (942, 2)	3.093197852036195
  (942, 1)	3.172766315838847
  (942, 0)	3.9036081637054467
this is the 62 epoch
rmse loss on training set is 0.986779156425042
rmse loss on test set is 1.0044504039692923
for this epoch using 110.77248406410217 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996531873774139
  (0, 1680)	3.000000000000002
  (0, 1679)	2.979962970949253
  (0, 1678)	2.99901111076981
  (0, 1677)	2.960914831128703
  (0, 1676)	2.9984671582952367
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9908885239908374
  (0, 1671)	2.9808745603089397
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.986308184991178
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.986308184991178
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9629378596704785
  (0, 1662)	2.986308184991178
  (0, 1661)	2.983297218257099
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002302869723324
  :	:
  (942, 24)	3.7937661766569657
  (942, 23)	3.490368812543615
  (942, 22)	4.001349980150739
  (942, 21)	4.113970125010752
  (942, 20)	3.042688004814879
  (942, 19)	3.379782335981168
  (942, 18)	3.7457047032657385
  (942, 17)	3.191139555193596
  (942, 16)	3.3285185065693828
  (942, 15)	3.352431526678566
  (942, 14)	3.8821606419221735
  (942, 13)	3.9826859692791823
  (942, 12)	3.426655777963816
  (942, 11)	4.236356440605931
  (942, 10)	3.7241145539681217
  (942, 9)	3.5638412751526065
  (942, 8)	3.9593332600046147
  (942, 7)	3.788783042671322
  (942, 6)	3.8323700241722265
  (942, 5)	3.462916539178074
  (942, 4)	3.3298538657549663
  (942, 3)	3.3678957977918884
  (942, 2)	3.0925430855894596
  (942, 1)	3.1730901057720713
  (942, 0)	3.9048633155601045
this is the 63 epoch
rmse loss on training set is 0.986051281068583
rmse loss on test set is 1.0038594962496603
for this epoch using 116.42238974571228 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996459940572073
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9796402979105734
  (0, 1678)	2.998989574638731
  (0, 1677)	2.960291021182411
  (0, 1676)	2.9984513964391786
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.990652107732677
  (0, 1671)	2.9805835273664756
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.986127360746593
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.986127360746593
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9623694711845356
  (0, 1662)	2.986127360746593
  (0, 1661)	2.9830532702576225
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002374432133298
  :	:
  (942, 24)	3.796344962731157
  (942, 23)	3.4932625759864337
  (942, 22)	4.007864060534904
  (942, 21)	4.119236651966009
  (942, 20)	3.041263340849219
  (942, 19)	3.3828110774723315
  (942, 18)	3.7525339740113623
  (942, 17)	3.191640875013046
  (942, 16)	3.3305768099807236
  (942, 15)	3.355186902734592
  (942, 14)	3.884216692348934
  (942, 13)	3.9875441152920303
  (942, 12)	3.4287125784227594
  (942, 11)	4.242603193861384
  (942, 10)	3.7283529154594315
  (942, 9)	3.5686410043272336
  (942, 8)	3.962915776586069
  (942, 7)	3.792749986687311
  (942, 6)	3.834056467736329
  (942, 5)	3.4674287635850582
  (942, 4)	3.3320196906946165
  (942, 3)	3.3699479112818627
  (942, 2)	3.0918680387320983
  (942, 1)	3.1733824426163193
  (942, 0)	3.90606840999959
this is the 64 epoch
rmse loss on training set is 0.9853441455170246
rmse loss on test set is 1.0032873493127965
for this epoch using 109.18233966827393 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9963880887459444
  (0, 1680)	3.000000000000002
  (0, 1679)	2.979317676156608
  (0, 1678)	2.9989680063509754
  (0, 1677)	2.9596673459622473
  (0, 1676)	2.9984361229531626
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9904142503688544
  (0, 1671)	2.9802930607525373
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9859469045098392
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9859469045098392
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9618017171147106
  (0, 1662)	2.9859469045098392
  (0, 1661)	2.9828097438996375
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0024468465142733
  :	:
  (942, 24)	3.7988225635905635
  (942, 23)	3.496079470173717
  (942, 22)	4.014225435165632
  (942, 21)	4.124347199971655
  (942, 20)	3.039822765030474
  (942, 19)	3.3857913657326204
  (942, 18)	3.759264636964921
  (942, 17)	3.192104772251714
  (942, 16)	3.3325891019468417
  (942, 15)	3.3578984514588566
  (942, 14)	3.8861780106430603
  (942, 13)	3.9922559829076834
  (942, 12)	3.4307115379512694
  (942, 11)	4.248663361538768
  (942, 10)	3.732482546448573
  (942, 9)	3.573364016150314
  (942, 8)	3.966367131523406
  (942, 7)	3.7966004375735984
  (942, 6)	3.835675909169529
  (942, 5)	3.4718928183309776
  (942, 4)	3.334138429356814
  (942, 3)	3.3719519031066834
  (942, 2)	3.0911737005478983
  (942, 1)	3.1736443992720815
  (942, 0)	3.9072268528933303
this is the 65 epoch
rmse loss on training set is 0.9846568295058283
rmse loss on test set is 1.0027330924987863
for this epoch using 110.98800158500671 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996316326996558
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9789951084885375
  (0, 1678)	2.9989464080937926
  (0, 1677)	2.959043808883284
  (0, 1676)	2.9984213312897743
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9901749964669153
  (0, 1671)	2.980003160246181
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9857668012566245
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9857668012566245
  (0, 1664)	3.000000000000002
  (0, 1663)	2.96123459356541
  (0, 1662)	2.9857668012566245
  (0, 1661)	2.9825666346495745
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002520097060241
  :	:
  (942, 24)	3.8012032332560612
  (942, 23)	3.4988215116918058
  (942, 22)	4.020438254916827
  (942, 21)	4.129307378418795
  (942, 20)	3.038367313367975
  (942, 19)	3.388724321416608
  (942, 18)	3.765898475076514
  (942, 17)	3.1925322865598083
  (942, 16)	3.3345566484981375
  (942, 15)	3.3605672691960247
  (942, 14)	3.8880492294188325
  (942, 13)	3.996826349692567
  (942, 12)	3.4326546189099423
  (942, 11)	4.254543114912688
  (942, 10)	3.736506602164312
  (942, 9)	3.5780118078157384
  (942, 8)	3.969692429982818
  (942, 7)	3.8003382950027036
  (942, 6)	3.8372319571882367
  (942, 5)	3.476309721260922
  (942, 4)	3.336211299916614
  (942, 3)	3.3739093556883097
  (942, 2)	3.090461019279485
  (942, 1)	3.1738770103858047
  (942, 0)	3.908341787432979
this is the 66 epoch
rmse loss on training set is 0.9839884654147273
rmse loss on test set is 1.0021959057759162
for this epoch using 112.55552411079407 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9962446634599367
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9786725975341337
  (0, 1678)	2.9989247819211866
  (0, 1677)	2.9584204131470835
  (0, 1676)	2.998407014741862
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.989934389519267
  (0, 1671)	2.9797138254253226
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.985587036596245
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.985587036596245
  (0, 1664)	3.000000000000002
  (0, 1663)	2.960668096579859
  (0, 1662)	2.985587036596245
  (0, 1661)	2.98232393813971
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0025941680607295
  :	:
  (942, 24)	3.8034910403451856
  (942, 23)	3.501490658348145
  (942, 22)	4.026506553789315
  (942, 21)	4.134122585716209
  (942, 20)	3.0368979794833817
  (942, 19)	3.3916110297715854
  (942, 18)	3.772437230180885
  (942, 17)	3.192924424532367
  (942, 16)	3.3364806743142457
  (942, 15)	3.3631944164862255
  (942, 14)	3.8898347436661824
  (942, 13)	4.001259829805994
  (942, 12)	3.4345437084006165
  (942, 11)	4.260248419875603
  (942, 10)	3.740428139953545
  (942, 9)	3.5825858387741123
  (942, 8)	3.972896572924517
  (942, 7)	3.8039673256265516
  (942, 6)	3.8387279974851682
  (942, 5)	3.48068045872254
  (942, 4)	3.338239483048539
  (942, 3)	3.3758217924428666
  (942, 2)	3.089730904148012
  (942, 1)	3.1740812738857413
  (942, 0)	3.909416113615265
this is the 67 epoch
rmse loss on training set is 0.9833382347612439
rmse loss on test set is 1.0016750163744135
for this epoch using 109.64375948905945 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9961731057350764
  (0, 1680)	3.000000000000002
  (0, 1679)	2.978350145755921
  (0, 1678)	2.998903129760128
  (0, 1677)	2.9577971617517145
  (0, 1676)	2.9983931664672165
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.989692471964693
  (0, 1671)	2.979425055678166
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9854075967448086
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9854075967448086
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9601022221463986
  (0, 1662)	2.9854075967448086
  (0, 1661)	2.9820816501600347
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0026690439127965
  :	:
  (942, 24)	3.8056898760269684
  (942, 23)	3.504088811131247
  (942, 22)	4.032434251955296
  (942, 21)	4.138798016829165
  (942, 20)	3.035415716377577
  (942, 19)	3.3944525419992617
  (942, 18)	3.7788826041538894
  (942, 17)	3.1932821609346855
  (942, 16)	3.338362364263043
  (942, 15)	3.3657809194393757
  (942, 14)	3.8915387228299
  (942, 13)	4.005560879557563
  (942, 12)	3.4363806213320696
  (942, 11)	4.265785043261477
  (942, 10)	3.7442501223441833
  (942, 9)	3.587087531965038
  (942, 8)	3.975984265099247
  (942, 7)	3.8074911673595446
  (942, 6)	3.840167206618044
  (942, 5)	3.4850059867655534
  (942, 4)	3.3402241232698255
  (942, 3)	3.3776906800985658
  (942, 2)	3.0889842270872836
  (942, 1)	3.174258152452331
  (942, 0)	3.910452506377205
this is the 68 epoch
rmse loss on training set is 0.9827053649525608
rmse loss on test set is 1.0011696956651432
for this epoch using 115.46550726890564 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.996101660910468
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9780277554589394
  (0, 1678)	2.998881453416469
  (0, 1677)	2.957174057501405
  (0, 1676)	2.9983797795113385
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9894492852096497
  (0, 1671)	2.9791368502140996
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.985228468499577
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.985228468499577
  (0, 1664)	3.000000000000002
  (0, 1663)	2.959536966204383
  (0, 1662)	2.985228468499577
  (0, 1661)	2.9818397666504755
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0027447091320285
  :	:
  (942, 24)	3.807803461662205
  (942, 23)	3.5066178161001598
  (942, 22)	4.038225158758447
  (942, 21)	4.143338670607099
  (942, 20)	3.0339214381209634
  (942, 19)	3.3972498765615082
  (942, 18)	3.7852362600353464
  (942, 17)	3.1936064398784727
  (942, 16)	3.3402028648793274
  (942, 15)	3.368327771053757
  (942, 14)	3.893165122312236
  (942, 13)	4.0097338028006035
  (942, 12)	3.4381671033625265
  (942, 11)	4.271158559034373
  (942, 10)	3.7479754200258895
  (942, 9)	3.5915182750026813
  (942, 8)	3.978960022762185
  (942, 7)	3.8109133335566145
  (942, 6)	3.841552565073236
  (942, 5)	3.489287232292069
  (942, 4)	3.342166330231528
  (942, 3)	3.3795174309251053
  (942, 2)	3.088221824396392
  (942, 1)	3.1744085749256192
  (942, 0)	3.911453432468616
this is the 69 epoch
rmse loss on training set is 0.9820891262752991
rmse loss on test set is 1.0006792562639966
for this epoch using 116.45069885253906 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9960303355895483
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9777054287981404
  (0, 1678)	2.998859754580613
  (0, 1677)	2.9565511030156704
  (0, 1676)	2.998366846828526
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9892048696494884
  (0, 1671)	2.9788492080739855
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9850496392144215
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9850496392144215
  (0, 1664)	3.000000000000002
  (0, 1663)	2.958972324649721
  (0, 1662)	2.9850496392144215
  (0, 1661)	2.981598283693598
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002821148362633
  :	:
  (942, 24)	3.8098353561384757
  (942, 23)	3.5090794662055775
  (942, 22)	4.043882975666559
  (942, 21)	4.1477493568996335
  (942, 20)	3.03241602147007
  (942, 19)	3.4000040204320143
  (942, 18)	3.791499823118721
  (942, 17)	3.1938981759505327
  (942, 16)	3.342003285785155
  (942, 15)	3.370835932480693
  (942, 14)	3.8947176944224435
  (942, 13)	4.013782756163215
  (942, 12)	3.439904833722956
  (942, 11)	4.276374354338404
  (942, 10)	3.751606814749059
  (942, 9)	3.5958794213164045
  (942, 8)	3.981828181110546
  (942, 7)	3.8142372170851755
  (942, 6)	3.842886869548677
  (942, 5)	3.4935250941591662
  (942, 4)	3.344067179959203
  (942, 3)	3.3813034048770136
  (942, 2)	3.0874444983142912
  (942, 1)	3.1745334376523284
  (942, 0)	3.912421166142237
this is the 70 epoch
rmse loss on training set is 0.9814888291042553
rmse loss on test set is 1.0002030493447056
for this epoch using 119.66891384124756 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995959135914889
  (0, 1680)	3.000000000000002
  (0, 1679)	2.977383167785502
  (0, 1678)	2.9988380348329238
  (0, 1677)	2.955928300738077
  (0, 1676)	2.9983543613012924
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9889592646894063
  (0, 1671)	2.978562128139976
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9848710967762764
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9848710967762764
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9584082933400495
  (0, 1662)	2.9848710967762764
  (0, 1661)	2.9813571975075686
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.002898346386538
  :	:
  (942, 24)	3.811788962909684
  (942, 23)	3.5114755030448253
  (942, 22)	4.04941129917384
  (942, 21)	4.152034703461589
  (942, 20)	3.030900307413686
  (942, 19)	3.402715930296176
  (942, 18)	3.797674882008218
  (942, 17)	3.1941582552957644
  (942, 16)	3.343764701054041
  (942, 15)	3.3733063342374408
  (942, 14)	3.896199998795758
  (942, 13)	4.0177117541187535
  (942, 12)	3.4415954279254573
  (942, 11)	4.281437635406605
  (942, 10)	3.755147002142594
  (942, 9)	3.6001722912477887
  (942, 8)	3.984592901452216
  (942, 7)	3.8174660942905896
  (942, 6)	3.8441727444989318
  (942, 5)	3.497720444235732
  (942, 4)	3.34592771604511
  (942, 3)	3.383049911653784
  (942, 2)	3.0866530185201424
  (942, 1)	3.174633605775019
  (942, 0)	3.9133578037374055
this is the 71 epoch
rmse loss on training set is 0.980903821313088
rmse loss on test set is 0.9997404621437961
for this epoch using 117.77635717391968 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9958880675914785
  (0, 1680)	3.000000000000002
  (0, 1679)	2.977060974296751
  (0, 1678)	2.998816295648887
  (0, 1677)	2.955305652944616
  (0, 1676)	2.998342315758262
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.988712508765207
  (0, 1671)	2.9782756091448386
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9846928295825585
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9846928295825585
  (0, 1664)	3.000000000000002
  (0, 1663)	2.957844868099631
  (0, 1662)	2.9846928295825585
  (0, 1661)	2.9811165044395733
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0029762881317477
  :	:
  (942, 24)	3.8136675367495214
  (942, 23)	3.5138076185529115
  (942, 22)	4.054813623650631
  (942, 21)	4.15619916264809
  (942, 20)	3.0293751026513775
  (942, 19)	3.4053865337008333
  (942, 18)	3.8037629896438436
  (942, 17)	3.1943875366562184
  (942, 16)	3.3454881505210916
  (942, 15)	3.3757398773700538
  (942, 14)	3.8976154123042672
  (942, 13)	4.021524673898251
  (942, 12)	3.4432404403606416
  (942, 11)	4.28635343332675
  (942, 10)	3.758598594451225
  (942, 9)	3.6043981731058943
  (942, 8)	3.987258178112091
  (942, 7)	3.820603128855198
  (942, 6)	3.8454126529830837
  (942, 5)	3.501874128415226
  (942, 4)	3.3477489507938003
  (942, 3)	3.38475821267934
  (942, 2)	3.0858481235627235
  (942, 1)	3.1747099144658226
  (942, 0)	3.9142652772292186
this is the 72 epoch
rmse loss on training set is 0.9803334858709583
rmse loss on test set is 0.9992909156431352
for this epoch using 122.08425641059875 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9958171359088164
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9767388500778384
  (0, 1678)	2.9987945384040384
  (0, 1677)	2.954683161751646
  (0, 1676)	2.998330702990604
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.988464639363864
  (0, 1671)	2.9779896496808274
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9845148265195816
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9845148265195816
  (0, 1664)	3.000000000000002
  (0, 1663)	2.957282044723873
  (0, 1662)	2.9845148265195816
  (0, 1661)	2.980876200959461
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0030549586798574
  :	:
  (942, 24)	3.815474190228369
  (942, 23)	3.516077456631716
  (942, 22)	4.060093344138754
  (942, 21)	4.160247017901673
  (942, 20)	3.0278411810072954
  (942, 19)	3.4080167301560667
  (942, 18)	3.8097656642950333
  (942, 17)	3.194586852367946
  (942, 16)	3.3471746410410157
  (942, 15)	3.378137434568356
  (942, 14)	3.898967138481281
  (942, 13)	4.025225260247328
  (942, 12)	3.4448413667882156
  (942, 11)	4.291126609662957
  (942, 10)	3.7619641231933794
  (942, 9)	3.6085583241819656
  (942, 8)	3.9898278450834153
  (942, 7)	3.823651375550946
  (942, 6)	3.8466089068542195
  (942, 5)	3.505986967586023
  (942, 4)	3.3495318663227462
  (942, 3)	3.386429523003676
  (942, 2)	3.0850305222222705
  (942, 1)	3.1747631701069796
  (942, 0)	3.915145366810569
this is the 73 epoch
rmse loss on training set is 0.9797772386108716
rmse loss on test set is 0.9988538624164188
for this epoch using 115.10212278366089 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995746345762133
  (0, 1680)	3.000000000000002
  (0, 1679)	2.976416796751134
  (0, 1678)	2.998772764378706
  (0, 1677)	2.9540608291235517
  (0, 1676)	2.9983195157671743
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9882156930438177
  (0, 1671)	2.9777042482080467
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9843370769418622
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9843370769418622
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9567198189836215
  (0, 1662)	2.9843370769418622
  (0, 1661)	2.9806362836538076
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0031343432728024
  :	:
  (942, 24)	3.8172118999231897
  (942, 23)	3.518286614719514
  (942, 22)	4.065253759090859
  (942, 21)	4.164182390033667
  (942, 20)	3.0262992847820427
  (942, 19)	3.4106073921908835
  (942, 18)	3.8156843905237268
  (942, 17)	3.194757009317233
  (942, 16)	3.3488251476961
  (942, 15)	3.38049985123468
  (942, 14)	3.9002582164804167
  (942, 13)	4.0288171300303866
  (942, 12)	3.4463996467244846
  (942, 11)	4.295761861932651
  (942, 10)	3.765246041740705
  (942, 9)	3.612653971725453
  (942, 8)	3.9923055824314266
  (942, 7)	3.8266137838864847
  (942, 6)	3.8477636763276912
  (942, 5)	3.510059758561284
  (942, 4)	3.3512774156198084
  (942, 3)	3.3880650131291383
  (942, 2)	3.08420089480797
  (942, 1)	3.1747941514205475
  (942, 0)	3.9159997125711725
this is the 74 epoch
rmse loss on training set is 0.9792345261563143
rmse loss on test set is 0.9984287846271578
for this epoch using 117.67213559150696 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9956757016725994
  (0, 1680)	3.000000000000002
  (0, 1679)	2.976094815821256
  (0, 1678)	2.9987509747624923
  (0, 1677)	2.953438656880024
  (0, 1676)	2.9983087468483642
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9879657054550384
  (0, 1671)	2.9774194030625094
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9841595706522765
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9841595706522765
  (0, 1664)	3.000000000000002
  (0, 1663)	2.956158186629178
  (0, 1662)	2.9841595706522765
  (0, 1661)	2.980396749220164
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0032144273189383
  :	:
  (942, 24)	3.818883512369498
  (942, 23)	3.5204366453030613
  (942, 22)	4.070298073052653
  (942, 21)	4.168009243302812
  (942, 20)	3.0247501260453076
  (942, 19)	3.4131593663646376
  (942, 18)	3.821520620117379
  (942, 17)	3.194898789857997
  (942, 16)	3.3504406149560566
  (942, 15)	3.382827946508387
  (942, 14)	3.901491529589783
  (942, 13)	4.032303776685485
  (942, 12)	3.447916665730942
  (942, 11)	4.300263728938686
  (942, 10)	3.768446727820594
  (942, 9)	3.616686313882583
  (942, 8)	3.9946949224565156
  (942, 7)	3.8294932016494907
  (942, 6)	3.848878998963384
  (942, 5)	3.5140932749695892
  (942, 4)	3.352986523559303
  (942, 3)	3.389665810764104
  (942, 2)	3.0833598943941425
  (942, 1)	3.174803610549469
  (942, 0)	3.916829825333439
this is the 75 epoch
rmse loss on training set is 0.978704823994111
rmse loss on test set is 0.9980151921667009
for this epoch using 117.23831224441528 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995605207806638
  (0, 1680)	3.000000000000002
  (0, 1679)	2.97577290868077
  (0, 1678)	2.9987291706585757
  (0, 1677)	2.952816646702972
  (0, 1676)	2.9982983889987675
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.987714711358815
  (0, 1671)	2.9771351124636856
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9839822978830797
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9839822978830797
  (0, 1664)	3.000000000000002
  (0, 1663)	2.955597143394047
  (0, 1662)	2.9839822978830797
  (0, 1661)	2.980157594461691
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0032951963984127
  :	:
  (942, 24)	3.820491749764822
  (942, 23)	3.522529057374068
  (942, 22)	4.075229399287235
  (942, 21)	4.171731391294207
  (942, 20)	3.0231943878718495
  (942, 19)	3.4156734742359913
  (942, 18)	3.8272757729929343
  (942, 17)	3.195012952691822
  (942, 16)	3.3520219577915573
  (942, 15)	3.385122514247791
  (942, 14)	3.902669813321297
  (942, 13)	4.03568857453263
  (942, 12)	3.449393757607509
  (942, 11)	4.304636595957251
  (942, 10)	3.771568485942932
  (942, 9)	3.620656520599136
  (942, 8)	3.9969992556241105
  (942, 7)	3.832292378345441
  (942, 6)	3.8499567880954144
  (942, 5)	3.5180882681084493
  (942, 4)	3.3546600878781767
  (942, 3)	3.3912330025066266
  (942, 2)	3.0825081479980354
  (942, 1)	3.1747922740919625
  (942, 0)	3.917637096701721
this is the 76 epoch
rmse loss on training set is 0.9781876346823049
rmse loss on test set is 0.9976126209217955
for this epoch using 114.35751056671143 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9955348679943596
  (0, 1680)	3.000000000000002
  (0, 1679)	2.975451076615537
  (0, 1678)	2.9987073530878487
  (0, 1677)	2.9521948001432223
  (0, 1676)	2.998288434998824
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9874627446472886
  (0, 1671)	2.9768513745217478
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.983805249277725
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.983805249277725
  (0, 1664)	3.000000000000002
  (0, 1663)	2.955036684998436
  (0, 1662)	2.983805249277725
  (0, 1661)	2.979918816282006
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.003376636267981
  :	:
  (942, 24)	3.8220392154324916
  (942, 23)	3.5245653178323417
  (942, 22)	4.080050762340838
  (942, 21)	4.175352502602154
  (942, 20)	3.021632725523358
  (942, 19)	3.41815051329124
  (942, 18)	3.832951238072345
  (942, 17)	3.1951002337121728
  (942, 16)	3.35357006274344
  (942, 15)	3.3873843239714145
  (942, 14)	3.9037956630942436
  (942, 13)	4.038974782939484
  (942, 12)	3.450832206494242
  (942, 11)	4.3088846997822285
  (942, 10)	3.774613549752698
  (942, 9)	3.624565734488707
  (942, 8)	3.9992218362687972
  (942, 7)	3.8350139685341174
  (942, 6)	3.8509988407412328
  (942, 5)	3.5220454677617745
  (942, 4)	3.35629898011407
  (942, 3)	3.3927676354605016
  (942, 2)	3.0816462577021104
  (942, 1)	3.1747608440915194
  (942, 0)	3.918422808378065
this is the 77 epoch
rmse loss on training set is 0.977682486182732
rmse loss on test set is 0.9972206311619758
for this epoch using 111.42512941360474 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995464685747164
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9751293208098333
  (0, 1678)	2.998685522992844
  (0, 1677)	2.951573118626831
  (0, 1676)	2.998278877655321
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.987209838362668
  (0, 1671)	2.9765681872444105
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.983628415873429
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.983628415873429
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9544768071525658
  (0, 1662)	2.983628415873429
  (0, 1661)	2.9796804116802704
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0034587328651843
  :	:
  (942, 24)	3.823528399054599
  (942, 23)	3.526546852837493
  (942, 22)	4.084765100549619
  (942, 21)	4.178876106320856
  (942, 20)	3.0200657675785227
  (942, 19)	3.420591257833629
  (942, 18)	3.838548374130402
  (942, 17)	3.1951613468143907
  (942, 16)	3.3550857889491907
  (942, 15)	3.389614121760041
  (942, 14)	3.9048715415316297
  (942, 13)	4.042165550347293
  (942, 12)	3.452233248885061
  (942, 11)	4.313012133627378
  (942, 10)	3.7775840843097934
  (942, 9)	3.628415071667867
  (942, 8)	4.001365788079543
  (942, 7)	3.837660535065311
  (942, 6)	3.852006845020252
  (942, 5)	3.5259655829831322
  (942, 4)	3.3579040465065306
  (942, 3)	3.3942707187863914
  (942, 2)	3.0807748017234164
  (942, 1)	3.174709998984261
  (942, 0)	3.919188140794304
this is the 78 epoch
rmse loss on training set is 0.9771889303090041
rmse loss on test set is 0.9968388060378033
for this epoch using 111.57871985435486 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995394664274526
  (0, 1680)	3.000000000000002
  (0, 1679)	2.974807642351311
  (0, 1678)	2.998663681241485
  (0, 1677)	2.9509516034611343
  (0, 1676)	2.998269709811076
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.986956024716236
  (0, 1671)	2.9762855485434776
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.983451789084495
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.983451789084495
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9539175055597395
  (0, 1662)	2.983451789084495
  (0, 1661)	2.979442377746588
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.003541472311995
  :	:
  (942, 24)	3.8249616816829595
  (942, 23)	3.5284750491110843
  (942, 22)	4.089375268487404
  (942, 21)	4.182305597346831
  (942, 20)	3.01849411701382
  (942, 19)	3.4229964598354097
  (942, 18)	3.844068510615956
  (942, 17)	3.1951969846727946
  (942, 16)	3.3565699691285205
  (942, 15)	3.391812631121403
  (942, 14)	3.9058997853874025
  (942, 13)	4.045263918161245
  (942, 12)	3.4535980755570788
  (942, 11)	4.3170228518877956
  (942, 10)	3.780482188297901
  (942, 9)	3.6322056225596584
  (942, 8)	4.003434109373366
  (942, 7)	3.840234552215581
  (942, 6)	3.852982387110764
  (942, 5)	3.529849302846239
  (942, 4)	3.3594761088632024
  (942, 3)	3.3957432251902957
  (942, 2)	3.0798943354326687
  (942, 1)	3.174640394505619
  (942, 0)	3.9199341811072044
this is the 79 epoch
rmse loss on training set is 0.9767065412811058
rmse loss on test set is 0.9964667501818182
for this epoch using 109.35238337516785 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9953248065000144
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9744860422356454
  (0, 1678)	2.9986418286306784
  (0, 1677)	2.9503302558406093
  (0, 1676)	2.998260924353617
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9867013351069835
  (0, 1671)	2.976003456241007
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9832753606863287
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9832753606863287
  (0, 1664)	3.000000000000002
  (0, 1663)	2.953358775919261
  (0, 1662)	2.9832753606863287
  (0, 1661)	2.979204711657536
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.003624840917968
  :	:
  (942, 24)	3.8263413405361875
  (942, 23)	3.5303512551911385
  (942, 22)	4.093884039354441
  (942, 21)	4.185644241497482
  (942, 20)	3.0169183522369707
  (942, 19)	3.425366849754081
  (942, 18)	3.8495129484469106
  (942, 17)	3.1952078194863436
  (942, 16)	3.3580234105297073
  (942, 15)	3.39398055381896
  (942, 14)	3.906882612121536
  (942, 13)	4.048272824508263
  (942, 12)	3.454927833418831
  (942, 11)	4.320920674762401
  (942, 10)	3.7833098961637988
  (942, 9)	3.6359384526666374
  (942, 8)	4.005429678164502
  (942, 7)	3.842738408727664
  (942, 6)	3.853926957772225
  (942, 5)	3.5336972971638643
  (942, 4)	3.361015965392193
  (942, 3)	3.3971860923518387
  (942, 2)	3.0790053923255347
  (942, 1)	3.174552664558172
  (942, 0)	3.9206619306006276
this is the 80 epoch
rmse loss on training set is 0.9762349143786937
rmse loss on test set is 0.9961040884045724
for this epoch using 112.76907563209534 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995255115076561
  (0, 1680)	3.000000000000002
  (0, 1679)	2.974164521371048
  (0, 1678)	2.9986199658897776
  (0, 1677)	2.949709076852319
  (0, 1676)	2.998252514223093
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9864458001399825
  (0, 1671)	2.9757219080752244
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9830991228001476
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9830991228001476
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9528006139290794
  (0, 1662)	2.9830991228001476
  (0, 1661)	2.978967410672027
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0037088251828976
  :	:
  (942, 24)	3.827669553591394
  (942, 23)	3.532176782640971
  (942, 22)	4.098294107307238
  (942, 21)	4.188895180450027
  (942, 20)	3.0153390280754495
  (942, 19)	3.4277031373145754
  (942, 18)	3.854882960780182
  (942, 17)	3.195194503694186
  (942, 16)	3.3594468958382313
  (942, 15)	3.396118570666327
  (942, 14)	3.9078221261398207
  (942, 13)	4.051195107866358
  (942, 12)	3.4562236272807527
  (942, 11)	4.324709292739628
  (942, 10)	3.786069180188917
  (942, 9)	3.639614603314767
  (942, 8)	4.007355257035729
  (942, 7)	3.8451744107545647
  (942, 6)	3.854841958458642
  (942, 5)	3.537510217176825
  (942, 4)	3.3625243915020357
  (942, 3)	3.3986002242946296
  (942, 2)	3.078108484948385
  (942, 1)	3.174447422042308
  (942, 0)	3.9213723115356913
this is the 81 epoch
rmse loss on training set is 0.9757736646857005
rmse loss on test set is 0.9957504644787597
for this epoch using 113.90005326271057 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9951855924010555
  (0, 1680)	3.000000000000002
  (0, 1679)	2.973843080582557
  (0, 1678)	2.9985980936838357
  (0, 1677)	2.949088067481281
  (0, 1676)	2.9982444724194233
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.986189449644476
  (0, 1671)	2.9754409017060888
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9829230678783
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9829230678783
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9522430152883787
  (0, 1662)	2.9829230678783
  (0, 1661)	2.9787304721272463
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0037934117990366
  :	:
  (942, 24)	3.8289484039783437
  (942, 23)	3.5339529072138713
  (942, 22)	4.1026080897299995
  (942, 21)	4.192061436505462
  (942, 20)	3.0137566767219295
  (942, 19)	3.4300060122586036
  (942, 18)	3.8601797937570566
  (942, 17)	3.195157670662408
  (942, 16)	3.3608411840493564
  (942, 15)	3.398227342288807
  (942, 14)	3.908720324714171
  (942, 13)	4.0540335105687655
  (942, 12)	3.457486521551128
  (942, 11)	4.328392270948324
  (942, 10)	3.788761952494935
  (942, 9)	3.6432350923692574
  (942, 8)	4.009213497819085
  (942, 7)	3.8475447847102093
  (942, 6)	3.85572870704744
  (942, 5)	3.541288696214145
  (942, 4)	3.3640021405707152
  (942, 3)	3.399986492701011
  (942, 2)	3.0772041057807464
  (942, 1)	3.1743252596514666
  (942, 0)	3.9220661734875106
this is the 82 epoch
rmse loss on training set is 0.9753224259196457
rmse loss on test set is 0.9954055400050711
for this epoch using 109.15969634056091 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.995116240628219
  (0, 1680)	3.000000000000002
  (0, 1679)	2.973521720616118
  (0, 1678)	2.9985762126167588
  (0, 1677)	2.9484672286154816
  (0, 1676)	2.998236792008679
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.98593231269159
  (0, 1671)	2.9751604347206304
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9827471886902486
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9827471886902486
  (0, 1664)	3.000000000000002
  (0, 1663)	2.951685975699882
  (0, 1662)	2.9827471886902486
  (0, 1661)	2.978493893434942
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0038785876529044
  :	:
  (942, 24)	3.83017988418391
  (942, 23)	3.5356808699755153
  (942, 22)	4.106828529448043
  (942, 21)	4.1951459171820185
  (942, 20)	3.0121718086386875
  (942, 19)	3.432276145062845
  (942, 18)	3.8654046672249978
  (942, 17)	3.1950979353431985
  (942, 16)	3.3622070113059905
  (942, 15)	3.400307509853494
  (942, 14)	3.9095791035988325
  (942, 13)	4.056790682186745
  (942, 12)	3.458717541860462
  (942, 11)	4.331973053376487
  (942, 10)	3.7913900669849006
  (942, 9)	3.6468009149237584
  (942, 8)	4.0110069460920865
  (942, 7)	3.8498516800287805
  (942, 6)	3.856588443206775
  (942, 5)	3.5450333503258107
  (942, 4)	3.3654499446849067
  (942, 3)	3.401345738173273
  (942, 2)	3.076292728076659
  (942, 1)	3.1741867506334582
  (942, 0)	3.922744299204335
this is the 83 epoch
rmse loss on training set is 0.9748808493392135
rmse loss on test set is 0.9950689933537554
for this epoch using 109.28281664848328 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9950470616838514
  (0, 1680)	3.000000000000002
  (0, 1679)	2.97320044214251
  (0, 1678)	2.998554323234298
  (0, 1677)	2.9478465610507225
  (0, 1676)	2.9982294661288504
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9856744176118553
  (0, 1671)	2.974880504637996
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.982571478309136
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.982571478309136
  (0, 1664)	3.000000000000002
  (0, 1663)	2.951129490872123
  (0, 1662)	2.982571478309136
  (0, 1661)	2.9782576720777403
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0039643398267253
  :	:
  (942, 24)	3.831365900074335
  (942, 23)	3.5373618783857643
  (942, 22)	4.110957896883742
  (942, 21)	4.198151419642791
  (942, 20)	3.0105849134228753
  (942, 19)	3.434514187627124
  (942, 18)	3.8705587754365123
  (942, 17)	3.195015894907589
  (942, 16)	3.3635450917034744
  (942, 15)	3.402359695769202
  (942, 14)	3.9104002623570766
  (942, 13)	4.059469182794633
  (942, 12)	3.4599176766174393
  (942, 11)	4.335454966960133
  (942, 10)	3.7939553212217714
  (942, 9)	3.650313043963776
  (942, 8)	4.01273804549649
  (942, 7)	3.852097171834901
  (942, 6)	3.8574223334230604
  (942, 5)	3.548744778889223
  (942, 4)	3.366868515350683
  (942, 3)	3.402678771443562
  (942, 2)	3.075374806666893
  (942, 1)	3.1740324495194647
  (942, 0)	3.9234074100230556
this is the 84 epoch
rmse loss on training set is 0.9744486027244986
rmse loss on test set is 0.9947405186764472
for this epoch using 112.52273726463318 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994978057277444
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9728792457610984
  (0, 1678)	2.9985324260269457
  (0, 1677)	2.947226065495245
  (0, 1676)	2.9982224879949713
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9854157920123088
  (0, 1671)	2.9746011089142548
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.982395930098895
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.982395930098895
  (0, 1664)	3.000000000000002
  (0, 1663)	2.950573556521435
  (0, 1662)	2.982395930098895
  (0, 1661)	2.978021805605779
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0040506555994346
  :	:
  (942, 24)	3.8325082747425983
  (942, 23)	3.538997107341319
  (942, 22)	4.114998592155804
  (942, 21)	4.201080634962228
  (942, 20)	3.0089964606343487
  (942, 19)	3.4367207739340895
  (942, 18)	3.875643287725872
  (942, 17)	3.194912129353049
  (942, 16)	3.36485611806246
  (942, 15)	3.404384504357651
  (942, 14)	3.911185509412673
  (942, 13)	4.062071486120543
  (942, 12)	3.4610878784991193
  (942, 11)	4.338841225545201
  (942, 10)	3.7964594582460873
  (942, 9)	3.6537724310055695
  (942, 8)	4.014409141885604
  (942, 7)	3.8542832635266593
  (942, 6)	3.8582314757092733
  (942, 5)	3.552423565190523
  (942, 4)	3.3682585441769324
  (942, 3)	3.4039863745344463
  (942, 2)	3.0744507787239677
  (942, 1)	3.1738628928221084
  (942, 0)	3.924056170872214
this is the 85 epoch
rmse loss on training set is 0.9740253694246985
rmse loss on test set is 0.9944198249832267
for this epoch using 111.2911651134491 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9949092289142336
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9725581320033894
  (0, 1678)	2.9985105214326664
  (0, 1677)	2.9466057425741203
  (0, 1676)	2.998215850903631
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.985156462793352
  (0, 1671)	2.9743222449469444
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.98222053770194
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.98222053770194
  (0, 1664)	3.000000000000002
  (0, 1663)	2.950018168373945
  (0, 1662)	2.98222053770194
  (0, 1661)	2.9777862916333957
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.004137522447466
  :	:
  (942, 24)	3.8336087521880735
  (942, 23)	3.540587700180931
  (942, 22)	4.118952947122562
  (942, 21)	4.203936152236133
  (942, 20)	3.007406900587865
  (942, 19)	3.4388965206815176
  (942, 18)	3.880659349164434
  (942, 17)	3.1947872020868133
  (942, 16)	3.3661407626713817
  (942, 15)	3.4063825224971147
  (942, 14)	3.911936466839317
  (942, 13)	4.064599982586464
  (942, 12)	3.462229065878299
  (942, 11)	4.342134933724909
  (942, 10)	3.798904168334371
  (942, 9)	3.6571800067114526
  (942, 8)	4.016022487306499
  (942, 7)	3.8564118892737476
  (942, 6)	3.859016904013336
  (942, 5)	3.556070276982038
  (942, 4)	3.3696207035326404
  (942, 3)	3.405269301872142
  (942, 2)	3.0735210644918114
  (942, 1)	3.173678599704072
  (942, 0)	3.924691194892181
this is the 86 epoch
rmse loss on training set is 0.9736108474681877
rmse loss on test set is 0.9941066352801878
for this epoch using 110.12170147895813 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994840577906614
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9722371013364963
  (0, 1678)	2.9984886098395043
  (0, 1677)	2.9459855928334986
  (0, 1676)	2.998209548237056
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9848964561652496
  (0, 1671)	2.9740439100794283
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.982045295027354
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.982045295027354
  (0, 1664)	3.000000000000002
  (0, 1663)	2.949463322167321
  (0, 1662)	2.982045295027354
  (0, 1661)	2.977551127836086
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0042249280451276
  :	:
  (942, 24)	3.834669000835116
  (942, 23)	3.5421347696545253
  (942, 22)	4.122823227370075
  (942, 21)	4.206720462539802
  (942, 20)	3.005816665111232
  (942, 19)	3.4410420278884253
  (942, 18)	3.8856080811952007
  (942, 17)	3.194641660486243
  (942, 16)	3.3673996779996322
  (942, 15)	3.408354320239604
  (942, 14)	3.9126546749012285
  (942, 13)	4.067056982241167
  (942, 12)	3.4633421241906364
  (942, 11)	4.345339090555672
  (942, 10)	3.801291090700086
  (942, 9)	3.6605366814826286
  (942, 8)	4.017580243823128
  (942, 7)	3.8584849164329023
  (942, 6)	3.8597795923450477
  (942, 5)	3.559685467016687
  (942, 4)	3.3709556471789974
  (942, 3)	3.4065282813543325
  (942, 2)	3.0725860679818138
  (942, 1)	3.1734800726185504
  (942, 0)	3.9253130476997415
this is the 87 epoch
rmse loss on training set is 0.9732047487306765
rmse loss on test set is 0.9938006857632901
for this epoch using 109.90533494949341 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9947721053850778
  (0, 1680)	3.000000000000002
  (0, 1679)	2.971916154166371
  (0, 1678)	2.99846669158812
  (0, 1677)	2.945365616744626
  (0, 1676)	2.998203573466562
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.984635797664394
  (0, 1671)	2.9737661016049963
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9818701962396115
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9818701962396115
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9489090136524845
  (0, 1662)	2.9818701962396115
  (0, 1661)	2.9773163119475083
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.004312860264731
  :	:
  (942, 24)	3.8356906168974376
  (942, 23)	3.54363939885774
  (942, 22)	4.126611634146114
  (942, 21)	4.2094359627390565
  (942, 20)	3.0042261682709905
  (942, 19)	3.443157879476377
  (942, 18)	3.890490582247462
  (942, 17)	3.194476036437011
  (942, 16)	3.3686334973828562
  (942, 15)	3.410300451403026
  (942, 14)	3.913341596357051
  (942, 13)	4.069444717589212
  (942, 12)	3.464427907244016
  (942, 11)	4.348456593154195
  (942, 10)	3.8036218151387353
  (942, 9)	3.663843346030332
  (942, 8)	4.019084487186308
  (942, 7)	3.8605041478828204
  (942, 6)	3.860520458638607
  (942, 5)	3.5632696735606255
  (942, 4)	3.3722640108775352
  (942, 3)	3.407764015374439
  (942, 2)	3.071646177636918
  (942, 1)	3.1732677979227897
  (942, 0)	3.925922251322871
this is the 88 epoch
rmse loss on training set is 0.9728067981571457
rmse loss on test set is 0.9935017250643715
for this epoch using 108.78328847885132 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9947038123085963
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9715952908409444
  (0, 1678)	2.9984447669741883
  (0, 1677)	2.9447458147077
  (0, 1676)	2.9981979201556603
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9843745121691594
  (0, 1671)	2.973488816770777
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.981695235747717
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.981695235747717
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9483552385951586
  (0, 1662)	2.981695235747717
  (0, 1661)	2.977081841756714
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.004401307176497
  :	:
  (942, 24)	3.83667512759447
  (942, 23)	3.5451026421332403
  (942, 22)	4.130320306240721
  (942, 21)	4.212084959158588
  (942, 20)	3.0026358070671018
  (942, 19)	3.4452446438267454
  (942, 18)	3.8953079283320604
  (942, 17)	3.19429084685019
  (942, 16)	3.3698428356813532
  (942, 15)	3.412221454139131
  (942, 14)	3.9139986205388766
  (942, 13)	4.071765346319745
  (942, 12)	3.465487238472798
  (942, 11)	4.351490240178874
  (942, 10)	3.8058978836188255
  (942, 9)	3.6671008719263605
  (942, 8)	4.020537210356107
  (942, 7)	3.8624713242807682
  (942, 6)	3.861240368367191
  (942, 5)	3.5668234208849
  (942, 4)	3.373546412975205
  (942, 3)	3.4089771818040164
  (942, 2)	3.070701766965328
  (942, 1)	3.173042246465967
  (942, 0)	3.926519287829472
this is the 89 epoch
rmse loss on training set is 0.9724167330338819
rmse loss on test set is 0.993209513545801
for this epoch using 109.41814398765564 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994635699474519
  (0, 1680)	3.000000000000002
  (0, 1679)	2.971274511653116
  (0, 1678)	2.998422836250668
  (0, 1677)	2.9441261870555717
  (0, 1676)	2.998192581962698
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9841126239155287
  (0, 1671)	2.9732120527814927
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9815204081948883
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9815204081948883
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9478019927773267
  (0, 1662)	2.9815204081948883
  (0, 1661)	2.976847715105492
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.004490257048126
  :	:
  (942, 24)	3.837623994225986
  (942, 23)	3.546525525940059
  (942, 22)	4.133951321814598
  (942, 21)	4.214669671112386
  (942, 20)	3.0010459620980865
  (942, 19)	3.447302874315355
  (942, 18)	3.9000611736180306
  (942, 17)	3.1940865941591605
  (942, 16)	3.3710282899127826
  (942, 15)	3.4141178514784705
  (942, 14)	3.914627067217792
  (942, 13)	4.074020953937983
  (942, 12)	3.466520912139102
  (942, 11)	4.35444273519816
  (942, 10)	3.8081207918203175
  (942, 9)	3.6703101121338286
  (942, 8)	4.021940326882385
  (942, 7)	3.8643881262429916
  (942, 6)	3.8619401379247784
  (942, 5)	3.570347219737167
  (942, 4)	3.374803454967396
  (942, 3)	3.410168434935114
  (942, 2)	3.0697531951452914
  (942, 1)	3.1728038741526117
  (942, 0)	3.9271046026723777
this is the 90 epoch
rmse loss on training set is 0.9720343023069524
rmse loss on test set is 0.9929238226402457
for this epoch using 109.45750308036804 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99456776752798
  (0, 1680)	3.000000000000002
  (0, 1679)	2.970953816843626
  (0, 1678)	2.998400899630023
  (0, 1677)	2.9435067340572325
  (0, 1676)	2.9981875526430644
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9838501565123536
  (0, 1671)	2.972935806802934
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9813457084486057
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9813457084486057
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9472492719985715
  (0, 1662)	2.9813457084486057
  (0, 1661)	2.976613929885818
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0045796983442226
  :	:
  (942, 24)	3.8385386151108745
  (942, 23)	3.5479090496923567
  (942, 22)	4.137506700176101
  (942, 21)	4.217192234300487
  (942, 20)	2.9994569981979318
  (942, 19)	3.4493331098251647
  (942, 18)	3.904751350991177
  (942, 17)	3.193863766797149
  (942, 16)	3.3721904398603457
  (942, 15)	3.415990151853345
  (942, 14)	3.915228190266445
  (942, 13)	4.076213556302971
  (942, 12)	3.467529694483538
  (942, 11)	4.357316689949005
  (942, 10)	3.8102919906221295
  (942, 9)	3.6734719015188717
  (942, 8)	4.023295674148746
  (942, 7)	3.866256176451283
  (942, 6)	3.862620537789713
  (942, 5)	3.5738415677943496
  (942, 4)	3.376035722039816
  (942, 3)	3.411338406384255
  (942, 2)	3.068800807602506
  (942, 1)	3.172553122482546
  (942, 0)	3.9276786077714396
this is the 91 epoch
rmse loss on training set is 0.971659265943895
rmse loss on test set is 0.9926444342324259
for this epoch using 107.03194522857666 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9945000169708584
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9706332066037717
  (0, 1678)	2.9983789572863198
  (0, 1677)	2.9428874559212175
  (0, 1676)	2.998182826051116
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.983587132956389
  (0, 1671)	2.972660075965372
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9811711315911165
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9811711315911165
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9466970720773737
  (0, 1662)	2.9811711315911165
  (0, 1661)	2.976380484037461
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.004669619725468
  :	:
  (942, 24)	3.8394203283957875
  (942, 23)	3.549254186568706
  (942, 22)	4.140988403508163
  (942, 21)	4.219654704076746
  (942, 20)	2.9978692650461496
  (942, 19)	3.4513358752383367
  (942, 18)	3.909379472595327
  (942, 17)	3.1936228396563338
  (942, 16)	3.373329848657282
  (942, 15)	3.417838849599697
  (942, 14)	3.915803181129146
  (942, 13)	4.078345102074586
  (942, 12)	3.4685143248275034
  (942, 11)	4.360114627488305
  (942, 10)	3.8124128875403462
  (942, 9)	3.6765870573443045
  (942, 8)	4.024605016485144
  (942, 7)	3.8680770416876067
  (942, 6)	3.8632822954836223
  (942, 5)	3.5773069500970824
  (942, 4)	3.3772437835900564
  (942, 3)	3.4124877059594487
  (942, 2)	3.06784493656133
  (942, 1)	3.1722904190685903
  (942, 0)	3.9282416843519505
this is the 92 epoch
rmse loss on training set is 0.971291394335635
rmse loss on test set is 0.9923711400799958
for this epoch using 113.07636761665344 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994432448170303
  (0, 1680)	3.000000000000002
  (0, 1679)	2.970312681078014
  (0, 1678)	2.9983570093572216
  (0, 1677)	2.942268352798806
  (0, 1676)	2.9981783961417166
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.983323575646903
  (0, 1671)	2.9723848573667127
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9809966729103703
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9809966729103703
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9461453888522287
  (0, 1662)	2.9809966729103703
  (0, 1661)	2.9761473755457164
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0047600100476286
  :	:
  (942, 24)	3.8402704147392837
  (942, 23)	3.5505618842931024
  (942, 22)	4.144398338545901
  (942, 21)	4.222059058591566
  (942, 20)	2.996283097752062
  (942, 19)	3.453311681908184
  (942, 18)	3.91394653035671
  (942, 17)	3.1933642745292246
  (942, 16)	3.374447063348845
  (942, 15)	3.4196644254388393
  (942, 14)	3.9163531721090785
  (942, 13)	4.080417475072796
  (942, 12)	3.4694755166291045
  (942, 11)	4.362838985240153
  (942, 10)	3.8144848481185933
  (942, 9)	3.6796563797457944
  (942, 8)	4.02587004815414
  (942, 7)	3.869852234799147
  (942, 6)	3.86392609833841
  (942, 5)	3.580743839466714
  (942, 4)	3.3784281937298197
  (942, 3)	3.413616922492002
  (942, 2)	3.0668859015711663
  (942, 1)	3.172016178132825
  (942, 0)	3.928794185557338
this is the 93 epoch
rmse loss on training set is 0.9709304677357512
rmse loss on test set is 0.9921037412707874
for this epoch using 108.6594717502594 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9943650613668398
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9699922403665076
  (0, 1678)	2.998335055945909
  (0, 1677)	2.941649424787102
  (0, 1676)	2.9981742569714713
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9830595064001137
  (0, 1671)	2.9721101480755467
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9808223278912678
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9808223278912678
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9455942181827917
  (0, 1662)	2.9808223278912678
  (0, 1661)	2.9759146024392074
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00485085836036
  :	:
  (942, 24)	3.8410900998765816
  (942, 23)	3.5518330658888937
  (942, 22)	4.147738358206323
  (942, 21)	4.2244072018143415
  (942, 20)	2.9946988174146596
  (942, 19)	3.455261028112328
  (942, 18)	3.918453496492248
  (942, 17)	3.193088520533156
  (942, 16)	3.3755426154326256
  (942, 15)	3.4214673469400054
  (942, 14)	3.916879239482091
  (942, 13)	4.082432496552467
  (942, 12)	3.4704139584947904
  (942, 11)	4.365492117941954
  (942, 10)	3.8165091972722274
  (942, 9)	3.682680652191467
  (942, 8)	4.02709239621582
  (942, 7)	3.8715832165957575
  (942, 6)	3.8645525960834273
  (942, 5)	3.5841526969057127
  (942, 4)	3.3795894917684484
  (942, 3)	3.414726624634445
  (942, 2)	3.0659240100093297
  (942, 1)	3.171730800982593
  (942, 0)	3.9293364388531367
this is the 94 epoch
rmse loss on training set is 0.9705762757346372
rmse loss on test set is 0.9918420477139465
for this epoch using 111.23162937164307 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994297856682104
  (0, 1680)	3.000000000000002
  (0, 1679)	2.969671884527442
  (0, 1678)	2.998313097122923
  (0, 1677)	2.941030671931965
  (0, 1676)	2.9981704026996887
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.982794946463209
  (0, 1671)	2.97183594513401
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9806480922073435
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9806480922073435
  (0, 1664)	3.000000000000002
  (0, 1663)	2.945043555950844
  (0, 1662)	2.9806480922073435
  (0, 1661)	2.975682162787853
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0049421539059042
  :	:
  (942, 24)	3.841880557070193
  (942, 23)	3.553068630406558
  (942, 22)	4.151010263170934
  (942, 21)	4.226700966439311
  (942, 20)	2.9931167316590273
  (942, 19)	3.4571843994875655
  (942, 18)	3.922901324002083
  (942, 17)	3.192796014518608
  (942, 16)	3.376617021378126
  (942, 15)	3.4232480689644733
  (942, 14)	3.9173824064457907
  (942, 13)	4.084391927396268
  (942, 12)	3.4713303151485224
  (942, 11)	4.36807630049209
  (942, 10)	3.8184872205876172
  (942, 9)	3.6856606419254576
  (942, 8)	4.028273623275941
  (942, 7)	3.8732713976818105
  (942, 6)	3.865162403264094
  (942, 5)	3.5875339719822548
  (942, 4)	3.380728202678694
  (942, 3)	3.4158173616259773
  (942, 2)	3.0649595575612922
  (942, 1)	3.1714346764669954
  (942, 0)	3.929868748237508
this is the 95 epoch
rmse loss on training set is 0.9702286167660118
rmse loss on test set is 0.9915858776626366
for this epoch using 110.43280243873596 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994230834126127
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9693516135793794
  (0, 1678)	2.9982911329279256
  (0, 1677)	2.940412094230836
  (0, 1676)	2.998166827589073
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.982529916528189
  (0, 1671)	2.9715622455605235
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.980473961712791
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.980473961712791
  (0, 1664)	3.000000000000002
  (0, 1663)	2.944493398061228
  (0, 1662)	2.980473961712791
  (0, 1661)	2.975450054700894
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.005033886117553
  :	:
  (942, 24)	3.8426429094512673
  (942, 23)	3.5542694536264667
  (942, 22)	4.154215803422613
  (942, 21)	4.228942116679141
  (942, 20)	2.991537135150459
  (942, 19)	3.459082269447428
  (942, 18)	3.9272909471470983
  (942, 17)	3.1924871814619946
  (942, 16)	3.3776707831264945
  (942, 15)	3.425007034092053
  (942, 14)	3.9178636459126
  (942, 13)	4.0862974702288115
  (942, 12)	3.4722252283603043
  (942, 11)	4.3705937307022165
  (942, 10)	3.820420165578192
  (942, 9)	3.6885971003962674
  (942, 8)	4.029415230122008
  (942, 7)	3.874918140224693
  (942, 6)	3.8657561015026363
  (942, 5)	3.590888103199519
  (942, 4)	3.3818448375452888
  (942, 3)	3.4168896640269124
  (942, 2)	3.0639928286797287
  (942, 1)	3.171128181414828
  (942, 0)	3.9303913962730315
this is the 96 epoch
rmse loss on training set is 0.9698872976436868
rmse loss on test set is 0.9913350572661723
for this epoch using 106.98891568183899 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994163993604349
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9690314275033782
  (0, 1678)	2.9982691633713574
  (0, 1677)	2.9397936916354097
  (0, 1676)	2.998163526006214
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9822644367453117
  (0, 1671)	2.9712890463524047
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.980299932434803
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.980299932434803
  (0, 1664)	3.000000000000002
  (0, 1663)	2.943943740442734
  (0, 1662)	2.980299932434803
  (0, 1661)	2.975218276325015
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0051260446180823
  :	:
  (942, 24)	3.843378232256423
  (942, 23)	3.555436388737638
  (942, 22)	4.1573566797376245
  (942, 21)	4.231132350950178
  (942, 20)	2.989960310087307
  (942, 19)	3.4609550995831757
  (942, 18)	3.931623281911833
  (942, 17)	3.192162434843698
  (942, 16)	3.3787043885711916
  (942, 15)	3.426744673030879
  (942, 14)	3.9183238831545837
  (942, 13)	4.0881507714546395
  (942, 12)	3.4730993178360205
  (942, 11)	4.373046531956841
  (942, 10)	3.822309242898486
  (942, 9)	3.691490763670458
  (942, 8)	4.0305186582515455
  (942, 7)	3.876524759661721
  (942, 6)	3.8663342416109576
  (942, 5)	3.594215518350636
  (942, 4)	3.3829398939972504
  (942, 3)	3.4179440444232827
  (942, 2)	3.063024097023143
  (942, 1)	3.1708116810547766
  (942, 0)	3.9309046459531376
this is the 97 epoch
rmse loss on training set is 0.9695521331264586
rmse loss on test set is 0.9910894201495836
for this epoch using 109.47378659248352 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994097334924223
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9687113262451286
  (0, 1678)	2.9982471884360633
  (0, 1677)	2.9391754640542023
  (0, 1676)	2.9981604924217713
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9819985267362865
  (0, 1671)	2.9710163444883015
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9801260005662673
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9801260005662673
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9433945790488707
  (0, 1662)	2.9801260005662673
  (0, 1661)	2.974986825842628
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0052186192179793
  :	:
  (942, 24)	3.8440875549644358
  (942, 23)	3.5565702669933414
  (942, 22)	4.160434545134089
  (942, 21)	4.233273304453154
  (942, 20)	2.9883865266733918
  (942, 19)	3.4628033400488873
  (942, 18)	3.935899226453388
  (942, 17)	3.1918221770118773
  (942, 16)	3.379718312020465
  (942, 15)	3.4284614050109803
  (942, 14)	3.9187639983078832
  (942, 13)	4.0899534232227825
  (942, 12)	3.4739531820699714
  (942, 11)	4.3754367557829195
  (942, 10)	3.8241556275176776
  (942, 9)	3.6943423528322583
  (942, 8)	4.031585292296952
  (942, 7)	3.87809252634756
  (942, 6)	3.8668973455650058
  (942, 5)	3.59751663485966
  (942, 4)	3.3840138566243523
  (942, 3)	3.4189809981029335
  (942, 2)	3.0620536258752007
  (942, 1)	3.1704855294186984
  (942, 0)	3.9314087424156376
this is the 98 epoch
rmse loss on training set is 0.9692229455093156
rmse loss on test set is 0.9908488070187871
for this epoch using 106.1225004196167 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.994030857801506
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9683913097169183
  (0, 1678)	2.9982252080788045
  (0, 1677)	2.9385574113550312
  (0, 1676)	2.9981577214105877
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9817322056072455
  (0, 1671)	2.970744136930548
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9799521624587446
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9799521624587446
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9428459098586064
  (0, 1662)	2.9799521624587446
  (0, 1661)	2.974755701470129
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0053115999136177
  :	:
  (942, 24)	3.8447718633373813
  (942, 23)	3.5576718983444837
  (942, 22)	4.163451006277824
  (942, 21)	4.235366551653202
  (942, 20)	2.986816043571116
  (942, 19)	3.4646274299314346
  (942, 18)	3.9401196615367295
  (942, 17)	3.19146679953275
  (942, 16)	3.3807130146423323
  (942, 15)	3.4301576381625623
  (942, 14)	3.9191848287439655
  (942, 13)	4.091706965320461
  (942, 12)	3.474787399161937
  (942, 11)	4.377766384332401
  (942, 10)	3.8259604598538215
  (942, 9)	3.697152574369778
  (942, 8)	4.032616462350953
  (942, 7)	3.8796226671439684
  (942, 6)	3.8674459083494703
  (942, 5)	3.6007918601094406
  (942, 4)	3.3850671973785946
  (942, 3)	3.420001003704293
  (942, 2)	3.061081668545677
  (942, 1)	3.170150069728667
  (942, 0)	3.9319039145151895
this is the 99 epoch
rmse loss on training set is 0.9688995642390549
rmse loss on test set is 0.9906130652895995
for this epoch using 112.58449482917786 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9939645618662465
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9680713777995376
  (0, 1678)	2.998203222231766
  (0, 1677)	2.9379395333673153
  (0, 1676)	2.998155207651469
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9814654919612975
  (0, 1671)	2.9704724206273996
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.979778414615786
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.979778414615786
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9422977288770564
  (0, 1662)	2.979778414615786
  (0, 1661)	2.9745249014563697
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0054049768853215
  :	:
  (942, 24)	3.8454321013701835
  (942, 23)	3.558742072051691
  (942, 22)	4.166407624846769
  (942, 21)	4.237413608662764
  (942, 20)	2.985249108335932
  (942, 19)	3.466427797606014
  (942, 18)	3.944285450956869
  (942, 17)	3.191096683527823
  (942, 16)	3.381688944892747
  (942, 15)	3.431833769879538
  (942, 14)	3.9195871713146873
  (942, 13)	4.093412886998454
  (942, 12)	3.4756025276002616
  (942, 11)	4.3800373327801365
  (942, 10)	3.8277248468702623
  (942, 9)	3.6999221205482984
  (942, 8)	4.033613446196554
  (942, 7)	3.8811163669538793
  (942, 6)	3.867980399681126
  (942, 5)	3.6040415917566886
  (942, 4)	3.386100375961186
  (942, 3)	3.421004523838927
  (942, 2)	3.060108468753841
  (942, 1)	3.1698056347685872
  (942, 0)	3.9323903762652312
this is the 100 epoch
rmse loss on training set is 0.9685818255528303
rmse loss on test set is 0.990382048739079
for this epoch using 110.53581428527832 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9938984466684433
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9677515303441186
  (0, 1678)	2.998181230803902
  (0, 1677)	2.9373218298843313
  (0, 1676)	2.998152945926919
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9811984039109336
  (0, 1671)	2.970201192515099
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.979604753686481
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.979604753686481
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9417500321360652
  (0, 1662)	2.979604753686481
  (0, 1661)	2.9742944240810805
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.005498740495314
  :	:
  (942, 24)	3.8460691731527112
  (942, 23)	3.5597815572769034
  (942, 22)	4.1693059188550725
  (942, 21)	4.239415935531034
  (942, 20)	2.983685957833203
  (942, 19)	3.4682048610777456
  (942, 18)	3.948397441948569
  (942, 17)	3.1907121999987558
  (942, 16)	3.3826465389278577
  (942, 15)	3.4334901871690255
  (942, 14)	3.919971784477584
  (942, 13)	4.095072628730722
  (942, 12)	3.4763991070123907
  (942, 11)	4.3822514516400135
  (942, 10)	3.8294498631353315
  (942, 9)	3.7026516697711958
  (942, 8)	4.034577471445535
  (942, 7)	3.882574770201458
  (942, 6)	3.868501265618546
  (942, 5)	3.607266218035138
  (942, 4)	3.3871138401956773
  (942, 3)	3.42199200568905
  (942, 2)	3.0591342609953096
  (942, 1)	3.169452547241079
  (942, 0)	3.9328683281596235
this is the 101 epoch
rmse loss on training set is 0.9682695721380407
rmse loss on test set is 0.9901556171776603
for this epoch using 114.82575750350952 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9938325116834585
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9674317671738453
  (0, 1678)	2.9981592336823164
  (0, 1677)	2.936704300665379
  (0, 1676)	2.998150931122684
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9809309590900948
  (0, 1671)	2.969930449519927
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9794311764593147
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9794311764593147
  (0, 1664)	3.000000000000002
  (0, 1663)	2.941202815694831
  (0, 1662)	2.9794311764593147
  (0, 1661)	2.9740642676534774
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0055928812856183
  :	:
  (942, 24)	3.8466839446481553
  (942, 23)	3.5607911036552085
  (942, 22)	4.172147363937809
  (942, 21)	4.2413749384433235
  (942, 20)	2.9821268186380308
  (942, 19)	3.4699590283101327
  (942, 18)	3.9524564655835985
  (942, 17)	3.190313710140209
  (942, 16)	3.3835862210007246
  (942, 15)	3.435127266987342
  (942, 14)	3.9203393903078867
  (942, 13)	4.096687583910306
  (942, 12)	3.4771776588843166
  (942, 11)	4.384410529001675
  (942, 10)	3.831136551846586
  (942, 9)	3.7053418869290544
  (942, 8)	4.035509717588832
  (942, 7)	3.883998982260017
  (942, 6)	3.8690089300655606
  (942, 5)	3.6104661180470337
  (942, 4)	3.388108026387828
  (942, 3)	3.4229638815809866
  (942, 2)	3.058159270892896
  (942, 1)	3.1690911201101066
  (942, 0)	3.9333379583831274
this is the 102 epoch
rmse loss on training set is 0.967962652812096
rmse loss on test set is 0.9899336361407381
for this epoch using 118.31637191772461 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.993766756317106
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9671120880856647
  (0, 1678)	2.9981372307335277
  (0, 1677)	2.9360869454377956
  (0, 1676)	2.998149158227115
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.98066317466598
  (0, 1671)	2.969660188560065
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9792576798563157
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9792576798563157
  (0, 1664)	3.000000000000002
  (0, 1663)	2.940656075640358
  (0, 1662)	2.9792576798563157
  (0, 1661)	2.9738344305108915
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0056873899759013
  :	:
  (942, 24)	3.847277245391467
  (942, 23)	3.5617714418477555
  (942, 22)	4.174933394597534
  (942, 21)	4.243291971833731
  (942, 20)	2.9805719074190566
  (942, 19)	3.471690697540813
  (942, 18)	3.956463337156518
  (942, 17)	3.1899015656414282
  (942, 16)	3.384508403843411
  (942, 15)	3.436745376563177
  (942, 14)	3.920690676402993
  (942, 13)	4.0982591004841815
  (942, 12)	3.4779386872502154
  (942, 11)	4.386516292690538
  (942, 10)	3.8327859258209025
  (942, 9)	3.707993423737309
  (942, 8)	4.036411317962613
  (942, 7)	3.8853900708295757
  (942, 6)	3.8695037961753274
  (942, 5)	3.6136416620435887
  (942, 4)	3.389083359672653
  (942, 3)	3.423920569535561
  (942, 2)	3.057183715532474
  (942, 1)	3.1687216569302974
  (942, 0)	3.933799443919449
this is the 103 epoch
rmse loss on training set is 0.9676609222208938
rmse loss on test set is 0.989715976598402
for this epoch using 116.2268316745758 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9937011799105187
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9667924928518166
  (0, 1678)	2.998115221804711
  (0, 1677)	2.9354697638989196
  (0, 1676)	2.9981476223304755
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9803950673505715
  (0, 1671)	2.9693904065474026
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.979084260927352
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.979084260927352
  (0, 1664)	3.000000000000002
  (0, 1663)	2.940109808087987
  (0, 1662)	2.979084260927352
  (0, 1661)	2.9736049110174516
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0057822574611985
  :	:
  (942, 24)	3.8478498701113524
  (942, 23)	3.5627232840765193
  (942, 22)	4.177665405413602
  (942, 21)	4.245168340414358
  (942, 20)	2.9790214313067502
  (942, 19)	3.4734002575852023
  (942, 18)	3.9604188565588685
  (942, 17)	3.1894761089767316
  (942, 16)	3.3854134890348258
  (942, 15)	3.4383448737083753
  (942, 14)	3.9210262976852266
  (942, 13)	4.09978848252905
  (942, 12)	3.4786826793536956
  (942, 11)	4.388570412353349
  (942, 10)	3.8343989684514073
  (942, 9)	3.71060691906307
  (942, 8)	4.037283361633297
  (942, 7)	3.8867490672656437
  (942, 6)	3.8699862476613274
  (942, 5)	3.6167932116949246
  (942, 4)	3.39004025434928
  (942, 3)	3.424862473796548
  (942, 2)	3.056207803784422
  (942, 1)	3.168344452163213
  (942, 0)	3.9342529515648597
this is the 104 epoch
rmse loss on training set is 0.9673642405546394
rmse loss on test set is 0.9895025146822181
for this epoch using 111.50463795661926 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9936357817447616
  (0, 1680)	3.000000000000002
  (0, 1679)	2.966472981221406
  (0, 1678)	2.998093206724854
  (0, 1677)	2.9348527557179573
  (0, 1676)	2.9981463186240727
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9801266534119426
  (0, 1671)	2.969121100389251
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.978910916844813
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.978910916844813
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9395640091817756
  (0, 1662)	2.978910916844813
  (0, 1661)	2.9733757075628704
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0058774748096555
  :	:
  (942, 24)	3.8484025802791435
  (942, 23)	3.563647324641412
  (942, 22)	4.180344752215338
  (942, 21)	4.247005301124269
  (942, 20)	2.9774755882469712
  (942, 19)	3.475088088128599
  (942, 18)	3.964323808642533
  (942, 17)	3.189037673685685
  (942, 16)	3.386301867355111
  (942, 15)	3.4399261071170097
  (942, 14)	3.921346878108079
  (942, 13)	4.101276991770204
  (942, 12)	3.47941010628175
  (942, 11)	4.390574501471934
  (942, 10)	3.835976634632483
  (942, 9)	3.71318299924146
  (942, 8)	4.038126895204948
  (942, 7)	3.8880769678608886
  (942, 6)	3.8704566500213655
  (942, 5)	3.619921120349878
  (942, 4)	3.390979114204036
  (942, 3)	3.425789985337911
  (942, 2)	3.0552317366113986
  (942, 1)	3.1679597914813686
  (942, 0)	3.9346986388546723
this is the 105 epoch
rmse loss on training set is 0.9670724732800258
rmse loss on test set is 0.9892931314278188
for this epoch using 113.96922516822815 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9935705610451655
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9661535529218024
  (0, 1678)	2.9980711853058613
  (0, 1677)	2.934235920537748
  (0, 1676)	2.9981452423993433
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9798579486852774
  (0, 1671)	2.9688522669899524
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9787376448983776
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9787376448983776
  (0, 1664)	3.000000000000002
  (0, 1663)	2.93901867509491
  (0, 1662)	2.9787376448983776
  (0, 1661)	2.97314681856129
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00597303326018
  :	:
  (942, 24)	3.8489361055878923
  (942, 23)	3.56454424042067
  (942, 22)	4.1829727532200165
  (942, 21)	4.248804065001071
  (942, 20)	2.9759345673404507
  (942, 19)	3.4767545600071785
  (942, 18)	3.968178963572427
  (942, 17)	3.1885865846430956
  (942, 16)	3.387173919126945
  (942, 15)	3.441489416653121
  (942, 14)	3.9216530122711024
  (942, 13)	4.102725849045656
  (942, 12)	3.4801214235727
  (942, 11)	4.392530119307178
  (942, 10)	3.8375198516538824
  (942, 9)	3.715722278381799
  (942, 8)	4.038942924551864
  (942, 7)	3.8893747350814136
  (942, 6)	3.8709153516803427
  (942, 5)	3.623025733286122
  (942, 4)	3.391900332822278
  (942, 3)	3.426703482350792
  (942, 2)	3.054255707363061
  (942, 1)	3.1675679520604283
  (942, 0)	3.9351366549095874
this is the 106 epoch
rmse loss on training set is 0.9667854908876236
rmse loss on test set is 0.9890877125324073
for this epoch using 109.87337875366211 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9935055169854956
  (0, 1680)	3.000000000000002
  (0, 1679)	2.965834207660079
  (0, 1678)	2.9980491573436536
  (0, 1677)	2.9336192579765057
  (0, 1676)	2.9981443890468253
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9795889685836188
  (0, 1671)	2.9685839032524104
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.978564442490101
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.978564442490101
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9384738020300487
  (0, 1662)	2.978564442490101
  (0, 1661)	2.972918242450155
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.006068924220064
  :	:
  (942, 24)	3.8494511453647005
  (942, 23)	3.5654146913549685
  (942, 22)	4.185550690136732
  (942, 21)	4.250565798978333
  (942, 20)	2.9743985491687868
  (942, 19)	3.4784000354784332
  (942, 18)	3.9719850771690766
  (942, 17)	3.188123158319511
  (942, 16)	3.388030014544418
  (942, 15)	3.443035133627654
  (942, 14)	3.9219452669483834
  (942, 13)	4.104136235717465
  (942, 12)	3.4808170717991476
  (942, 11)	4.394438772775839
  (942, 10)	3.839029520065052
  (942, 9)	3.718225358664305
  (942, 8)	4.039732416479898
  (942, 7)	3.8906432987589623
  (942, 6)	3.8713626850569685
  (942, 5)	3.6261073879511057
  (942, 4)	3.3928042938893506
  (942, 3)	3.4276033307112534
  (942, 2)	3.0532799020583696
  (942, 1)	3.1671692028601375
  (942, 0)	3.9355671412081725
this is the 107 epoch
rmse loss on training set is 0.9665031686534983
rmse loss on test set is 0.9888861481261885
for this epoch using 109.33790755271912 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99344064869187
  (0, 1680)	3.000000000000002
  (0, 1679)	2.965514945124292
  (0, 1678)	2.998027122619179
  (0, 1677)	2.9330027676294006
  (0, 1676)	2.998143754055012
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.979319728108415
  (0, 1671)	2.9683160060795637
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9783913071296757
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9783913071296757
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9379293862196327
  (0, 1662)	2.9783913071296757
  (0, 1661)	2.972689977689158
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0061651392625803
  :	:
  (942, 24)	3.849948369919316
  (942, 23)	3.5662593209160094
  (942, 22)	4.1880798092370375
  (942, 21)	4.2522916276114655
  (942, 20)	2.972867706107569
  (942, 19)	3.480024868481429
  (942, 18)	3.975742891241208
  (942, 17)	3.187647703032394
  (942, 16)	3.3888705139899518
  (942, 15)	3.444563581065108
  (942, 14)	3.922224182534979
  (942, 13)	4.105509295032127
  (942, 12)	3.4814974771272027
  (942, 11)	4.396301918262131
  (942, 10)	3.840506514510601
  (942, 9)	3.7206928306273483
  (942, 8)	4.04049630031895
  (942, 7)	3.8918835572408215
  (942, 6)	3.8717989675593856
  (942, 5)	3.6291664141941022
  (942, 4)	3.3936913714812285
  (942, 3)	3.428489884429247
  (942, 2)	3.0523044996561013
  (942, 1)	3.1667638048945106
  (942, 0)	3.935990232291388
this is the 108 epoch
rmse loss on training set is 0.9662253864142711
rmse loss on test set is 0.9886883325567514
for this epoch using 108.05723762512207 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.993375955246461
  (0, 1680)	3.000000000000002
  (0, 1679)	2.965195764984751
  (0, 1678)	2.9980050808993637
  (0, 1677)	2.9323864490701386
  (0, 1676)	2.998143333009201
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.979050241859789
  (0, 1671)	2.968048572375736
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9782182364298966
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9782182364298966
  (0, 1664)	3.000000000000002
  (0, 1663)	2.937385423926157
  (0, 1662)	2.9782182364298966
  (0, 1661)	2.9724620227592773
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0062616701245455
  :	:
  (942, 24)	3.850428421831781
  (942, 23)	3.5670787565599773
  (942, 22)	4.1905613223933695
  (942, 21)	4.253982634734859
  (942, 20)	2.971342202627224
  (942, 19)	3.481629404887433
  (942, 18)	3.979453133908878
  (942, 17)	3.1871605191885544
  (942, 16)	3.389695768339716
  (942, 15)	3.4460750739602135
  (942, 14)	3.9224902744158063
  (942, 13)	4.106846133431975
  (942, 12)	3.482163051852738
  (942, 11)	4.398120963366424
  (942, 10)	3.8419516845380035
  (942, 9)	3.7231252734459805
  (942, 8)	4.041235469449698
  (942, 7)	3.893096378498724
  (942, 6)	3.872224502514415
  (942, 5)	3.632203134489775
  (942, 4)	3.394561930345114
  (942, 3)	3.4293634860799838
  (942, 2)	3.051329672314031
  (942, 1)	3.1663520114916004
  (942, 0)	3.9364060564046617
this is the 109 epoch
rmse loss on training set is 0.9659520283546327
rmse loss on test set is 0.9884941641857691
for this epoch using 109.69299364089966 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9933114356910093
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9648766668952504
  (0, 1678)	2.9979830319380842
  (0, 1677)	2.931770301852417
  (0, 1676)	2.9981431215902283
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9787805240465866
  (0, 1671)	2.9677815990479433
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9780452281023
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9780452281023
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9368419114424187
  (0, 1662)	2.9780452281023
  (0, 1661)	2.9722343761618073
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0063585087038627
  :	:
  (942, 24)	3.8508919171819085
  (942, 23)	3.567873610166727
  (942, 22)	4.192996408086107
  (942, 21)	4.255639865053038
  (942, 20)	2.969822195582011
  (942, 19)	3.4832139827411672
  (942, 18)	3.9831165199173677
  (942, 17)	3.1866618995179956
  (942, 16)	3.390506119258026
  (942, 15)	3.4475699195251712
  (942, 14)	3.9227440342611604
  (942, 13)	4.108147821819406
  (942, 12)	3.48281419491596
  (942, 11)	4.399897268593067
  (942, 10)	3.843365855378427
  (942, 9)	3.725523255201707
  (942, 8)	4.041950782767173
  (942, 7)	3.894282601198112
  (942, 6)	3.87263958003483
  (942, 5)	3.6352178641536894
  (942, 4)	3.3954163261704635
  (942, 3)	3.4302244672181397
  (942, 2)	3.0503555856373916
  (942, 1)	3.1659340685435144
  (942, 0)	3.9368147360824266
this is the 110 epoch
rmse loss on training set is 0.9656829828066453
rmse loss on test set is 0.9883035451970101
for this epoch using 111.05875945091248 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.993247089030192
  (0, 1680)	3.000000000000002
  (0, 1679)	2.964557650494199
  (0, 1678)	2.9979609754770227
  (0, 1677)	2.931154325511373
  (0, 1676)	2.9981431155731615
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9785105884961474
  (0, 1671)	2.967515083007151
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.977872279953046
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.977872279953046
  (0, 1664)	3.000000000000002
  (0, 1663)	2.936298845091743
  (0, 1662)	2.977872279953046
  (0, 1661)	2.9720070364174473
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.006455647057053
  :	:
  (942, 24)	3.8513394467232014
  (942, 23)	3.5686444784648934
  (942, 22)	4.19538621238036
  (942, 21)	4.257264325668288
  (942, 20)	2.968307834487841
  (942, 19)	3.4847789324932372
  (942, 18)	3.9867337509422454
  (942, 17)	3.1861521292997064
  (942, 16)	3.39130189948127
  (942, 15)	3.44904841742783
  (942, 14)	3.9229859312525823
  (942, 13)	4.1094153967755345
  (942, 12)	3.4834512923949785
  (942, 11)	4.401632148979424
  (942, 10)	3.8447498287015605
  (942, 9)	3.7278873331442406
  (942, 8)	4.042643066083844
  (942, 7)	3.895443035729401
  (942, 6)	3.873044477828519
  (942, 5)	3.6382109115500323
  (942, 4)	3.3962549058507823
  (942, 3)	3.4310731487757535
  (942, 2)	3.04938239891705
  (942, 1)	3.1655102147469156
  (942, 0)	3.9372163886799414
this is the 111 epoch
rmse loss on training set is 0.9654181420599627
rmse loss on test set is 0.988116381415151
for this epoch using 109.30637741088867 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9931829142347306
  (0, 1680)	3.000000000000002
  (0, 1679)	2.964238715405721
  (0, 1678)	2.997938911246541
  (0, 1677)	2.930538519564901
  (0, 1676)	2.998143310825974
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9782404486639265
  (0, 1671)	2.9672490211694105
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9776993898789
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9776993898789
  (0, 1664)	3.000000000000002
  (0, 1663)	2.93575622122814
  (0, 1662)	2.9776993898789
  (0, 1661)	2.9717800020655067
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0065530773967466
  :	:
  (942, 24)	3.8517715770036056
  (942, 23)	3.5693919434437453
  (942, 22)	4.197731849873104
  (942, 21)	4.258856987547312
  (942, 20)	2.9667992617892587
  (942, 19)	3.486324577224021
  (942, 18)	3.9903055158857015
  (942, 17)	3.185631486579597
  (942, 16)	3.3920834330915635
  (942, 15)	3.450510860021106
  (942, 14)	3.923216413243008
  (942, 13)	4.110649861735106
  (942, 12)	3.4840747179794906
  (942, 11)	4.403326875668069
  (942, 10)	3.8461043833455393
  (942, 9)	3.7302180539451575
  (942, 8)	4.043313113474524
  (942, 7)	3.8965784652022593
  (942, 6)	3.87343946195371
  (942, 5)	3.6411825782918688
  (942, 4)	3.397078007736653
  (942, 3)	3.431909841444547
  (942, 2)	3.0484102653580054
  (942, 1)	3.1650806818345134
  (942, 0)	3.9376111268565785
this is the 112 epoch
rmse loss on training set is 0.9651574021824434
rmse loss on test set is 0.9879325821346195
for this epoch using 109.00848245620728 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9931189102444073
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9639198612407087
  (0, 1678)	2.997916838966469
  (0, 1677)	2.9299228835149456
  (0, 1676)	2.9981437033081098
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9779701176427973
  (0, 1671)	2.9669834104569732
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.977526555863452
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.977526555863452
  (0, 1664)	3.000000000000002
  (0, 1663)	2.935214036236483
  (0, 1662)	2.977526555863452
  (0, 1661)	2.9715532716630504
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0066507920892365
  :	:
  (942, 24)	3.8521888514356113
  (942, 23)	3.5701165727521027
  (942, 22)	4.200034404611791
  (942, 21)	4.2604187869292005
  (942, 20)	2.96529661311616
  (942, 19)	3.4878512328594384
  (942, 18)	3.993832491164738
  (942, 17)	3.185100242380989
  (942, 16)	3.3928510357807746
  (942, 15)	3.4519575325641334
  (942, 14)	3.9234359078545387
  (942, 13)	4.11185218811916
  (942, 12)	3.4846848334252494
  (942, 11)	4.4049826774240834
  (942, 10)	3.8474302760224965
  (942, 9)	3.732515953944131
  (942, 8)	4.043961688565713
  (942, 7)	3.89768964640438
  (942, 6)	3.873824787523522
  (942, 5)	3.64413315943437
  (942, 4)	3.3978859618801476
  (942, 3)	3.432734846043157
  (942, 2)	3.0474393322984756
  (942, 1)	3.164645694797861
  (942, 0)	3.9379990590146408
this is the 113 epoch
rmse loss on training set is 0.9649006628504421
rmse loss on test set is 0.9877520599578317
for this epoch using 108.39245462417603 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.993055075970867
  (0, 1680)	3.000000000000002
  (0, 1679)	2.963601087597817
  (0, 1678)	2.9978947583469013
  (0, 1677)	2.929307416848737
  (0, 1676)	2.9981442890690753
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9776996081722023
  (0, 1671)	2.9667182477993284
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9773537759734565
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9773537759734565
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9346722865326433
  (0, 1662)	2.9773537759734565
  (0, 1661)	2.9713268437841407
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.006748783651924
  :	:
  (942, 24)	3.8525917913178946
  (942, 23)	3.5708189200847893
  (942, 22)	4.202294930985167
  (942, 21)	4.261950626677156
  (942, 20)	2.9638000175306325
  (942, 19)	3.4893592083788554
  (942, 18)	3.9973153409912157
  (942, 17)	3.184558660907875
  (942, 16)	3.3936050151051718
  (942, 15)	3.453388713435373
  (942, 14)	3.9236448235173205
  (942, 13)	4.113023316427065
  (942, 12)	3.4852819889902515
  (942, 11)	4.406600742099314
  (942, 10)	3.8487282420009095
  (942, 9)	3.7347815593876894
  (942, 8)	4.044589525771475
  (942, 7)	3.898777310726058
  (942, 6)	3.8742006993633282
  (942, 5)	3.6470629436611115
  (942, 4)	3.398679090271119
  (942, 3)	3.433548453870052
  (942, 2)	3.046469741420116
  (942, 1)	3.1642054721019384
  (942, 0)	3.938380289697308
this is the 114 epoch
rmse loss on training set is 0.9646478271881398
rmse loss on test set is 0.9875747306423366
for this epoch using 117.07819414138794 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9929914103002924
  (0, 1680)	3.000000000000002
  (0, 1679)	2.963282394064441
  (0, 1678)	2.9978726690889084
  (0, 1677)	2.9286921190399666
  (0, 1676)	2.998145064246961
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.977428932647051
  (0, 1671)	2.966453530134207
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.977181048355338
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.977181048355338
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9341309685636014
  (0, 1662)	2.977181048355338
  (0, 1661)	2.9711007170191612
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0068470447508693
  :	:
  (942, 24)	3.852980896810742
  (942, 23)	3.5714995255571744
  (942, 22)	4.204514454587088
  (942, 21)	4.263453377576038
  (942, 20)	2.9623095977643272
  (942, 19)	3.490848806015595
  (942, 18)	4.000754717644233
  (942, 17)	3.1840069997412677
  (942, 16)	3.394345670731097
  (942, 15)	3.454804674338091
  (942, 14)	3.9238435504525384
  (942, 13)	4.114164157289374
  (942, 12)	3.485866523853452
  (942, 11)	4.40818221804547
  (942, 10)	3.849998995765348
  (942, 9)	3.737015386661026
  (942, 8)	4.045197331478213
  (942, 7)	3.8998421650515263
  (942, 6)	3.874567432624089
  (942, 5)	3.649972213463927
  (942, 4)	3.3994577070656726
  (942, 3)	3.434350947042719
  (942, 2)	3.045501628949794
  (942, 1)	3.1637602258917377
  (942, 0)	3.93875491994924
this is the 115 epoch
rmse loss on training set is 0.9643988016154461
rmse loss on test set is 0.9874005129562042
for this epoch using 115.6018717288971 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9929279120959165
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9629637802175863
  (0, 1678)	2.997850570885285
  (0, 1677)	2.9280769895498877
  (0, 1676)	2.998146025066984
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9771581031263987
  (0, 1671)	2.966189254408487
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9770083712318547
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9770083712318547
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9335900788074984
  (0, 1662)	2.9770083712318547
  (0, 1661)	2.9708748899740813
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0069455681982555
  :	:
  (942, 24)	3.853356647867251
  (942, 23)	3.5721589160681746
  (942, 22)	4.2066939730542146
  (942, 21)	4.264927879578045
  (942, 20)	2.9608254704468058
  (942, 19)	3.492320321450169
  (942, 18)	4.0041512617349
  (942, 17)	3.18344551002898
  (942, 16)	3.395073294672058
  (942, 15)	3.45620568049849
  (942, 14)	3.9240324616026943
  (942, 13)	4.115275592482997
  (942, 12)	3.4864387665166356
  (942, 11)	4.409728215477655
  (942, 10)	3.8512432316544487
  (942, 9)	3.7392179425130414
  (942, 8)	4.045785785180269
  (942, 7)	3.9008848926184783
  (942, 6)	3.8749252133545737
  (942, 5)	3.6528612453164304
  (942, 4)	3.400222118806973
  (942, 3)	3.435142598823538
  (942, 2)	3.044535125853231
  (942, 1)	3.1633101621912902
  (942, 0)	3.9391230476428336
this is the 116 epoch
rmse loss on training set is 0.9641534957039682
rmse loss on test set is 0.9872293285412587
for this epoch using 114.59985852241516 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992864580200421
  (0, 1680)	3.000000000000002
  (0, 1679)	2.962645245624775
  (0, 1678)	2.9978284634211745
  (0, 1677)	2.927462027828385
  (0, 1676)	2.9981471678399467
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.976887131341967
  (0, 1671)	2.965925417579081
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.976835742898894
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.976835742898894
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9330496137737616
  (0, 1662)	2.976835742898894
  (0, 1661)	2.9706493612698353
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0070443469499457
  :	:
  (942, 24)	3.853719505122434
  (942, 23)	3.572797605652146
  (942, 22)	4.208834456878387
  (942, 21)	4.266374942998402
  (942, 20)	2.9593477463253057
  (942, 19)	3.4937740439967606
  (942, 18)	4.007505602463969
  (942, 17)	3.182874436668942
  (942, 16)	3.395788171517495
  (942, 15)	3.457591990856839
  (942, 14)	3.9242119135119387
  (942, 13)	4.1163584759101
  (942, 12)	3.486999035190291
  (942, 11)	4.411239807790219
  (942, 10)	3.852461624477875
  (942, 9)	3.7413897242748035
  (942, 8)	4.046355540568608
  (942, 7)	3.9019061538467805
  (942, 6)	3.875274259035154
  (942, 5)	3.6557303098415663
  (942, 4)	3.4009726246388823
  (942, 3)	3.435923673933189
  (942, 2)	3.0435703580209403
  (942, 1)	3.162855481095368
  (942, 0)	3.93948476777311
this is the 117 epoch
rmse loss on training set is 0.9639118220404167
rmse loss on test set is 0.9870611017836156
for this epoch using 115.10824823379517 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992801413438147
  (0, 1680)	3.000000000000002
  (0, 1679)	2.962326789844864
  (0, 1678)	2.997806346374737
  (0, 1677)	2.926847233314992
  (0, 1676)	2.998148488960717
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9766160287063874
  (0, 1671)	2.9656620166137677
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9766631617224095
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9766631617224095
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9325095700030817
  (0, 1662)	2.9766631617224095
  (0, 1661)	2.9704241295417098
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.00714337410297
  :	:
  (942, 24)	3.8540699107420133
  (942, 23)	3.5734160958200207
  (942, 22)	4.210936850194374
  (942, 21)	4.2677953496631735
  (942, 20)	2.9578765304760486
  (942, 19)	3.4952102567830496
  (942, 18)	4.010818357872404
  (942, 17)	3.1822940184865094
  (942, 16)	3.396490578653675
  (942, 15)	3.458963858251795
  (942, 14)	3.9243822471589627
  (942, 13)	4.1174136345418795
  (942, 12)	3.487547638164073
  (942, 11)	4.41271803282645
  (942, 10)	3.8536548301129905
  (942, 9)	3.7435312200717843
  (942, 8)	4.046907226574298
  (942, 7)	3.90290658713755
  (942, 6)	3.875614779075814
  (942, 5)	3.6585796719734067
  (942, 4)	3.401709516512575
  (942, 3)	3.4366944288517436
  (942, 2)	3.0426074464467785
  (942, 1)	3.1623963769542853
  (942, 0)	3.939840172723903
this is the 118 epoch
rmse loss on training set is 0.9636736960972011
rmse loss on test set is 0.986895759691128
for this epoch using 119.84699845314026 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992738410617261
  (0, 1680)	3.000000000000002
  (0, 1679)	2.962008412428811
  (0, 1678)	2.9977842194177646
  (0, 1677)	2.9262326054398553
  (0, 1676)	2.9981499849067212
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.976344806321303
  (0, 1671)	2.965399048491955
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.976490626135477
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.976490626135477
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9319699440674953
  (0, 1662)	2.976490626135477
  (0, 1661)	2.9701991934387424
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.007242642893047
  :	:
  (942, 24)	3.8544082892327527
  (942, 23)	3.574014875890135
  (942, 22)	4.213002071543731
  (942, 21)	4.26918985401104
  (942, 20)	2.956411922507779
  (942, 19)	3.4966292369237157
  (942, 18)	4.014090135085159
  (942, 17)	3.18170448840578
  (942, 16)	3.3971807864768624
  (942, 15)	3.4603215295982963
  (942, 14)	3.9245437887453543
  (942, 13)	4.118441869328814
  (942, 12)	3.4880848741625874
  (942, 11)	4.414163894103761
  (942, 10)	3.8548234860819663
  (942, 9)	3.745642909030011
  (942, 8)	4.047441448368799
  (942, 7)	3.9038868096435033
  (942, 6)	3.8759469752807796
  (942, 5)	3.6614095911133897
  (942, 4)	3.4024330793863893
  (942, 3)	3.437455112108296
  (942, 2)	3.041646507399485
  (942, 1)	3.1619330385519246
  (942, 0)	3.9401893525077334
this is the 119 epoch
rmse loss on training set is 0.9634390361096358
rmse loss on test set is 0.9867332317772773
for this epoch using 111.54863739013672 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992675570531704
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9616901129204525
  (0, 1678)	2.9977620822162474
  (0, 1677)	2.9256181436246647
  (0, 1676)	2.9981516522363565
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9760734749852467
  (0, 1671)	2.9651365102054257
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9763181346355045
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9763181346355045
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9314307325703632
  (0, 1662)	2.9763181346355045
  (0, 1661)	2.969974551623202
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.007342146692176
  :	:
  (942, 24)	3.8547350482161398
  (942, 23)	3.5745944233091413
  (942, 22)	4.215031014615647
  (942, 21)	4.270559184150933
  (942, 20)	2.954954016757612
  (942, 19)	3.498031255687971
  (942, 18)	4.017321530548411
  (942, 17)	3.181106073615342
  (942, 16)	3.397859058599268
  (942, 15)	3.4616652460592205
  (942, 14)	3.92469685044155
  (942, 13)	4.1194439560783565
  (942, 12)	3.488611032687055
  (942, 11)	4.415578361995914
  (942, 10)	3.855968212109934
  (942, 9)	3.74772526147638
  (942, 8)	4.047958788322912
  (942, 7)	3.9048474180117574
  (942, 6)	3.876271042281919
  (942, 5)	3.6642203212812805
  (942, 4)	3.4031435914192283
  (942, 3)	3.4382059645593928
  (942, 2)	3.040687652587411
  (942, 1)	3.16146564927744
  (942, 0)	3.9405323949818203
this is the 120 epoch
rmse loss on training set is 0.9632077629594787
rmse loss on test set is 0.9865734499511839
for this epoch using 110.33607482910156 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9926128919631103
  (0, 1680)	3.000000000000002
  (0, 1679)	2.961371890857227
  (0, 1678)	2.9977399344309377
  (0, 1677)	2.9250038472835134
  (0, 1676)	2.9981534875874756
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9758020452013207
  (0, 1671)	2.9648743987590525
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.976145685781528
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.976145685781528
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9308919321463796
  (0, 1662)	2.976145685781528
  (0, 1661)	2.969750202770062
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0074418790061617
  :	:
  (942, 24)	3.855050579167023
  (942, 23)	3.5751552039632113
  (942, 22)	4.217024548965281
  (942, 21)	4.271904042877207
  (942, 20)	2.9535029024795763
  (942, 19)	3.4994165786611533
  (942, 18)	4.020513130260441
  (942, 17)	3.1804989957285406
  (942, 16)	3.3985256520478555
  (942, 15)	3.4629952432111217
  (942, 14)	3.924841731092821
  (942, 13)	4.120420646301465
  (942, 12)	3.489126394343534
  (942, 11)	4.416962374873749
  (942, 10)	3.8570896106649055
  (942, 9)	3.7497787391334083
  (942, 8)	4.0484598069258135
  (942, 7)	3.9057889891000617
  (942, 6)	3.876587167943103
  (942, 5)	3.667012111261038
  (942, 4)	3.403841324157792
  (942, 3)	3.4389472196567423
  (942, 2)	3.0397309893169595
  (942, 1)	3.160994387290754
  (942, 0)	3.9408693860420105
this is the 121 epoch
rmse loss on training set is 0.9629798000643143
rmse loss on test set is 0.9864163484133321
for this epoch using 109.97096467018127 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9925503736825805
  (0, 1680)	3.000000000000002
  (0, 1679)	2.96105374577081
  (0, 1678)	2.99771777571789
  (0, 1677)	2.924389715823737
  (0, 1676)	2.998155487675837
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9755305271847337
  (0, 1671)	2.9646127111713825
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9759732781916277
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9759732781916277
  (0, 1664)	3.000000000000002
  (0, 1663)	2.930353539461533
  (0, 1662)	2.9759732781916277
  (0, 1661)	2.969526145566494
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0075418334722066
  :	:
  (942, 24)	3.855355258118754
  (942, 23)	3.5756976724800436
  (942, 22)	4.218983520710375
  (942, 21)	4.273225108644158
  (942, 20)	2.952058664026193
  (942, 19)	3.5007854659009148
  (942, 18)	4.023665509996381
  (942, 17)	3.1798834709385364
  (942, 16)	3.3991808174563802
  (942, 15)	3.4643117512041854
  (942, 14)	3.9249787168872854
  (942, 13)	4.12137266802892
  (942, 12)	3.489631231158135
  (942, 11)	4.41831684020584
  (942, 10)	3.8581882674800334
  (942, 9)	3.7518037953084855
  (942, 8)	4.048945043666258
  (942, 7)	3.906712080667306
  (942, 6)	3.8768955337375397
  (942, 5)	3.669785204741832
  (942, 4)	3.404526542717678
  (942, 3)	3.439679103704794
  (942, 2)	3.0387766206447493
  (942, 1)	3.160519425682185
  (942, 0)	3.941200409796913
this is the 122 epoch
rmse loss on training set is 0.9627550732725605
rmse loss on test set is 0.9862618635566958
for this epoch using 109.27768158912659 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9924880144523343
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9607356771878273
  (0, 1678)	2.9976956057289543
  (0, 1677)	2.923775748646707
  (0, 1676)	2.998157649293522
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.975258930870089
  (0, 1671)	2.96435144447531
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9758009105404812
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9758009105404812
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9298155512130717
  (0, 1662)	2.9758009105404812
  (0, 1661)	2.969302378711425
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.007642003856525
  :	:
  (942, 24)	3.8556494463364275
  (942, 23)	3.5762222725218726
  (942, 22)	4.220908753206904
  (942, 21)	4.274523036501423
  (942, 20)	2.950621381023398
  (942, 19)	3.502138172087916
  (942, 18)	4.026779235527006
  (942, 17)	3.1792597101683473
  (942, 16)	3.399824799251005
  (942, 15)	3.4656149949167543
  (942, 14)	3.9251080819882533
  (942, 13)	4.122300726598809
  (942, 12)	3.490125806879973
  (942, 11)	4.419642635620626
  (942, 10)	3.8592647520589063
  (942, 9)	3.753800875077955
  (942, 8)	4.049415017877084
  (942, 7)	3.9076172320392564
  (942, 6)	3.8771963150997246
  (942, 5)	3.6725398404543017
  (942, 4)	3.405199505958811
  (942, 3)	3.440401836108365
  (942, 2)	3.0378246455239717
  (942, 1)	3.160040932626475
  (942, 0)	3.9415255487237784
this is the 123 epoch
rmse loss on training set is 0.962533510763696
rmse loss on test set is 0.9861099338729381
for this epoch using 111.50486350059509 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9924258130272983
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9604176846304227
  (0, 1678)	2.9976734241122354
  (0, 1677)	2.923161945148611
  (0, 1676)	2.9981599693074275
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9749872659185397
  (0, 1671)	2.964090595718619
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9756285815569936
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9756285815569936
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9292779641294584
  (0, 1662)	2.9756285815569936
  (0, 1661)	2.9690789009150746
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.007742384051926
  :	:
  (942, 24)	3.8559334909596505
  (942, 23)	3.5767294370698592
  (942, 22)	4.222801047704162
  (942, 21)	4.275798458991902
  (942, 20)	2.9491911285389234
  (942, 19)	3.503474946671487
  (942, 18)	4.029854862831738
  (942, 17)	3.1786279192160376
  (942, 16)	3.4004578358295405
  (942, 15)	3.4669051941045477
  (942, 14)	3.925230089132613
  (942, 13)	4.123205505415978
  (942, 12)	3.4906103772721617
  (942, 11)	4.4209406099311295
  (942, 10)	3.8603196181643313
  (942, 9)	3.7557704154661256
  (942, 8)	4.0498702295449664
  (942, 7)	3.9085049647505876
  (942, 6)	3.8774896817538718
  (942, 5)	3.6752762523023734
  (942, 4)	3.4058604666552754
  (942, 3)	3.441115629611001
  (942, 2)	3.0368751589451666
  (942, 1)	3.159559071531313
  (942, 0)	3.94184488380771
this is the 124 epoch
rmse loss on training set is 0.9623150429534567
rmse loss on test set is 0.985960499863386
for this epoch using 110.79893898963928 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9923637681565625
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9600997676168643
  (0, 1678)	2.9976512305126097
  (0, 1677)	2.922548304721125
  (0, 1676)	2.998162444657692
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.97471554172477
  (0, 1671)	2.9638301619645095
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9754562900220423
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9754562900220423
  (0, 1664)	3.000000000000002
  (0, 1663)	2.928740774970316
  (0, 1662)	2.9754562900220423
  (0, 1661)	2.9688557108985765
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0078429680755097
  :	:
  (942, 24)	3.856207725616218
  (942, 23)	3.5772195887001472
  (942, 22)	4.2246611839801735
  (942, 21)	4.277051987013629
  (942, 20)	2.9477679772446526
  (942, 19)	3.5047960340103033
  (942, 18)	4.032892938306171
  (942, 17)	3.1779882988952557
  (942, 16)	3.401080159734748
  (942, 15)	3.4681825635448433
  (942, 14)	3.9253449901971895
  (942, 13)	4.1240876666846
  (942, 12)	3.491085190391557
  (942, 11)	4.422211584123796
  (942, 10)	3.8613534042913056
  (942, 9)	3.7577128456194155
  (942, 8)	4.050311160086434
  (942, 7)	3.909375783163709
  (942, 6)	3.877775798020467
  (942, 5)	3.677994669490769
  (942, 4)	3.4065096716598027
  (942, 3)	3.441820690524166
  (942, 2)	3.035928252071573
  (942, 1)	3.1590740011806884
  (942, 0)	3.942158494665972
this is the 125 epoch
rmse loss on training set is 0.9620996024036645
rmse loss on test set is 0.9858135039545342
for this epoch using 109.89022135734558 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992301878584797
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9597819256621154
  (0, 1678)	2.9976290245720945
  (0, 1677)	2.9219348267521434
  (0, 1676)	2.998165072356203
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9744437674237516
  (0, 1671)	2.9635701402921373
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.975284034766294
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.975284034766294
  (0, 1664)	3.000000000000002
  (0, 1663)	2.928203980526311
  (0, 1662)	2.975284034766294
  (0, 1661)	2.9686328073935435
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0079437500663078
  :	:
  (942, 24)	3.8564724710080265
  (942, 23)	3.5776931398518337
  (942, 22)	4.2264899209578815
  (942, 21)	4.278284210647199
  (942, 20)	2.946351993572975
  (942, 19)	3.5061016735084
  (942, 18)	4.035893998964068
  (942, 17)	3.1773410451712825
  (942, 16)	3.401691997821788
  (942, 15)	3.4694473131757833
  (942, 14)	3.9254530267348833
  (942, 13)	4.124947852114963
  (942, 12)	3.4915504868575202
  (942, 11)	4.423456352312584
  (942, 10)	3.862366634124648
  (942, 9)	3.7596285869758654
  (942, 8)	4.050738273092012
  (942, 7)	3.910230175065666
  (942, 6)	3.878054823102263
  (942, 5)	3.680695316648273
  (942, 4)	3.4071473620630437
  (942, 3)	3.4425172189478808
  (942, 2)	3.034984012369392
  (942, 1)	3.1585858758733054
  (942, 0)	3.942466459658499
this is the 126 epoch
rmse loss on training set is 0.9618871237364734
rmse loss on test set is 0.9856688904177826
for this epoch using 112.3667094707489 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992240143053548
  (0, 1680)	3.000000000000002
  (0, 1679)	2.959464158278346
  (0, 1678)	2.9976068059303014
  (0, 1677)	2.9213215106263917
  (0, 1676)	2.9981678494850543
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.97417195189742
  (0, 1671)	2.9633105277970304
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.975111814668148
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.975111814668148
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9276675776191157
  (0, 1662)	2.975111814668148
  (0, 1661)	2.968410189141733
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0080447242829824
  :	:
  (942, 24)	3.856728035470441
  (942, 23)	3.578150493087161
  (942, 22)	4.228287997302743
  (942, 21)	4.279495699949943
  (942, 20)	2.944943239867493
  (942, 19)	3.5073920997465997
  (942, 18)	4.038858572634268
  (942, 17)	3.176686349292807
  (942, 16)	3.402293571420167
  (942, 15)	3.4706996482310064
  (942, 14)	3.9255544304820353
  (942, 13)	4.125786683605109
  (942, 12)	3.492006500110369
  (942, 11)	4.424675682659559
  (942, 10)	3.863359816981825
  (942, 9)	3.7615180534299673
  (942, 8)	4.0511520150393565
  (942, 7)	3.9110686122434792
  (942, 6)	3.8783269113512775
  (942, 5)	3.683378413947097
  (942, 4)	3.4077737733479347
  (942, 3)	3.4432054089829798
  (942, 2)	3.0340425237331314
  (942, 1)	3.15809484555616
  (942, 0)	3.942768855985962
this is the 127 epoch
rmse loss on training set is 0.9616775435527776
rmse loss on test set is 0.9855266052931813
for this epoch using 110.91169381141663 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992178560302444
  (0, 1680)	3.000000000000002
  (0, 1679)	2.959146464975439
  (0, 1678)	2.997584574224816
  (0, 1677)	2.9207083557260654
  (0, 1676)	2.998170773195025
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9739001037810966
  (0, 1671)	2.963051321591565
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.974939628651762
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.974939628651762
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9271315631012853
  (0, 1662)	2.974939628651762
  (0, 1661)	2.9681878548946834
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008145885101545
  :	:
  (942, 24)	3.856974715506412
  (942, 23)	3.578592041344226
  (942, 22)	4.23005613200234
  (942, 21)	4.280687005718336
  (942, 20)	2.9435417745283137
  (942, 19)	3.5086675426095875
  (942, 18)	4.041787178152425
  (942, 17)	3.1760243979195297
  (942, 16)	3.402885096490243
  (942, 15)	3.4719397693697713
  (942, 14)	3.925649423838817
  (942, 13)	4.126604763898683
  (942, 12)	3.4924534566597383
  (942, 11)	4.425870318263166
  (942, 10)	3.8643334482414637
  (942, 9)	3.763381651493276
  (942, 8)	4.0515528159770895
  (942, 7)	3.9118915510390138
  (942, 6)	3.8785922125179613
  (942, 5)	3.686044177218378
  (942, 4)	3.4083891355392475
  (942, 3)	3.4438854489354576
  (942, 2)	3.033103866606222
  (942, 1)	3.157601055953507
  (942, 0)	3.9430657597766707
this is the 128 epoch
rmse loss on training set is 0.9614708003545247
rmse loss on test set is 0.9853865963169398
for this epoch using 111.17748689651489 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9921171290703557
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9588288452615017
  (0, 1678)	2.997562329091587
  (0, 1677)	2.920095361431409
  (0, 1676)	2.9981738407041267
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.973628231469812
  (0, 1671)	2.962792518805363
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9747674756851232
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9747674756851232
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9265959338561296
  (0, 1662)	2.9747674756851232
  (0, 1661)	2.967965803413401
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008247227013123
  :	:
  (942, 24)	3.857212796296404
  (942, 23)	3.579018168182378
  (942, 22)	4.2317950249285365
  (942, 21)	4.2818586602199264
  (942, 20)	2.94214765215206
  (942, 19)	3.509928227408765
  (942, 18)	4.044680325547858
  (942, 17)	3.1753553732457136
  (942, 16)	3.403466783774633
  (942, 15)	3.473167872802857
  (942, 14)	3.9257382203239364
  (942, 13)	4.127402677219428
  (942, 12)	3.492891576323442
  (942, 11)	4.4270409780154
  (942, 10)	3.865288009758144
  (942, 9)	3.765219780450678
  (942, 8)	4.0519410901802555
  (942, 7)	3.912699432883969
  (942, 6)	3.878850871983924
  (942, 5)	3.6886928180639607
  (942, 4)	3.4089936733484576
  (942, 3)	3.444557521513092
  (942, 2)	3.032168118097196
  (942, 1)	3.1571046486914276
  (942, 0)	3.9433572461631536
this is the 129 epoch
rmse loss on training set is 0.9612668344707705
rmse loss on test set is 0.9852488128525438
for this epoch using 111.70631265640259 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.992055848096477
  (0, 1680)	3.000000000000002
  (0, 1679)	2.958511298643269
  (0, 1678)	2.997540070165267
  (0, 1677)	2.919482527121272
  (0, 1676)	2.9981770492960798
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.973356343124454
  (0, 1671)	2.962534116585654
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9745953547782404
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9745953547782404
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9260606867976398
  (0, 1662)	2.9745953547782404
  (0, 1661)	2.9677440334680147
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0083487446217236
  :	:
  (942, 24)	3.857442552185346
  (942, 23)	3.5794292480206362
  (942, 22)	4.233505357382667
  (942, 21)	4.28301117789585
  (942, 20)	2.9407609236669225
  (942, 19)	3.511174375001193
  (942, 18)	4.047538516225701
  (942, 17)	3.174679453119977
  (942, 16)	3.404038838944662
  (942, 15)	3.4743841504141892
  (942, 14)	3.925821025005137
  (942, 13)	4.128180989883521
  (942, 12)	3.4933210724571433
  (942, 11)	4.428188357428815
  (942, 10)	3.866223970263799
  (942, 9)	3.7670328325126072
  (942, 8)	4.052317236778742
  (942, 7)	3.9134926848157323
  (942, 6)	3.879103030979185
  (942, 5)	3.6913245439646802
  (942, 4)	3.40958760631423
  (942, 3)	3.445221804014815
  (942, 2)	3.0312353520915383
  (942, 1)	3.1566057614181857
  (942, 0)	3.9436433893496314
this is the 130 epoch
rmse loss on training set is 0.9610655879871388
rmse loss on test set is 0.9851132058251901
for this epoch using 109.7890853881836 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9919947161213196
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9581938246265875
  (0, 1678)	2.997517797079545
  (0, 1677)	2.9188698521736374
  (0, 1676)	2.9981803963188787
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9730844466777424
  (0, 1671)	2.9622761120976566
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.974423264981409
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.974423264981409
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9255258188703333
  (0, 1662)	2.974423264981409
  (0, 1661)	2.967522543837514
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008450432642039
  :	:
  (942, 24)	3.8576642471475515
  (942, 23)	3.579825646369273
  (942, 22)	4.235187792624418
  (942, 21)	4.284145056035424
  (942, 20)	2.9393816364628766
  (942, 19)	3.512406201904518
  (942, 18)	4.050362243144391
  (942, 17)	3.1739968111613055
  (942, 16)	3.404601462741969
  (942, 15)	3.4755887898785534
  (942, 14)	3.9258980349067953
  (942, 13)	4.12894025089035
  (942, 12)	3.4937421521752228
  (942, 11)	4.429313129434581
  (942, 10)	3.867141785756244
  (942, 9)	3.7688211929633124
  (942, 8)	4.052681640359718
  (942, 7)	3.9142717199748214
  (942, 6)	3.879348826785062
  (942, 5)	3.6939395583851793
  (942, 4)	3.4101711489386064
  (942, 3)	3.4458784685129857
  (942, 2)	3.030305639359392
  (942, 1)	3.1561045279204776
  (942, 0)	3.9439242626710285
this is the 131 epoch
rmse loss on training set is 0.9608670046787353
rmse loss on test set is 0.9849797276594321
for this epoch using 109.64854598045349 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991933731887636
  (0, 1680)	3.000000000000002
  (0, 1679)	2.957876422716789
  (0, 1678)	2.997495509467478
  (0, 1677)	2.9182573359661106
  (0, 1676)	2.998183879183343
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.972812549840104
  (0, 1671)	2.962018502524873
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9742512053835495
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9742512053835495
  (0, 1664)	3.000000000000002
  (0, 1663)	2.924991327049111
  (0, 1662)	2.9742512053835495
  (0, 1661)	2.967301333309469
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008552285897293
  :	:
  (942, 24)	3.8578781352306137
  (942, 23)	3.580207720054918
  (942, 22)	4.236842976384801
  (942, 21)	4.285260775423614
  (942, 20)	2.938009834517263
  (942, 19)	3.513623920408288
  (942, 18)	4.053151990988722
  (942, 17)	3.1733076168715177
  (942, 16)	3.405154851115588
  (942, 15)	3.4767819747754998
  (942, 14)	3.925969439395889
  (942, 13)	4.129680992492718
  (942, 12)	3.4941550165632735
  (942, 11)	4.430415945152571
  (942, 10)	3.8680418998752866
  (942, 9)	3.7705852403052864
  (942, 8)	4.053034671545235
  (942, 7)	3.915036938084544
  (942, 6)	3.8795883929237287
  (942, 5)	3.696538060875511
  (942, 4)	3.410744510818973
  (942, 3)	3.4465276820289263
  (942, 2)	3.029379047659357
  (942, 1)	3.1556010782357533
  (942, 0)	3.9441999386446116
this is the 132 epoch
rmse loss on training set is 0.96067102994605
rmse loss on test set is 0.9848483322198025
for this epoch using 108.76756620407104 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9918728941413337
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9575590924191038
  (0, 1678)	2.997473206961798
  (0, 1677)	2.9176449778764098
  (0, 1676)	2.9981874953616914
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.972540660105349
  (0, 1671)	2.9617612850694193
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9740791751105737
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9740791751105737
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9244572083391525
  (0, 1662)	2.9740791751105737
  (0, 1661)	2.9670804006797655
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0086542993171155
  :	:
  (942, 24)	3.858084460979328
  (942, 23)	3.5805758174391795
  (942, 22)	4.238471537363696
  (942, 21)	4.286358800962735
  (942, 20)	2.936645558515944
  (942, 19)	3.5148277386816345
  (942, 18)	4.055908236338503
  (942, 17)	3.1726120357442595
  (942, 16)	3.4056991953545372
  (942, 15)	3.4779638846994594
  (942, 14)	3.9260354205473744
  (942, 13)	4.130403730746956
  (942, 12)	3.494559860882423
  (942, 11)	4.431497434634377
  (942, 10)	3.868924744266899
  (942, 9)	3.772325346399969
  (942, 8)	4.0533766875459945
  (942, 7)	3.9157887259135395
  (942, 6)	3.8798218593353986
  (942, 5)	3.6991202471694904
  (942, 4)	3.4113078967760675
  (942, 3)	3.4471696067019293
  (942, 2)	3.0284556418384883
  (942, 1)	3.155095538760809
  (942, 0)	3.944470489014783
this is the 133 epoch
rmse loss on training set is 0.9604776107539088
rmse loss on test set is 0.9847189747543023
for this epoch using 107.89059901237488 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991812201632264
  (0, 1680)	3.000000000000002
  (0, 1679)	2.957241833239005
  (0, 1678)	2.997450889195202
  (0, 1677)	2.9170327772828126
  (0, 1676)	2.998191242386111
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9722687847562286
  (0, 1671)	2.961504456952268
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.973907173323903
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.973907173323903
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9239234597757178
  (0, 1662)	2.973907173323903
  (0, 1661)	2.9668597447523486
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008756467935421
  :	:
  (942, 24)	3.8582834598405173
  (942, 23)	3.5809302786312673
  (942, 22)	4.240074087712544
  (942, 21)	4.287439582269335
  (942, 20)	2.935288845970185
  (942, 19)	3.516017860877517
  (942, 18)	4.058631447833156
  (942, 17)	3.1719102293706887
  (942, 16)	3.4062346822161964
  (942, 15)	3.4791346953664495
  (942, 14)	3.9260961534904006
  (942, 13)	4.13110896604394
  (942, 12)	3.4949568747659767
  (942, 11)	4.432558207580401
  (942, 10)	3.8697907389356523
  (942, 9)	3.774041876604948
  (942, 8)	4.053708032692223
  (942, 7)	3.916527457721862
  (942, 6)	3.880049352543807
  (942, 5)	3.701686309280072
  (942, 4)	3.411861506978193
  (942, 3)	3.4478043999521586
  (942, 2)	3.02753548392869
  (942, 1)	3.154588032356755
  (942, 0)	3.9447359847918366
this is the 134 epoch
rmse loss on training set is 0.9602866955731927
rmse loss on test set is 0.9845916118405499
for this epoch using 108.84055614471436 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991751653114984
  (0, 1680)	3.000000000000002
  (0, 1679)	2.956924644682585
  (0, 1678)	2.997428555800608
  (0, 1677)	2.916420733564566
  (0, 1676)	2.998195117847417
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9719969308698695
  (0, 1671)	2.9612480154135383
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9737351992189516
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9737351992189516
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9233900784240356
  (0, 1662)	2.9737351992189516
  (0, 1661)	2.966639364339024
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0088587868883523
  :	:
  (942, 24)	3.858475358549547
  (942, 23)	3.581271435694601
  (942, 22)	4.241651223502575
  (942, 21)	4.288503554247189
  (942, 20)	2.9339397313294207
  (942, 19)	3.517194487233716
  (942, 18)	4.061322086332035
  (942, 17)	3.1712023555419564
  (942, 16)	3.406761494050542
  (942, 15)	3.480294578717231
  (942, 14)	3.9261518067360126
  (942, 13)	4.131797183621604
  (942, 12)	3.495346242408665
  (942, 11)	4.433598854031758
  (942, 10)	3.870640292586086
  (942, 9)	3.7757351899076084
  (942, 8)	4.0540290389427245
  (942, 7)	3.9172534956910527
  (942, 6)	3.8802709958110086
  (942, 5)	3.704236435591811
  (942, 4)	3.412405537061585
  (942, 3)	3.4484322146373807
  (942, 2)	3.026618633239631
  (942, 1)	3.154078678450506
  (942, 0)	3.9449964962852757
this is the 135 epoch
rmse loss on training set is 0.9600982343252223
rmse loss on test set is 0.9844662013344738
for this epoch using 107.64623475074768 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991691247349498
  (0, 1680)	3.000000000000002
  (0, 1679)	2.956607526256866
  (0, 1678)	2.997406206411431
  (0, 1677)	2.9158088461023026
  (0, 1676)	2.998199119393646
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9717251053230362
  (0, 1671)	2.9609919577127104
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.973563252023766
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.973563252023766
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9228570613791134
  (0, 1662)	2.973563252023766
  (0, 1661)	2.9664192582592004
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.008961251412235
  :	:
  (942, 24)	3.8586603754994457
  (942, 23)	3.5815996128476804
  (942, 22)	4.243203525179017
  (942, 21)	4.289551137637555
  (942, 20)	2.9325982460900923
  (942, 19)	3.5183578141705807
  (942, 18)	4.063980605071004
  (942, 17)	3.1704885683485635
  (942, 16)	3.407279808920399
  (942, 15)	3.4814437030172414
  (942, 14)	3.9262025424876987
  (942, 13)	4.132468854059599
  (942, 12)	3.4957281427486615
  (942, 11)	4.434619945037978
  (942, 10)	3.871473802953191
  (942, 9)	3.7774056390554693
  (942, 8)	4.054340026372989
  (942, 7)	3.917967190339032
  (942, 6)	3.8804869092821117
  (942, 5)	3.7067708109504163
  (942, 4)	3.4129401782472466
  (942, 3)	3.449053199204138
  (942, 2)	3.0257051464483027
  (942, 1)	3.1535675931329004
  (942, 0)	3.9452520931322117
this is the 136 epoch
rmse loss on training set is 0.9599121783286229
rmse loss on test set is 0.9843427023213946
for this epoch using 109.64663004875183 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99163098310186
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9562904774701244
  (0, 1678)	2.997383840661831
  (0, 1677)	2.9151971142784134
  (0, 1676)	2.9982032447287406
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.971453314797283
  (0, 1671)	2.9607362811288342
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9733913309976705
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9733913309976705
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9223244057655866
  (0, 1662)	2.9733913309976705
  (0, 1661)	2.9661994253397177
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.009063856841572
  :	:
  (942, 24)	3.8588387210935147
  (942, 23)	3.5819151266593976
  (942, 22)	4.244731558001796
  (942, 21)	4.29058273954748
  (942, 20)	2.931264418900727
  (942, 19)	3.5195080343857206
  (942, 18)	4.066607449815017
  (942, 17)	3.169769018276798
  (942, 16)	3.407789800717966
  (942, 15)	3.482582232953429
  (942, 14)	3.9262485169354613
  (942, 13)	4.133124433756757
  (942, 12)	3.4961027496428727
  (942, 11)	4.435622033301383
  (942, 10)	3.8722916571224384
  (942, 9)	3.77905357068331
  (942, 8)	4.054641303643102
  (942, 7)	3.918668880920119
  (942, 6)	3.8806972101207413
  (942, 5)	3.709289616749722
  (942, 4)	3.413465617454347
  (942, 3)	3.449667497833286
  (942, 2)	3.024795077685434
  (942, 1)	3.1530548892536525
  (942, 0)	3.945502844321513
this is the 137 epoch
rmse loss on training set is 0.9597284802486163
rmse loss on test set is 0.9842210750694058
for this epoch using 108.89555644989014 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9915708591448316
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9559734978321646
  (0, 1678)	2.9973614581869144
  (0, 1677)	2.9145855374774157
  (0, 1676)	2.9982074916112085
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.971181565783983
  (0, 1671)	2.9604809829607395
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9732194354299564
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9732194354299564
  (0, 1664)	3.000000000000002
  (0, 1663)	2.921792108737533
  (0, 1662)	2.9732194354299564
  (0, 1661)	2.965979864414648
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.009166598607077
  :	:
  (942, 24)	3.8590105980819187
  (942, 23)	3.5822182862389638
  (942, 22)	4.246235872473012
  (942, 21)	4.291598753957084
  (942, 20)	2.9299382756632792
  (942, 19)	3.520645336945762
  (942, 18)	4.069203059007048
  (942, 17)	3.1690438523021895
  (942, 16)	3.4082916392775537
  (942, 15)	3.4837103297278564
  (942, 14)	3.9262898805345445
  (942, 13)	4.1337643653921
  (942, 12)	3.496470232035614
  (942, 11)	4.436605653798825
  (942, 10)	3.8730942318396804
  (942, 9)	3.780679325437024
  (942, 8)	4.054933168446496
  (942, 7)	3.91935889581086
  (942, 6)	3.880902012635691
  (942, 5)	3.7117930310158536
  (942, 4)	3.4139820374101695
  (942, 3)	3.4502752505803085
  (942, 2)	3.0238884786188467
  (942, 1)	3.152540676513155
  (942, 0)	3.945748818213946
this is the 138 epoch
rmse loss on training set is 0.9595470940485397
rmse loss on test set is 0.9841012809848536
for this epoch using 109.23608088493347 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.99151087425842
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9556565868546363
  (0, 1678)	2.9973390586230035
  (0, 1677)	2.9139741150862704
  (0, 1676)	2.998211857852834
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9709098645892325
  (0, 1671)	2.960226060527193
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.973047564638676
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.973047564638676
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9212601674783127
  (0, 1662)	2.973047564638676
  (0, 1661)	2.965760574325095
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.009269472233707
  :	:
  (942, 24)	3.8591762018833355
  (942, 23)	3.582509393420619
  (942, 22)	4.247717004751777
  (942, 21)	4.292599562206705
  (942, 20)	2.9286198396310708
  (942, 19)	3.5217699073752957
  (942, 18)	4.07176786391346
  (942, 17)	3.1683132139802788
  (942, 16)	3.408785490484845
  (942, 15)	3.4848281511486117
  (942, 14)	3.92632677826943
  (942, 13)	4.134389078369759
  (942, 12)	3.496830754121022
  (942, 11)	4.437571324381873
  (942, 10)	3.8738818938114097
  (942, 9)	3.782283238094602
  (942, 8)	4.055215907940229
  (942, 7)	3.9200375528821665
  (942, 6)	3.8811014283996847
  (942, 5)	3.714281228489086
  (942, 4)	3.4144896167569434
  (942, 3)	3.450876593510515
  (942, 2)	3.022985398533808
  (942, 1)	3.1520250615513823
  (942, 0)	3.945990082559031
this is the 139 epoch
rmse loss on training set is 0.9593679749435152
rmse loss on test set is 0.9839832825699131
for this epoch using 109.06624031066895 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9914510272304247
  (0, 1680)	3.000000000000002
  (0, 1679)	2.955339744051251
  (0, 1678)	2.997316641607789
  (0, 1677)	2.9133628464947128
  (0, 1676)	2.998216341317405
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.970638217338584
  (0, 1671)	2.959971511167063
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9728757179694627
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9728757179694627
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9207285792003597
  (0, 1662)	2.9728757179694627
  (0, 1661)	2.9655415539190595
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0093724733387908
  :	:
  (942, 24)	3.8593357208919667
  (942, 23)	3.5827887429433325
  (942, 22)	4.249175477056679
  (942, 21)	4.293585533464641
  (942, 20)	2.9273091315033124
  (942, 19)	3.522881927743022
  (942, 18)	4.074302288765728
  (942, 17)	3.167577243534669
  (942, 16)	3.4092715163827565
  (942, 15)	3.4859358517176813
  (942, 14)	3.9263593499040614
  (942, 13)	4.134998989248651
  (942, 12)	3.4971844754994312
  (942, 11)	4.438519546355836
  (942, 10)	3.8746549999954363
  (942, 9)	3.7838656376840087
  (942, 8)	4.055489799157576
  (942, 7)	3.920705159858122
  (942, 6)	3.881295566360472
  (942, 5)	3.7167543807030747
  (942, 4)	3.414988530155421
  (942, 3)	3.451471658829347
  (942, 2)	3.0220858844106706
  (942, 1)	3.1515081480338383
  (942, 0)	3.946226704508726
this is the 140 epoch
rmse loss on training set is 0.959191079356097
rmse loss on test set is 0.9838670433820479
for this epoch using 112.83839964866638 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9913913168568853
  (0, 1680)	3.000000000000002
  (0, 1679)	2.955022968938081
  (0, 1678)	2.9972942067805817
  (0, 1677)	2.9127517310955757
  (0, 1676)	2.9982209399194386
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.970366629981736
  (0, 1671)	2.959717332239455
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9727038947943747
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9727038947943747
  (0, 1664)	3.000000000000002
  (0, 1663)	2.920197341145015
  (0, 1662)	2.9727038947943747
  (0, 1661)	2.9653228020512397
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.009475597630103
  :	:
  (942, 24)	3.8594893367710044
  (942, 23)	3.583056622625541
  (942, 22)	4.250611798056234
  (942, 21)	4.294557025176433
  (942, 20)	2.9260061695163504
  (942, 19)	3.523981576745317
  (942, 18)	4.076806750898877
  (942, 17)	3.166836077942452
  (942, 16)	3.409749875273955
  (942, 15)	3.487033582716224
  (942, 14)	3.926387730218939
  (942, 13)	4.135594502157281
  (942, 12)	3.4975315513279646
  (942, 11)	4.439450805038719
  (942, 10)	3.875413897882547
  (942, 9)	3.7854268475982837
  (942, 8)	4.055755109403653
  (942, 7)	3.921362014662261
  (942, 6)	3.8814845329449943
  (942, 5)	3.719212656061905
  (942, 4)	3.415478948385518
  (942, 3)	3.4520605750079976
  (942, 2)	3.021189980999717
  (942, 1)	3.1509900367348087
  (942, 0)	3.946458750628473
this is the 141 epoch
rmse loss on training set is 0.9590163648739605
rmse loss on test set is 0.9837525279953018
for this epoch using 110.38665246963501 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9913317419425502
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9547062610337362
  (0, 1678)	2.9972717537824436
  (0, 1677)	2.9121407682850315
  (0, 1676)	2.9982256516229837
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9700951082970444
  (0, 1671)	2.95946352112386
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.972532094510842
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.972532094510842
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9196664505823406
  (0, 1662)	2.972532094510842
  (0, 1661)	2.965104317582921
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0095788409040543
  :	:
  (942, 24)	3.8596372247327304
  (942, 23)	3.5833133135351876
  (942, 22)	4.252026463247846
  (942, 21)	4.295514383496293
  (942, 20)	2.9247109695318856
  (942, 19)	3.5250690297872405
  (942, 18)	4.079281660886604
  (942, 17)	3.1660898510171855
  (942, 16)	3.4102207218201954
  (942, 15)	3.4881214922871666
  (942, 14)	3.926412049235982
  (942, 13)	4.136176009194291
  (942, 12)	3.497872132465552
  (942, 11)	4.4403655703006475
  (942, 10)	3.876158925769356
  (942, 9)	3.786967185707811
  (942, 8)	4.0560120966348645
  (942, 7)	3.922008405751343
  (942, 6)	3.881668432157116
  (942, 5)	3.721656219914743
  (942, 4)	3.4159610384440313
  (942, 3)	3.4526434669045485
  (942, 2)	3.0202977308934527
  (942, 1)	3.1504708256179463
  (942, 0)	3.9466862869059716
this is the 142 epoch
rmse loss on training set is 0.9588437902092887
rmse loss on test set is 0.9836397019633389
for this epoch using 106.7142825126648 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991272301301275
  (0, 1680)	3.000000000000002
  (0, 1679)	2.954389619859639
  (0, 1678)	2.9972492822563996
  (0, 1677)	2.911529957462882
  (0, 1676)	2.9982304744403905
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9698236578959523
  (0, 1671)	2.959210075220205
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9723603165406027
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9723603165406027
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9191359048109007
  (0, 1662)	2.9723603165406027
  (0, 1661)	2.9648860993817987
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.009682199043852
  :	:
  (942, 24)	3.85977955380627
  (942, 23)	3.583559090155221
  (942, 22)	4.253419955325511
  (942, 21)	4.296457943701438
  (942, 20)	2.923423545122058
  (942, 19)	3.526144459061082
  (942, 18)	4.081727422673122
  (942, 17)	3.1653386934894283
  (942, 16)	3.4106842071386594
  (942, 15)	3.4891997255153187
  (942, 14)	3.9264324324315423
  (942, 13)	4.136743890815222
  (942, 12)	3.49820636561265
  (942, 11)	4.441264297084383
  (942, 10)	3.876890413022593
  (942, 9)	3.7884869644699566
  (942, 8)	4.056261009822797
  (942, 7)	3.9226446124374763
  (942, 6)	3.8818473656692105
  (942, 5)	3.7240852346283932
  (942, 4)	3.416434963639479
  (942, 3)	3.4532204558807456
  (942, 2)	3.0194091745964355
  (942, 1)	3.1499506099142773
  (942, 0)	3.9469093787576846
this is the 143 epoch
rmse loss on training set is 0.9586733151599837
rmse loss on test set is 0.9835285317841173
for this epoch using 108.27406883239746 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9912129937563976
  (0, 1680)	3.000000000000002
  (0, 1679)	2.954073044940197
  (0, 1678)	2.99722679184759
  (0, 1677)	2.9109192980327996
  (0, 1676)	2.9982354064311365
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.969552284227282
  (0, 1671)	2.9589569919490284
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9721885603287475
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9721885603287475
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9186057011576065
  (0, 1662)	2.9721885603287475
  (0, 1661)	2.964668146321897
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0097856680177286
  :	:
  (942, 24)	3.859916487093185
  (942, 23)	3.583794220544529
  (942, 22)	4.254792744536616
  (942, 21)	4.297388030590123
  (942, 20)	2.9221439076517677
  (942, 19)	3.527208033622629
  (942, 18)	4.084144433701936
  (942, 17)	3.1645827330849103
  (942, 16)	3.411140478895282
  (942, 15)	3.490268424504996
  (942, 14)	3.9264490009385464
  (942, 13)	4.137298516206084
  (942, 12)	3.4985343934458273
  (942, 11)	4.44214742590792
  (942, 10)	3.877608680335219
  (942, 9)	3.789986491036036
  (942, 8)	4.0565020893031285
  (942, 7)	3.9232709051987285
  (942, 6)	3.8820214329081684
  (942, 5)	3.7264998596576953
  (942, 4)	3.4169008836842547
  (942, 3)	3.453791659914594
  (942, 2)	3.0185243505926205
  (942, 1)	3.149429482197765
  (942, 0)	3.947128091033814
this is the 144 epoch
rmse loss on training set is 0.9585049005724775
rmse loss on test set is 0.9834189848661136
for this epoch using 111.79440212249756 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.991153818141058
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9537565358029934
  (0, 1678)	2.9972042822034046
  (0, 1677)	2.910308789402579
  (0, 1676)	2.99824044570065
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.969280992581447
  (0, 1671)	2.958704268751468
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.972016825342714
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.972016825342714
  (0, 1664)	3.000000000000002
  (0, 1663)	2.918075836977467
  (0, 1662)	2.972016825342714
  (0, 1661)	2.964450457283376
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0098892438771894
  :	:
  (942, 24)	3.8600481820118597
  (942, 23)	3.5840189664946718
  (942, 22)	4.256145289028225
  (942, 21)	4.298304958863895
  (942, 20)	2.92087206635811
  (942, 19)	3.5282599194650155
  (942, 18)	4.086533085041605
  (942, 17)	3.163822094600509
  (942, 16)	3.4115896813952533
  (942, 15)	3.4913277284553272
  (942, 14)	3.926461871737995
  (942, 13)	4.137840243644052
  (942, 12)	3.4988563547473865
  (942, 11)	4.443015383349353
  (942, 10)	3.8783140399745606
  (942, 9)	3.7914660673558034
  (942, 8)	4.056735567110353
  (942, 7)	3.9238875459788156
  (942, 6)	3.882190731136266
  (942, 5)	3.728900251613966
  (942, 4)	3.417358954784165
  (942, 3)	3.4543571937089896
  (942, 2)	3.0176432954105272
  (942, 1)	3.1489075324585096
  (942, 0)	3.9473424880214876
this is the 145 epoch
rmse loss on training set is 0.9583385083061196
rmse loss on test set is 0.9833110294960458
for this epoch using 106.69339537620544 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9910947732985522
  (0, 1680)	3.000000000000002
  (0, 1679)	2.953440091979
  (0, 1678)	2.997181752973671
  (0, 1677)	2.9096984309843363
  (0, 1676)	2.998245590399207
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.969009788094556
  (0, 1671)	2.9584519030894167
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9718451110714192
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9718451110714192
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9175463096534187
  (0, 1662)	2.9718451110714192
  (0, 1661)	2.9642330311524896
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0099929227552797
  :	:
  (942, 24)	3.8601747905307615
  (942, 23)	3.5842335836823835
  (942, 22)	4.257478035183169
  (942, 21)	4.299209033494787
  (942, 20)	2.919608028427205
  (942, 19)	3.5293002795905575
  (942, 18)	4.088893761508476
  (942, 17)	3.163056899977956
  (942, 16)	3.4120319556707086
  (942, 15)	3.492377773733257
  (942, 14)	3.9264711578407927
  (942, 13)	4.138369420845909
  (942, 12)	3.4991723845304086
  (942, 11)	4.443868582515024
  (942, 10)	3.87900679602276
  (942, 9)	3.7929259902793837
  (942, 8)	4.056961667298765
  (942, 7)	3.9244947884761805
  (942, 6)	3.8823553555270296
  (942, 5)	3.7312865643313757
  (942, 4)	3.417809329725308
  (942, 3)	3.454917168796488
  (942, 2)	3.0167660436861037
  (942, 1)	3.1483848481736127
  (942, 0)	3.947552633446748
this is the 146 epoch
rmse loss on training set is 0.9581741011990909
rmse loss on test set is 0.9832046348079625
for this epoch using 108.39000749588013 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9910358580825522
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9531237130027215
  (0, 1678)	2.9971592038107393
  (0, 1677)	2.9090882221947107
  (0, 1676)	2.998250838720799
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.968738675752364
  (0, 1671)	2.958199892445508
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.971673417024378
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.971673417024378
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9170171165961176
  (0, 1662)	2.971673417024378
  (0, 1661)	2.964015866821403
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010096700864919
  :	:
  (942, 24)	3.8602964593915265
  (942, 23)	3.5844383218180282
  (942, 22)	4.258791417946189
  (942, 21)	4.300100550078102
  (942, 20)	2.9183517990683865
  (942, 19)	3.5303292740803442
  (942, 18)	4.091226841786693
  (942, 17)	3.1622872683755316
  (942, 16)	3.4124674395659076
  (942, 15)	3.4934186939444403
  (942, 14)	3.9264769684601126
  (942, 13)	4.138886385304569
  (942, 12)	3.4994826141590822
  (942, 11)	4.444707423491366
  (942, 10)	3.8796872446098574
  (942, 9)	3.7943665516569687
  (942, 8)	4.057180606250338
  (942, 7)	3.9250928784228427
  (942, 6)	3.8825153992367194
  (942, 5)	3.733658948931513
  (942, 4)	3.4182521579586553
  (942, 3)	3.4554716936403533
  (942, 2)	3.015892628223505
  (942, 1)	3.1478615143759163
  (942, 0)	3.9477585904753587
this is the 147 epoch
rmse loss on training set is 0.9580116430356631
rmse loss on test set is 0.9830997707537
for this epoch using 108.75163316726685 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9909770713574093
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9528073984123657
  (0, 1678)	2.9971366343696477
  (0, 1677)	2.9084781624550766
  (0, 1676)	2.9982561889020225
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.968467660394201
  (0, 1671)	2.9579482343231973
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.971501742730848
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.971501742730848
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9164882552437317
  (0, 1662)	2.971501742730848
  (0, 1661)	2.9637989631881707
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010200574497219
  :	:
  (942, 24)	3.860413330321942
  (942, 23)	3.5846334247901415
  (942, 22)	4.260085861140556
  (942, 21)	4.300979795171279
  (942, 20)	2.917103381585951
  (942, 19)	3.5313470601618264
  (942, 18)	4.09353269854529
  (942, 17)	3.1615133162376625
  (942, 16)	3.4128962678196477
  (942, 15)	3.494450620001934
  (942, 14)	3.926479409174961
  (942, 13)	4.139391464614197
  (942, 12)	3.499787171464839
  (942, 11)	4.445532293781001
  (942, 10)	3.8803556741395586
  (942, 9)	3.7957880384360316
  (942, 8)	4.057392592970061
  (942, 7)	3.925682053853503
  (942, 6)	3.882670953471573
  (942, 5)	3.736017553886063
  (942, 4)	3.4186875856820693
  (942, 3)	3.4560208737320735
  (942, 2)	3.0150230800538362
  (942, 1)	3.147337613720563
  (942, 0)	3.9479604217127053
this is the 148 epoch
rmse loss on training set is 0.9578510985148736
rmse loss on test set is 0.9829964080746045
for this epoch using 108.44421362876892 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9909184119983516
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9524911477499747
  (0, 1678)	2.9971140443082307
  (0, 1677)	2.9078682511917147
  (0, 1676)	2.9982616392210724
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9681967467167185
  (0, 1671)	2.9576969262467596
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9713300877390507
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9713300877390507
  (0, 1664)	3.000000000000002
  (0, 1663)	2.915959723061706
  (0, 1662)	2.9713300877390507
  (0, 1661)	2.963582319156586
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.01030454001987
  :	:
  (942, 24)	3.8605255402395238
  (942, 23)	3.5848191308061277
  (942, 22)	4.261361777775324
  (942, 21)	4.301847046619461
  (942, 20)	2.9158627774484565
  (942, 19)	3.532353792274478
  (942, 18)	4.095811698552663
  (942, 17)	3.1607351573625038
  (942, 16)	3.413318572145398
  (942, 15)	3.4954736801929114
  (942, 14)	3.9264785820854353
  (942, 13)	4.139884976784241
  (942, 12)	3.50008618085822
  (942, 11)	4.446343568723796
  (942, 10)	3.8810123655081967
  (942, 9)	3.7971907327564116
  (942, 8)	4.057597829369159
  (942, 7)	3.926262545365032
  (942, 6)	3.882822107551104
  (942, 5)	3.738362525077798
  (942, 4)	3.4191157559201564
  (942, 3)	3.4565648116854604
  (942, 2)	3.014157428491916
  (942, 1)	3.1468132265494835
  (942, 0)	3.9481581892027817
this is the 149 epoch
rmse loss on training set is 0.9576924332204387
rmse loss on test set is 0.9828945182744182
for this epoch using 106.13324308395386 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9908598788917
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9521749605615937
  (0, 1678)	2.997091433287219
  (0, 1677)	2.9072584878359726
  (0, 1676)	2.998267187996628
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9679259392776385
  (0, 1671)	2.95744596576135
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9711584516154024
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9711584516154024
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9154315175425944
  (0, 1662)	2.9711584516154024
  (0, 1661)	2.9633659336360876
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010408593875535
  :	:
  (942, 24)	3.8606332214461245
  (942, 23)	3.5849956725292835
  (942, 22)	4.262619570343644
  (942, 21)	4.302702573868312
  (942, 20)	2.914629986355758
  (942, 19)	3.533349622133524
  (942, 18)	4.098064202788359
  (942, 17)	3.1599529029677327
  (942, 16)	3.4137344813089214
  (942, 15)	3.49648800024336
  (942, 14)	3.9264745859600487
  (942, 13)	4.140367230542835
  (942, 12)	3.5003797634368006
  (942, 11)	4.447141611903222
  (942, 10)	3.8816575923169063
  (942, 9)	3.7985749120431396
  (942, 8)	4.057796510536902
  (942, 7)	3.926834576366955
  (942, 6)	3.88296894896782
  (942, 5)	3.7406940058597447
  (942, 4)	3.419536808601788
  (942, 3)	3.4571036073274337
  (942, 2)	3.013295701191176
  (942, 1)	3.146288430954014
  (942, 0)	3.9483519544266485
this is the 150 epoch
rmse loss on training set is 0.9575356135918768
rmse loss on test set is 0.9827940735933489
for this epoch using 107.29297375679016 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9908014709350286
  (0, 1680)	3.000000000000002
  (0, 1679)	2.951858836397386
  (0, 1678)	2.99706880097035
  (0, 1677)	2.906648871824415
  (0, 1676)	2.998272833586883
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.96765524249929
  (0, 1671)	2.957195350432974
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.970986833943751
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.970986833943751
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9149036362058194
  (0, 1662)	2.970986833943751
  (0, 1661)	2.963149805541732
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0105127325802896
  :	:
  (942, 24)	3.8607365018138218
  (942, 23)	3.585163277212239
  (942, 22)	4.263859631112265
  (942, 21)	4.303546638264616
  (942, 20)	2.913405006303791
  (942, 19)	3.534334698791836
  (942, 18)	4.100290566552407
  (942, 17)	3.159166661754375
  (942, 16)	3.4141441212035835
  (942, 15)	3.4974937033809765
  (942, 14)	3.9264675163755984
  (942, 13)	4.140838525629926
  (942, 12)	3.50066803708915
  (942, 11)	4.447926775538673
  (942, 10)	3.882291621077294
  (942, 9)	3.799940849097183
  (942, 8)	4.057988825001234
  (942, 7)	3.927398363323055
  (942, 6)	3.8831115634436255
  (942, 5)	3.7430121371128067
  (942, 4)	3.4199508806355077
  (942, 3)	3.4576373577856416
  (942, 2)	3.0124379241967145
  (942, 1)	3.1457633028354706
  (942, 0)	3.9485417783002905
this is the 151 epoch
rmse loss on training set is 0.9573806068968755
rmse loss on test set is 0.9826950469831985
for this epoch using 111.31722950935364 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.990743187037343
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9515427748117333
  (0, 1678)	2.99704614702448
  (0, 1677)	2.9060394025989904
  (0, 1676)	2.998278574388524
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.967384660672168
  (0, 1671)	2.956945077848522
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9708152343247125
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9708152343247125
  (0, 1664)	3.000000000000002
  (0, 1663)	2.914376076597464
  (0, 1662)	2.9708152343247125
  (0, 1661)	2.9629339337940603
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010616952722091
  :	:
  (942, 24)	3.8608355049626066
  (942, 23)	3.5853221668269257
  (942, 22)	4.26508234240268
  (942, 21)	4.304379493345061
  (942, 20)	2.912187833647219
  (942, 19)	3.5353091687000786
  (942, 18)	4.102491139572
  (942, 17)	3.1583765399689256
  (942, 16)	3.414547614923569
  (942, 15)	3.4984909103961286
  (942, 14)	3.9264574658500293
  (942, 13)	4.14129915308053
  (942, 12)	3.5009511165952127
  (942, 11)	4.448699400864259
  (942, 10)	3.8829147114109075
  (942, 9)	3.801288812184085
  (942, 8)	4.058174954978849
  (942, 7)	3.927954115984538
  (942, 6)	3.8832500349829715
  (942, 5)	3.7453170573017465
  (942, 4)	3.4203581059828063
  (942, 3)	3.4581661575730824
  (942, 2)	3.011584121996641
  (942, 1)	3.145237915964018
  (942, 0)	3.9487277211720686
this is the 152 epoch
rmse loss on training set is 0.9572273812046419
rmse loss on test set is 0.9825974120835664
for this epoch using 111.9474048614502 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.990685026119205
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9512267753633856
  (0, 1678)	2.997023471119646
  (0, 1677)	2.9054300796071293
  (0, 1676)	2.998284408835776
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9671141979583187
  (0, 1671)	2.956695145615718
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9706436523749598
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9706436523749598
  (0, 1664)	3.000000000000002
  (0, 1663)	2.913848836290052
  (0, 1662)	2.9706436523749598
  (0, 1661)	2.9627183173190534
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010721250959268
  :	:
  (942, 24)	3.8609303504302255
  (942, 23)	3.5854725581910993
  (942, 22)	4.266288076863961
  (942, 21)	4.305201385113816
  (942, 20)	2.9109784631600184
  (942, 19)	3.5362731757651096
  (942, 18)	4.104666266105945
  (942, 17)	3.1575826414636787
  (942, 16)	3.4149450828347523
  (942, 15)	3.4994797397010773
  (942, 14)	3.926444523968508
  (942, 13)	4.141749395498412
  (942, 12)	3.5012291137230416
  (942, 11)	4.449459818494386
  (942, 10)	3.88352711624253
  (942, 9)	3.80261906512061
  (942, 8)	4.058355076615144
  (942, 7)	3.9285020376149435
  (942, 6)	3.8833844459232005
  (942, 5)	3.7476089025296355
  (942, 4)	3.4207586157294565
  (942, 3)	3.4586900986697238
  (942, 2)	3.0107343175717065
  (942, 1)	3.14471234203566
  (942, 0)	3.948909842819842
this is the 153 epoch
rmse loss on training set is 0.9570759053604159
rmse loss on test set is 0.9825011431989711
for this epoch using 113.47219944000244 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9906269871128472
  (0, 1680)	3.000000000000002
  (0, 1679)	2.950910837615524
  (0, 1678)	2.9970007729291552
  (0, 1677)	2.904820902301892
  (0, 1676)	2.998290335399421
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9668438583946815
  (0, 1671)	2.956445551363141
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.970472087726606
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.970472087726606
  (0, 1664)	3.000000000000002
  (0, 1663)	2.913321912882346
  (0, 1662)	2.970472087726606
  (0, 1661)	2.962502955048059
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.010825624019013
  :	:
  (942, 24)	3.861021153834593
  (942, 23)	3.585614663091668
  (942, 22)	4.26747719773773
  (942, 21)	4.306012552309285
  (942, 20)	2.909776888094012
  (942, 19)	3.537226861406833
  (942, 18)	4.10681628504659
  (942, 17)	3.1567850677554317
  (942, 16)	3.4153366426436342
  (942, 15)	3.5004603073875256
  (942, 14)	3.9264287775033377
  (942, 13)	4.142189527320593
  (942, 12)	3.5015021373221495
  (942, 11)	4.450208348776769
  (942, 10)	3.884129081987658
  (942, 9)	3.8039318673594784
  (942, 8)	4.058529360214375
  (942, 7)	3.929042325207277
  (942, 6)	3.883514876982135
  (942, 5)	3.7498878065907797
  (942, 4)	3.4211525381547423
  (942, 3)	3.459209270601373
  (942, 2)	3.009888532443335
  (942, 1)	3.1441866507275766
  (942, 0)	3.949088202447988
this is the 154 epoch
rmse loss on training set is 0.9569261489609174
rmse loss on test set is 0.9824062152769618
for this epoch using 111.02099299430847 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9905690689622735
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9505949611358924
  (0, 1678)	2.996978052129699
  (0, 1677)	2.9042118701420865
  (0, 1676)	2.998296352585889
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.966573645896357
  (0, 1671)	2.9561962927401693
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.970300540026566
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.970300540026566
  (0, 1664)	3.000000000000002
  (0, 1663)	2.912795303999121
  (0, 1662)	2.970300540026566
  (0, 1661)	2.9622878459177278
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0109300686959797
  :	:
  (942, 24)	3.861108027028897
  (942, 23)	3.5857486884048204
  (942, 22)	4.268650059115384
  (942, 21)	4.306813226660438
  (942, 20)	2.908583100235568
  (942, 19)	3.538170364613307
  (942, 18)	4.108941530019617
  (942, 17)	3.1559839180825766
  (942, 16)	3.415722409464196
  (942, 15)	3.501432727282489
  (942, 14)	3.926410310527807
  (942, 13)	4.142619815072907
  (942, 12)	3.501770293413566
  (942, 11)	4.450945302133277
  (942, 10)	3.8847208487343248
  (942, 9)	3.8052274740721197
  (942, 8)	4.058697970460523
  (942, 7)	3.9295751696934764
  (942, 6)	3.8836414073031458
  (942, 5)	3.752153901022258
  (942, 4)	3.421539998798943
  (942, 3)	3.4597237605158186
  (942, 2)	3.0090467867201003
  (942, 1)	3.1436609097518153
  (942, 0)	3.9492628586840617
this is the 155 epoch
rmse loss on training set is 0.9567780823307943
rmse loss on test set is 0.9823126038871031
for this epoch using 111.71342206001282 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9905112706233545
  (0, 1680)	3.000000000000002
  (0, 1679)	2.950279145496844
  (0, 1678)	2.9969553084013607
  (0, 1677)	2.9036029825923353
  (0, 1676)	2.9983024589363523
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.966303564259731
  (0, 1671)	2.95594736741695
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9701290089359773
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9701290089359773
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9122690072909347
  (0, 1662)	2.9701290089359773
  (0, 1661)	2.962072988869961
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011034581850852
  :	:
  (942, 24)	3.8611910782500525
  (942, 23)	3.585874836212994
  (942, 22)	4.269807006187836
  (942, 21)	4.307603633133315
  (942, 20)	2.90739708996038
  (942, 19)	3.539103821994454
  (942, 18)	4.111042329481474
  (942, 17)	3.1551792894605395
  (942, 16)	3.4161024958828827
  (942, 15)	3.502397111002576
  (942, 14)	3.926389204524434
  (942, 13)	4.143040517617108
  (942, 12)	3.502033685276749
  (942, 11)	4.451670979388962
  (942, 10)	3.8853026504193586
  (942, 9)	3.8065061362296375
  (942, 8)	4.0588610666292535
  (942, 7)	3.9301007561465497
  (942, 6)	3.8837641144979074
  (942, 5)	3.7544073151540416
  (942, 4)	3.421921120528854
  (942, 3)	3.4602336532562954
  (942, 2)	3.008209099142685
  (942, 1)	3.1431351849073415
  (942, 0)	3.9494338695756186
this is the 156 epoch
rmse loss on training set is 0.9566316764999542
rmse loss on test set is 0.9822202852007966
for this epoch using 106.62890243530273 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9904535910638756
  (0, 1680)	3.000000000000002
  (0, 1679)	2.949963390275469
  (0, 1678)	2.9969325414277366
  (0, 1677)	2.902994239123213
  (0, 1676)	2.998308653025815
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9660336171655923
  (0, 1671)	2.9556987730843702
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.969957494129603
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.969957494129603
  (0, 1664)	3.000000000000002
  (0, 1663)	2.911743020433934
  (0, 1662)	2.969957494129603
  (0, 1661)	2.9618583828518346
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011139160408914
  :	:
  (942, 24)	3.861270412260564
  (942, 23)	3.5859933039189342
  (942, 22)	4.2709483754880475
  (942, 21)	4.3083839901677985
  (942, 20)	2.9062188462864564
  (942, 19)	3.5400273678341927
  (942, 18)	4.113119006814726
  (942, 17)	3.154371276735792
  (942, 16)	3.4164770120215677
  (942, 15)	3.503353568006712
  (942, 14)	3.926365538487792
  (942, 13)	4.143451886389625
  (942, 12)	3.5022924135333704
  (942, 11)	4.4523856720897665
  (942, 10)	3.885874714999371
  (942, 9)	3.807768100681992
  (942, 8)	4.059018802791231
  (942, 7)	3.9306192639757094
  (942, 6)	3.883883074686972
  (942, 5)	3.7566481761577566
  (942, 4)	3.4222960236016005
  (942, 3)	3.4607390314325577
  (942, 2)	3.0073754871274154
  (942, 1)	3.1426095401305933
  (942, 0)	3.949601292586853
this is the 157 epoch
rmse loss on training set is 0.9564869031818137
rmse loss on test set is 0.9821292359719465
for this epoch using 108.98116087913513 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.990396029263593
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9496476950536215
  (0, 1678)	2.9969097508959543
  (0, 1677)	2.9023856392112948
  (0, 1676)	2.9983149334622596
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.965763808182126
  (0, 1671)	2.955450507453998
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9697859952953225
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9697859952953225
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9112173411296243
  (0, 1662)	2.9697859952953225
  (0, 1661)	2.961644026815587
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0112438013587313
  :	:
  (942, 24)	3.8613461304840997
  (942, 23)	3.586104284356773
  (942, 22)	4.272074495126537
  (942, 21)	4.309154509905393
  (942, 20)	2.9050483569253833
  (942, 19)	3.540941134141184
  (942, 18)	4.115171880421247
  (942, 17)	3.1535599726383703
  (942, 16)	3.4168460655988615
  (942, 15)	3.504302205647386
  (942, 14)	3.9263393890224054
  (942, 13)	4.143854165632337
  (942, 12)	3.502546576228348
  (942, 11)	4.453089662809346
  (942, 10)	3.8864372646165712
  (942, 9)	3.809013610235351
  (942, 8)	4.059171328007284
  (942, 7)	3.931130867114585
  (942, 6)	3.88399836253832
  (942, 5)	3.758876609094132
  (942, 4)	3.4226648257266796
  (942, 3)	3.4612399754894
  (942, 2)	3.006545966808361
  (942, 1)	3.142084037544529
  (942, 0)	3.9497651845953343
this is the 158 epoch
rmse loss on training set is 0.9563437347522904
rmse loss on test set is 0.9820394335183352
for this epoch using 110.34867453575134 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.990338584214269
  (0, 1680)	3.000000000000002
  (0, 1679)	2.94933205941801
  (0, 1678)	2.9968869364967716
  (0, 1677)	2.9017771823392584
  (0, 1676)	2.998321298885814
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9654941407678406
  (0, 1671)	2.955202568258009
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.969614512133601
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.969614512133601
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9106919671046447
  (0, 1662)	2.969614512133601
  (0, 1661)	2.961429919718544
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011348501750792
  :	:
  (942, 24)	3.861418331135308
  (942, 23)	3.5862079659003174
  (942, 22)	4.273185685020067
  (942, 21)	4.3099153984081315
  (942, 20)	2.903885608331975
  (942, 19)	3.5418452506981555
  (942, 18)	4.117201263813406
  (942, 17)	3.1527454678329137
  (942, 16)	3.417209761989583
  (942, 15)	3.505243129220465
  (942, 14)	3.9263108304358405
  (942, 13)	4.144247592615759
  (942, 12)	3.502796268907899
  (942, 11)	4.453783225445222
  (942, 10)	3.886990515759669
  (942, 9)	3.8102429037278447
  (942, 8)	4.059318786515685
  (942, 7)	3.9316357342030526
  (942, 6)	3.8841100513039972
  (942, 5)	3.7610927369592155
  (942, 4)	3.4230276421263643
  (942, 3)	3.4617365637729973
  (942, 2)	3.0057205530781963
  (942, 1)	3.1415587375062395
  (942, 0)	3.949925601888937
this is the 159 epoch
rmse loss on training set is 0.9562021442297199
rmse loss on test set is 0.9819508557037947
for this epoch using 108.95990872383118 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9902812549196924
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9490164829602588
  (0, 1678)	2.9968640979245693
  (0, 1677)	2.901168867995942
  (0, 1676)	2.9983277479678954
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9652246182744273
  (0, 1671)	2.9549549532491555
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9694430443569626
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9694430443569626
  (0, 1664)	3.000000000000002
  (0, 1663)	2.91016689611055
  (0, 1662)	2.9694430443569626
  (0, 1661)	2.9612160605230637
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011453258696191
  :	:
  (942, 24)	3.861487109343798
  (942, 23)	3.5863045325685126
  (942, 22)	4.2742822571137316
  (942, 21)	4.310666855869048
  (942, 20)	2.9027305857522117
  (942, 19)	3.5427398451099044
  (942, 18)	4.119207465703196
  (942, 17)	3.1519278509683852
  (942, 16)	3.4175682042825244
  (942, 15)	3.506176442013603
  (942, 14)	3.9262799348271527
  (942, 13)	4.144632397854714
  (942, 12)	3.5030415846949676
  (942, 11)	4.45446662550488
  (942, 10)	3.887534679419845
  (942, 9)	3.811456216103592
  (942, 8)	4.059461317911856
  (942, 7)	3.9321340287625404
  (942, 6)	3.884218212855051
  (942, 5)	3.7632966807293275
  (942, 4)	3.4233845855944516
  (942, 3)	3.4622288725948196
  (942, 2)	3.0048992596276287
  (942, 1)	3.141033698653173
  (942, 0)	3.9500826001626757
this is the 160 epoch
rmse loss on training set is 0.9560621052553899
rmse loss on test set is 0.981863480921046
for this epoch using 108.94605779647827 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9902240403957077
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9487009652769363
  (0, 1678)	2.9968412348774613
  (0, 1677)	2.9005606956764187
  (0, 1676)	2.9983342794104386
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.964955243949537
  (0, 1671)	2.9547076602006563
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9692715916895445
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9692715916895445
  (0, 1664)	3.000000000000002
  (0, 1663)	2.909642125923602
  (0, 1662)	2.9692715916895445
  (0, 1661)	2.961002448196533
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011558069365378
  :	:
  (942, 24)	3.861552557272853
  (942, 23)	3.586394164128359
  (942, 22)	4.275364515596668
  (942, 21)	4.311409076814577
  (942, 20)	2.901583273269703
  (942, 19)	3.543625042850001
  (942, 18)	4.121190790089463
  (942, 17)	3.151107208726438
  (942, 16)	3.4179214933366127
  (942, 15)	3.5071022453533045
  (942, 14)	3.9262467721712597
  (942, 13)	4.1450088053168725
  (942, 12)	3.5032826143620963
  (942, 11)	4.455140120381971
  (942, 10)	3.8880699612421803
  (942, 9)	3.8126537784851195
  (942, 8)	4.059599057320912
  (942, 7)	3.932625909365445
  (942, 6)	3.8843229177148384
  (942, 5)	3.7654885594048784
  (942, 4)	3.4237357665534436
  (942, 3)	3.462716976293588
  (942, 2)	3.004082098983679
  (942, 1)	3.1405089779480218
  (942, 0)	3.9502362345159625
this is the 161 epoch
rmse loss on training set is 0.9559235920749377
rmse loss on test set is 0.9817772880752014
for this epoch using 108.67983865737915 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9901669396701953
  (0, 1680)	3.000000000000002
  (0, 1679)	2.948385505969651
  (0, 1678)	2.996818347057271
  (0, 1677)	2.899952664882031
  (0, 1676)	2.9983408919450842
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9646860209394945
  (0, 1671)	2.954460686906168
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9691001538666204
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9691001538666204
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9091176543445454
  (0, 1662)	2.9691001538666204
  (0, 1661)	2.9607890817113067
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011662930986853
  :	:
  (942, 24)	3.861614764232918
  (942, 23)	3.586477036195124
  (942, 22)	4.2764327571115395
  (942, 21)	4.312142250299214
  (942, 20)	2.9004436538506098
  (942, 19)	3.5445009673062318
  (942, 18)	4.123151536343219
  (942, 17)	3.1502836258684286
  (942, 16)	3.418269727835432
  (942, 15)	3.508020638650678
  (942, 14)	3.9262114103990826
  (942, 13)	4.145377032624436
  (942, 12)	3.503519446401677
  (942, 11)	4.455803959623056
  (942, 10)	3.888596561672526
  (942, 9)	3.8138358182442547
  (942, 8)	4.059732135563231
  (942, 7)	3.9331115297984263
  (942, 6)	3.884424235090767
  (942, 5)	3.7676684900529507
  (942, 4)	3.4240812931102544
  (942, 3)	3.4632009472949927
  (942, 2)	3.003269082546734
  (942, 1)	3.139984630722335
  (942, 0)	3.9503865594498855
this is the 162 epoch
rmse loss on training set is 0.9557865795203221
rmse loss on test set is 0.9816922565679377
for this epoch using 110.97721099853516 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9901099517830447
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9480701046450326
  (0, 1678)	2.996795434169613
  (0, 1677)	2.8993447751204457
  (0, 1676)	2.9983475843324174
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9644169522919332
  (0, 1671)	2.954214031179658
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.968928730634152
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.968928730634152
  (0, 1664)	3.000000000000002
  (0, 1663)	2.908593479198379
  (0, 1662)	2.968928730634152
  (0, 1661)	2.9605759600446646
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011767840845973
  :	:
  (942, 24)	3.8616738167901064
  (942, 23)	3.5865533203300433
  (942, 22)	4.277487270958041
  (942, 21)	4.312866560092646
  (942, 20)	2.8993117093871517
  (942, 19)	3.545367739824796
  (942, 18)	4.125089999291088
  (942, 17)	3.149457185281286
  (942, 16)	3.418613004340278
  (942, 15)	3.5089317194459673
  (942, 14)	3.9261739154739628
  (942, 13)	4.145737291249114
  (942, 12)	3.5037521670939302
  (942, 11)	4.456458385185197
  (942, 10)	3.8891146760999415
  (942, 9)	3.815002559071433
  (942, 8)	4.059860679313363
  (942, 7)	3.9335910392202686
  (942, 6)	3.8845222329046356
  (942, 5)	3.769836587848912
  (942, 4)	3.42442127111035
  (942, 3)	3.4636808561694803
  (942, 2)	3.0024602206264457
  (942, 1)	3.1394607107188
  (942, 0)	3.950533628864751
this is the 163 epoch
rmse loss on training set is 0.9556510429925194
rmse loss on test set is 0.9816083662822211
for this epoch using 107.60686373710632 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.990053075786185
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9477547609148154
  (0, 1678)	2.996772495923909
  (0, 1677)	2.8987370259057226
  (0, 1676)	2.9983543553612284
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9641480409583916
  (0, 1671)	2.953967690855341
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.968757321748396
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.968757321748396
  (0, 1664)	3.000000000000002
  (0, 1663)	2.908069598334142
  (0, 1662)	2.968757321748396
  (0, 1661)	2.960363082178798
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0118727962837206
  :	:
  (942, 24)	3.8617297988702237
  (942, 23)	3.5866231841356124
  (942, 22)	4.278528339290505
  (942, 21)	4.313582184859797
  (942, 20)	2.898187420739647
  (942, 19)	3.5462254797534007
  (942, 18)	4.127006469296983
  (942, 17)	3.1486279680220424
  (942, 16)	3.418951417341682
  (942, 15)	3.5098355834517707
  (942, 14)	3.926134351464433
  (942, 13)	4.146089786700615
  (942, 12)	3.503980860572464
  (942, 11)	4.4571036316847925
  (942, 10)	3.8896244949950796
  (942, 9)	3.816154221043576
  (942, 8)	4.059984811252649
  (942, 7)	3.934064582314177
  (942, 6)	3.884616977821759
  (942, 5)	3.771992966116783
  (942, 4)	3.424755804190511
  (942, 3)	3.4641567716881188
  (942, 2)	3.0016555224764723
  (942, 1)	3.1389372701324416
  (942, 0)	3.9506774960579096
this is the 164 epoch
rmse loss on training set is 0.95551695844487
rmse loss on test set is 0.9815255975676841
for this epoch using 106.90596508979797 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9899963107434964
  (0, 1680)	3.000000000000002
  (0, 1679)	2.947439474395856
  (0, 1678)	2.99674953203341
  (0, 1677)	2.898129416758303
  (0, 1676)	2.998361203847793
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.96387928979678
  (0, 1671)	2.953721663787595
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9685859269754795
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9685859269754795
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9075460096247125
  (0, 1662)	2.9685859269754795
  (0, 1661)	2.9601504471007782
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.011977794695547
  :	:
  (942, 24)	3.861782791857988
  (942, 23)	3.5866867913484755
  (942, 22)	4.279556237309955
  (942, 21)	4.314289298333972
  (942, 20)	2.8970707677772927
  (942, 19)	3.5470743044831092
  (942, 18)	4.1289012323420256
  (942, 17)	3.147796053361225
  (942, 16)	3.4192850593094675
  (942, 15)	3.5107323245952062
  (942, 14)	3.9260927806135593
  (942, 13)	4.146434718709084
  (942, 12)	3.5042056088876845
  (942, 11)	4.457739926637821
  (942, 10)	3.8901262040442353
  (942, 9)	3.817291020690439
  (942, 8)	4.060104650215647
  (942, 7)	3.934532299434701
  (942, 6)	3.884708535278686
  (942, 5)	3.77413773636866
  (942, 4)	3.425084993830245
  (942, 3)	3.4646287608765425
  (942, 2)	3.000854996328213
  (942, 1)	3.1384143596505214
  (942, 0)	3.950818213721802
this is the 165 epoch
rmse loss on training set is 0.9553843023669507
rmse loss on test set is 0.9814439312264845
for this epoch using 108.17324900627136 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9899396557307987
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9471242447101536
  (0, 1678)	2.996726542215234
  (0, 1677)	2.8975219472050724
  (0, 1676)	2.998368128635121
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.963610701573886
  (0, 1671)	2.953475947850847
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9684145460909876
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9684145460909876
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9070227109665523
  (0, 1662)	2.9684145460909876
  (0, 1661)	2.9599380538025017
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.012082833530189
  :	:
  (942, 24)	3.8618328746923227
  (942, 23)	3.586744301929934
  (942, 22)	4.280571233450557
  (942, 21)	4.314988069483469
  (942, 20)	2.895961729417523
  (942, 19)	3.5479143294891995
  (942, 18)	4.130774570102736
  (942, 17)	3.146961518825159
  (942, 16)	3.4196140207414745
  (942, 15)	3.5116220350588163
  (942, 14)	3.9260492634050057
  (942, 13)	4.146772281401413
  (942, 12)	3.5044264920679895
  (942, 11)	4.458367490691901
  (942, 10)	3.890619984279709
  (942, 9)	3.818413171059641
  (942, 8)	4.060220311330777
  (942, 7)	3.934994326749797
  (942, 6)	3.884796969509946
  (942, 5)	3.7762710083431514
  (942, 4)	3.4254089394017146
  (942, 3)	3.4650968890671923
  (942, 2)	3.000058649423432
  (942, 1)	3.137892028491379
  (942, 0)	3.9509558339422477
this is the 166 epoch
rmse loss on training set is 0.9552530517691422
rmse loss on test set is 0.9813633484997493
for this epoch using 106.1199939250946 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9898831098358043
  (0, 1680)	3.000000000000002
  (0, 1679)	2.946809071484881
  (0, 1678)	2.9967035261903847
  (0, 1677)	2.8969146167793887
  (0, 1676)	2.998375128592314
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.963342278967685
  (0, 1671)	2.9532305409394772
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9682431788796633
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9682431788796633
  (0, 1664)	3.000000000000002
  (0, 1663)	2.906499700279533
  (0, 1662)	2.9682431788796633
  (0, 1661)	2.9597259012807013
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.012187910288559
  :	:
  (942, 24)	3.861880123957233
  (942, 23)	3.586795872154315
  (942, 22)	4.28157358956083
  (942, 21)	4.315678662671774
  (942, 20)	2.8948602836642197
  (942, 19)	3.5487456683709113
  (942, 18)	4.132626760027563
  (942, 17)	3.146124440237037
  (942, 16)	3.4199383902109077
  (942, 15)	3.5125048053204364
  (942, 14)	3.926003858626043
  (942, 13)	4.147102663471808
  (942, 12)	3.50464358817897
  (942, 11)	4.458986537850467
  (942, 10)	3.8911060122060857
  (942, 9)	3.8195208817802095
  (942, 8)	4.06033190615533
  (942, 7)	3.935450796377821
  (942, 6)	3.884882343573692
  (942, 5)	3.778392890042737
  (942, 4)	3.425727738218543
  (942, 3)	3.4655612199497203
  (942, 2)	2.999266488045915
  (942, 1)	3.1373703244421525
  (942, 0)	3.951090408197083
this is the 167 epoch
rmse loss on training set is 0.9551231841676359
rmse loss on test set is 0.9812838310544931
for this epoch using 109.41896033287048 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.989826672158051
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9464939543524085
  (0, 1678)	2.9966804836837486
  (0, 1677)	2.8963074250210736
  (0, 1676)	2.9983822026138656
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.96307402456973
  (0, 1671)	2.95298544096773
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9680718251349654
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9680718251349654
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9059769755066682
  (0, 1662)	2.9680718251349654
  (0, 1661)	2.959513988536894
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.012293022522616
  :	:
  (942, 24)	3.8619246139690766
  (942, 23)	3.5868416546950503
  (942, 22)	4.282563561079698
  (942, 21)	4.316361237811784
  (942, 20)	2.8937664076446272
  (942, 19)	3.549568432890095
  (942, 18)	4.134458075411796
  (942, 17)	3.145284891757043
  (942, 16)	3.42025825441235
  (942, 15)	3.5133807241919564
  (942, 14)	3.9259566234276777
  (942, 13)	4.147426048346831
  (942, 12)	3.5048569733805714
  (942, 11)	4.459597275689298
  (942, 10)	3.8915844599229934
  (942, 9)	3.820614359124884
  (942, 8)	4.060439542805089
  (942, 7)	3.935901836519844
  (942, 6)	3.8849647193762094
  (942, 5)	3.7805034877702366
  (942, 4)	3.4260414855831707
  (942, 3)	3.4660218156198623
  (942, 2)	2.998478517552143
  (942, 1)	3.136849293895459
  (942, 0)	3.9512219873550745
this is the 168 epoch
rmse loss on training set is 0.9549946775700439
rmse loss on test set is 0.9812053609710429
for this epoch using 104.69785785675049 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9897703418088355
  (0, 1680)	3.000000000000002
  (0, 1679)	2.946178892950295
  (0, 1678)	2.996657414424127
  (0, 1677)	2.89570037147646
  (0, 1676)	2.9983893496190004
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9628059408873666
  (0, 1671)	2.952740645869573
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.967900484658796
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.967900484658796
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9054545346139475
  (0, 1662)	2.967900484658796
  (0, 1661)	2.959302314577378
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.01239816783429
  :	:
  (942, 24)	3.861966416859869
  (942, 23)	3.5868817987086516
  (942, 22)	4.283541397207482
  (942, 21)	4.317035950514154
  (942, 20)	2.8926800776451005
  (942, 19)	3.5503827330089894
  (942, 18)	4.1362687854709375
  (942, 17)	3.144442945921304
  (942, 16)	3.4205736982065864
  (942, 15)	3.514249878857085
  (942, 14)	3.9259076133820066
  (942, 13)	4.147742614344964
  (942, 12)	3.5050667219823812
  (942, 11)	4.4601999055656085
  (942, 10)	3.8920554952441413
  (942, 9)	3.82169380607103
  (942, 8)	4.060543326078898
  (942, 7)	3.9363475715874743
  (942, 6)	3.885044157695751
  (942, 5)	3.7826029061643656
  (942, 4)	3.426350274833114
  (942, 3)	3.466478736626593
  (942, 2)	2.997694742401037
  (942, 1)	3.136328981885006
  (942, 0)	3.9513506216750587
this is the 169 epoch
rmse loss on training set is 0.9548675104615139
rmse loss on test set is 0.9811279207308989
for this epoch using 109.61354279518127 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9897141179111792
  (0, 1680)	3.000000000000002
  (0, 1679)	2.945863886921309
  (0, 1678)	2.99663431814427
  (0, 1677)	2.8950934556983605
  (0, 1676)	2.998396568551031
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.962538030345964
  (0, 1671)	2.9524961535986276
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.967729157261127
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.967729157261127
  (0, 1664)	3.000000000000002
  (0, 1663)	2.904932375590086
  (0, 1662)	2.967729157261127
  (0, 1661)	2.959090878413197
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0125033438744246
  :	:
  (942, 24)	3.862005602657198
  (942, 23)	3.5869164499166213
  (942, 22)	4.284507341072105
  (942, 21)	4.317702952230062
  (942, 20)	2.891601269145732
  (942, 19)	3.5511886769269214
  (942, 18)	4.13805915541245
  (942, 17)	3.1435986736799277
  (942, 16)	3.4208848046641154
  (942, 15)	3.515112354908051
  (942, 14)	3.925856882536905
  (942, 13)	4.148052534831102
  (942, 12)	3.505272906497027
  (942, 11)	4.460794622820117
  (942, 10)	3.892519281812951
  (942, 9)	3.8227594223603614
  (942, 8)	4.060643357578081
  (942, 7)	3.936788122326189
  (942, 6)	3.8851207182052763
  (942, 5)	3.7846912482343527
  (942, 4)	3.4266541973859628
  (942, 3)	3.466932042017758
  (942, 2)	2.996915166182819
  (942, 1)	3.135809432120283
  (942, 0)	3.9514763608054224
this is the 170 epoch
rmse loss on training set is 0.9547416617912854
rmse loss on test set is 0.9810514932050666
for this epoch using 107.23004865646362 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.989657999599687
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9455489359134717
  (0, 1678)	2.996611194580828
  (0, 1677)	2.894486677246116
  (0, 1676)	2.998403858376754
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9622702952910442
  (0, 1671)	2.9522519621280217
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9675578427597067
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9675578427597067
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9044104964463253
  (0, 1662)	2.9675578427597067
  (0, 1661)	2.9588796790601295
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0126085483417313
  :	:
  (942, 24)	3.862042239360664
  (942, 23)	3.5869457506853233
  (942, 22)	4.28546162989052
  (942, 21)	4.318362390388584
  (942, 20)	2.890529956853836
  (942, 19)	3.551986371116094
  (942, 18)	4.139829446506017
  (942, 17)	3.1427521444339965
  (942, 16)	3.4211916551075254
  (942, 15)	3.5159682363813856
  (942, 14)	3.92580448346833
  (942, 13)	4.148355978365826
  (942, 12)	3.5054755976918712
  (942, 11)	4.461381616972161
  (942, 10)	3.8929759792147465
  (942, 9)	3.823811404557258
  (942, 8)	4.060739735821393
  (942, 7)	3.9372236059344705
  (942, 6)	3.8851944594944614
  (942, 5)	3.786768615393737
  (942, 4)	3.4269533427831993
  (942, 3)	3.467381789384299
  (942, 2)	2.9961397916469217
  (942, 1)	3.1352906870201864
  (942, 0)	3.951599253783927
this is the 171 epoch
rmse loss on training set is 0.9546171109597728
rmse loss on test set is 0.980976061642781
for this epoch using 107.76780867576599 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.98960198602052
  (0, 1680)	3.000000000000002
  (0, 1679)	2.945234039579986
  (0, 1678)	2.9965880434744023
  (0, 1677)	2.8938800356855734
  (0, 1676)	2.998411218085833
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.962002737990402
  (0, 1671)	2.9520080694502777
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.967386540979782
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.967386540979782
  (0, 1664)	3.000000000000002
  (0, 1663)	2.903888895216215
  (0, 1662)	2.967386540979782
  (0, 1661)	2.958668715538671
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0127137789817513
  :	:
  (942, 24)	3.862076393015123
  (942, 23)	3.586969840103936
  (942, 22)	4.286404495125648
  (942, 21)	4.319014408528994
  (942, 20)	2.8894661147364182
  (942, 19)	3.5527759203565163
  (942, 18)	4.141579916152425
  (942, 17)	3.1419034260715923
  (942, 16)	3.421494329152728
  (942, 15)	3.5168176057927165
  (942, 14)	3.925750467330208
  (942, 13)	4.1486531088500636
  (942, 12)	3.5056748646389084
  (942, 11)	4.461961071908283
  (942, 10)	3.8934257430858032
  (942, 9)	3.8248499461061014
  (942, 8)	4.060832556355166
  (942, 7)	3.9376541361788986
  (942, 6)	3.8852654390910404
  (942, 5)	3.788835107493266
  (942, 4)	3.4272477987329792
  (942, 3)	3.467828034902887
  (942, 2)	2.9953686207291326
  (942, 1)	3.134772787745834
  (942, 0)	3.9517193490377527
this is the 172 epoch
rmse loss on training set is 0.9544938378060319
rmse loss on test set is 0.9809016096606689
for this epoch using 108.83022212982178 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.989546076331307
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9449191975793094
  (0, 1678)	2.9965648645695415
  (0, 1677)	2.893273530589078
  (0, 1676)	2.998418646690186
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9617353606361028
  (0, 1671)	2.9517644735771893
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9672152517537427
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9672152517537427
  (0, 1664)	3.000000000000002
  (0, 1663)	2.90336756995538
  (0, 1662)	2.9672152517537427
  (0, 1661)	2.958457986874004
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0128190335859
  :	:
  (942, 24)	3.8621081277807665
  (942, 23)	3.5869888540603916
  (942, 22)	4.28733616263881
  (942, 21)	4.319659146427999
  (942, 20)	2.8884097160515925
  (942, 19)	3.5535574277699653
  (942, 18)	4.143310817950895
  (942, 17)	3.141052585002968
  (942, 16)	3.421792904749023
  (942, 15)	3.517660544170644
  (942, 14)	3.925694883902147
  (942, 13)	4.148944085664887
  (942, 12)	3.5058707747631206
  (942, 11)	4.462533166064329
  (942, 10)	3.8938687252190625
  (942, 9)	3.8258752373872
  (942, 8)	4.06092191185936
  (942, 7)	3.9380798235052428
  (942, 6)	3.885333713481315
  (942, 5)	3.790890822853038
  (942, 4)	3.427537651151735
  (942, 3)	3.4682708333773467
  (942, 2)	2.994601654577907
  (942, 1)	3.1342557742323858
  (942, 0)	3.9518366943838363
this is the 173 epoch
rmse loss on training set is 0.9543718225957396
rmse loss on test set is 0.9808281212322759
for this epoch using 108.02375221252441 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9894902697010104
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9446044095751063
  (0, 1678)	2.9965416576147317
  (0, 1677)	2.8926671615354747
  (0, 1676)	2.998426143223431
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9614681653465253
  (0, 1671)	2.951521172539717
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9670439749209203
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9670439749209203
  (0, 1664)	3.000000000000002
  (0, 1663)	2.902846518741353
  (0, 1662)	2.9670439749209203
  (0, 1661)	2.9582474920960093
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0129243099904497
  :	:
  (942, 24)	3.862137506000417
  (942, 23)	3.587002925315603
  (942, 22)	4.288256852837929
  (942, 21)	4.320296740222364
  (942, 20)	2.8873607333789426
  (942, 19)	3.554330994853166
  (942, 18)	4.1450224017651065
  (942, 17)	3.1401996861947072
  (942, 16)	3.422087458218191
  (942, 15)	3.5184971310898243
  (942, 14)	3.925637781635034
  (942, 13)	4.149229063806908
  (942, 12)	3.506063393889149
  (942, 11)	4.46309807260143
  (942, 10)	3.8943050736669207
  (942, 9)	3.8268874657717156
  (942, 8)	4.0610078922492185
  (942, 7)	3.9385007751457928
  (942, 6)	3.885399338130041
  (942, 5)	3.792935858293767
  (942, 4)	3.427822984204742
  (942, 3)	3.4687102382785775
  (942, 2)	2.9938388935798756
  (942, 1)	3.1337396852200174
  (942, 0)	3.9519513370295964
this is the 174 epoch
rmse loss on training set is 0.9542510460094771
rmse loss on test set is 0.9807555806779686
for this epoch using 106.87416052818298 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9894345653098675
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9442896752362486
  (0, 1678)	2.996518422362387
  (0, 1677)	2.8920609281101126
  (0, 1676)	2.9984337067403275
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.961201154168253
  (0, 1671)	2.951278164387833
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9668727103272747
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9668727103272747
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9023257396733055
  (0, 1662)	2.9668727103272747
  (0, 1661)	2.9580372302392184
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0130296060756065
  :	:
  (942, 24)	3.8621645882637385
  (942, 23)	3.587012183575761
  (942, 22)	4.289166780821494
  (942, 21)	4.320927322526844
  (942, 20)	2.8863191386490445
  (942, 19)	3.5550967215101266
  (942, 18)	4.146714913787813
  (942, 17)	3.1393447932031244
  (942, 16)	3.422378064292391
  (942, 15)	3.519327444703026
  (942, 14)	3.925579207694625
  (942, 13)	4.149508194019226
  (942, 12)	3.506252786286512
  (942, 11)	4.463655959576047
  (942, 10)	3.894734932841003
  (942, 9)	3.827886815675385
  (942, 8)	4.061090584773069
  (942, 7)	3.938917095223006
  (942, 6)	3.8854623674997324
  (942, 5)	3.794970309167422
  (942, 4)	3.428103880345667
  (942, 3)	3.469146301783321
  (942, 2)	2.9930803373845687
  (942, 1)	3.1332245582841023
  (942, 0)	3.9520633235738285
this is the 175 epoch
rmse loss on training set is 0.9541314891315358
rmse loss on test set is 0.9806839726552004
for this epoch using 109.91869401931763 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9893789623492912
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9439749942368256
  (0, 1678)	2.9964951585688633
  (0, 1677)	2.891454829904789
  (0, 1676)	2.9984413363162083
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.960934329077995
  (0, 1671)	2.951035447190418
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9667014578251587
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9667014578251587
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9018052308718896
  (0, 1662)	2.9667014578251587
  (0, 1661)	2.9578272003428334
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0131349197645525
  :	:
  (942, 24)	3.8621894334690268
  (942, 23)	3.587016755562899
  (942, 22)	4.290066156518552
  (942, 21)	4.321551022547831
  (942, 20)	2.8852849031719026
  (942, 19)	3.555854706083617
  (942, 18)	4.14838859660414
  (942, 17)	3.138487968206704
  (942, 16)	3.4226647961511865
  (942, 15)	3.5201515617725443
  (942, 14)	3.925519208003259
  (942, 13)	4.149781622918231
  (942, 12)	3.506439014713287
  (942, 11)	4.464206990104211
  (942, 10)	3.895158443609054
  (942, 9)	3.8288734686111425
  (942, 8)	4.061170074106207
  (942, 7)	3.939328884849629
  (942, 6)	3.8855228550692913
  (942, 5)	3.7969942693869316
  (942, 4)	3.4283804203551087
  (942, 3)	3.4695790748115636
  (942, 2)	2.9923259849284665
  (942, 1)	3.1327104298644657
  (942, 0)	3.95217270000794
this is the 176 epoch
rmse loss on training set is 0.9540131334389695
rmse loss on test set is 0.9806132821491119
for this epoch using 108.71241688728333 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.989323460021736
  (0, 1680)	3.000000000000002
  (0, 1679)	2.943660366256114
  (0, 1678)	2.996471865994455
  (0, 1677)	2.8908488665177794
  (0, 1676)	2.9984490310464533
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.960667691984392
  (0, 1671)	2.950793019035092
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9665302172730525
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9665302172730525
  (0, 1664)	3.000000000000002
  (0, 1663)	2.90128499047899
  (0, 1662)	2.9665302172730525
  (0, 1661)	2.957617401450692
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0132402490225423
  :	:
  (942, 24)	3.8622120988822397
  (942, 23)	3.587016765083771
  (942, 22)	4.290955184824718
  (942, 21)	4.322167966192742
  (942, 20)	2.8842579976646094
  (942, 19)	3.556605045385946
  (942, 18)	4.150043689253623
  (942, 17)	3.137629272037794
  (942, 16)	3.422947725457503
  (942, 15)	3.52096955770062
  (942, 14)	3.925457827279755
  (942, 13)	4.15004949311626
  (942, 12)	3.5066221404584703
  (942, 11)	4.4647513225202715
  (942, 10)	3.895575743389036
  (942, 9)	3.82984760324072
  (942, 8)	4.061246442441151
  (942, 7)	3.939736242225406
  (942, 6)	3.8855808533522223
  (942, 5)	3.799007831455303
  (942, 4)	3.4286526833781545
  (942, 3)	3.4700086070628573
  (942, 2)	2.9915758344582595
  (942, 1)	3.1321973352939567
  (942, 0)	3.952279511717455
this is the 177 epoch
rmse loss on training set is 0.953895960791106
rmse loss on test set is 0.9805434944634602
for this epoch using 107.65428185462952 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.989268057540628
  (0, 1680)	3.000000000000002
  (0, 1679)	2.943345790978569
  (0, 1678)	2.996448544403368
  (0, 1677)	2.890243037553778
  (0, 1676)	2.9984567900460006
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.960401244729843
  (0, 1671)	2.950550878028135
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.966358988535382
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.966358988535382
  (0, 1664)	3.000000000000002
  (0, 1663)	2.9007650166575267
  (0, 1662)	2.966358988535382
  (0, 1661)	2.957407832611268
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0133455918560186
  :	:
  (942, 24)	3.8622326401936693
  (942, 23)	3.587012333097023
  (942, 22)	4.29183406573437
  (942, 21)	4.3227782761754225
  (942, 20)	2.8832383922780664
  (942, 19)	3.557347834728895
  (942, 18)	4.151680427290884
  (942, 17)	3.1367687642134543
  (942, 16)	3.423226922392659
  (942, 15)	3.521781506559198
  (942, 14)	3.9253951090776344
  (942, 13)	4.150311943340433
  (942, 12)	3.5068022233828375
  (942, 11)	4.465289110530276
  (942, 10)	3.895986966240583
  (942, 9)	3.8308093954251112
  (942, 8)	4.061319769574332
  (942, 7)	3.9401392627304634
  (942, 6)	3.8856364139141726
  (942, 5)	3.8010110864939777
  (942, 4)	3.4289207469610177
  (942, 3)	3.4704349470513547
  (942, 2)	2.9908298835534812
  (942, 1)	3.1316853088261913
  (942, 0)	3.9523838034837455
this is the 178 epoch
rmse loss on training set is 0.9537799534193743
rmse loss on test set is 0.9804745952118556
for this epoch using 110.50496673583984 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9892127541302274
  (0, 1680)	3.000000000000002
  (0, 1679)	2.943031268093808
  (0, 1678)	2.9964251935637125
  (0, 1677)	2.8896373426239004
  (0, 1676)	2.998464612448793
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9601349890922375
  (0, 1671)	2.9503090222942956
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.966187771482215
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.966187771482215
  (0, 1664)	3.000000000000002
  (0, 1663)	2.900245307591274
  (0, 1662)	2.966187771482215
  (0, 1661)	2.9571984928776347
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0134509463117074
  :	:
  (942, 24)	3.86225111157224
  (942, 23)	3.587003577778814
  (942, 22)	4.292702994469221
  (942, 21)	4.323382072117637
  (942, 20)	2.8822260566228737
  (942, 19)	3.5580831679530465
  (942, 18)	4.153299042845247
  (942, 17)	3.135906502965563
  (942, 16)	3.4235024556904974
  (942, 15)	3.5225874811189533
  (942, 14)	3.9253310958217384
  (942, 13)	4.1505691085475585
  (942, 12)	3.5069793219586036
  (942, 11)	4.465820503360159
  (942, 10)	3.8963922429537488
  (942, 9)	3.8317590182741497
  (942, 8)	4.0613901329894455
  (942, 7)	3.940538039015616
  (942, 6)	3.8856895873901687
  (942, 5)	3.8030041242705424
  (942, 4)	3.429184687086728
  (942, 3)	3.470858142139877
  (942, 2)	2.9900881291484698
  (942, 1)	3.1311743836625476
  (942, 0)	3.9524856194860956
this is the 179 epoch
rmse loss on training set is 0.9536650939174715
rmse loss on test set is 0.9804065703093052
for this epoch using 107.19318675994873 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9891575490255344
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9427167972965793
  (0, 1678)	2.996401813247512
  (0, 1677)	2.8890317813456465
  (0, 1676)	2.998472497407316
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9598689267866503
  (0, 1671)	2.9500674499767006
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.966016565989131
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.966016565989131
  (0, 1664)	3.000000000000002
  (0, 1663)	2.899725861484594
  (0, 1662)	2.966016565989131
  (0, 1661)	2.956989381307507
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0135563104757943
  :	:
  (942, 24)	3.8622675657175645
  (942, 23)	3.5869906145867145
  (942, 22)	4.2935621616031945
  (942, 21)	4.323979470646816
  (942, 20)	2.8812209597943803
  (942, 19)	3.558811137456242
  (942, 18)	4.154899764678915
  (942, 17)	3.135042545270106
  (942, 16)	3.4237743926705595
  (942, 15)	3.523387552877383
  (942, 14)	3.925265828843273
  (942, 13)	4.150821120035484
  (942, 12)	3.5071534933077704
  (942, 11)	4.466345645899004
  (942, 10)	3.896791701135284
  (942, 9)	3.8326966421948994
  (942, 8)	4.061457607937504
  (942, 7)	3.9409326610895192
  (942, 6)	3.885740423501183
  (942, 5)	3.8049870332256726
  (942, 4)	3.4294445782099454
  (942, 3)	3.4712782385727334
  (942, 2)	2.9893505675536063
  (942, 1)	3.1306645919784675
  (942, 0)	3.952585003303955
this is the 180 epoch
rmse loss on training set is 0.9535513652318329
rmse loss on test set is 0.9803394059640428
for this epoch using 109.50828123092651 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9891024414721628
  (0, 1680)	3.000000000000002
  (0, 1679)	2.942402378286764
  (0, 1678)	2.99637840323066
  (0, 1677)	2.8884263533428762
  (0, 1676)	2.998480444092115
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.95960305946701
  (0, 1671)	2.949826159236674
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.965845371936939
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.965845371936939
  (0, 1664)	3.000000000000002
  (0, 1663)	2.899206676562296
  (0, 1662)	2.965845371936939
  (0, 1661)	2.9567804969631757
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0136616824730624
  :	:
  (942, 24)	3.862282053909817
  (942, 23)	3.5869735563222043
  (942, 22)	4.294411753183947
  (942, 21)	4.324570585490313
  (942, 20)	2.8802230703969927
  (942, 19)	3.559531834221485
  (942, 18)	4.156482818244229
  (942, 17)	3.1341769468757814
  (942, 16)	3.42404279927045
  (942, 15)	3.5241817920864507
  (942, 14)	3.925199348413427
  (942, 13)	4.151068105550831
  (942, 12)	3.5073247932392144
  (942, 11)	4.4668646788374105
  (942, 10)	3.8971854652924742
  (942, 9)	3.8336224349392545
  (942, 8)	4.0615222675138565
  (942, 7)	3.9413232164028815
  (942, 6)	3.885788971070485
  (942, 5)	3.806959900499597
  (942, 4)	3.4297004932909054
  (942, 3)	3.4716952815076314
  (942, 2)	2.9886171944760513
  (942, 1)	3.1301559649490605
  (942, 0)	3.9526819979194228
this is the 181 epoch
rmse loss on training set is 0.9534387506524505
rmse loss on test set is 0.9802730886696439
for this epoch using 108.34557390213013 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9890474307262176
  (0, 1680)	3.000000000000002
  (0, 1679)	2.942088010769345
  (0, 1678)	2.9963549632929296
  (0, 1677)	2.8878210582457657
  (0, 1676)	2.998488451691326
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9593373887277323
  (0, 1671)	2.9495851482536364
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9656741892115437
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9656741892115437
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8986877510693914
  (0, 1662)	2.9656741892115437
  (0, 1661)	2.9565718389115374
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.013767060466093
  :	:
  (942, 24)	3.8622946260577176
  (942, 23)	3.5869525131916045
  (942, 22)	4.29525195085087
  (942, 21)	4.325155527566095
  (942, 20)	2.879232356567646
  (942, 19)	3.560245347844071
  (942, 18)	4.158048425739542
  (942, 17)	3.133309762331859
  (942, 16)	3.4243077400773543
  (942, 15)	3.5249702677793144
  (942, 14)	3.9251316937756355
  (942, 13)	4.151310189393341
  (942, 12)	3.5074932762846585
  (942, 11)	4.467377738801303
  (942, 10)	3.8975736569145005
  (942, 9)	3.8345365616504647
  (942, 8)	4.061584182732082
  (942, 7)	3.941709789929894
  (942, 6)	3.8858352780394236
  (942, 5)	3.8089228119577503
  (942, 4)	3.429952503828539
  (942, 3)	3.472109315046517
  (942, 2)	2.987888005039772
  (942, 1)	3.129648532773961
  (942, 0)	3.9527766457200024
this is the 182 epoch
rmse loss on training set is 0.9533272338039458
rmse loss on test set is 0.9802076051973662
for this epoch using 107.66048455238342 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9889925160542012
  (0, 1680)	3.000000000000002
  (0, 1679)	2.941773694454364
  (0, 1678)	2.9963314932179403
  (0, 1677)	2.887215895690794
  (0, 1676)	2.9984965194102267
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9590719161052474
  (0, 1671)	2.9493444152249344
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9655030177037074
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9655030177037074
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8981690832709024
  (0, 1662)	2.9655030177037074
  (0, 1661)	2.9563634062240567
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0138724426544368
  :	:
  (942, 24)	3.862305330744338
  (942, 23)	3.5869275928655244
  (942, 22)	4.296082931949934
  (942, 21)	4.3257344050702935
  (942, 20)	2.8782487859985904
  (942, 19)	3.560951766558153
  (942, 18)	4.159596806164158
  (942, 17)	3.132441045015379
  (942, 16)	3.4245692783586787
  (942, 15)	3.5257530477964933
  (942, 14)	3.9250629031764506
  (942, 13)	4.151547492516914
  (942, 12)	3.5076589957334408
  (942, 11)	4.467884958481207
  (942, 10)	3.8979563945516205
  (942, 9)	3.8354391849087626
  (942, 8)	4.0616434225952505
  (942, 7)	3.942092464246821
  (942, 6)	3.885879391482882
  (942, 5)	3.8108758522159554
  (942, 4)	3.430200679892727
  (942, 3)	3.4725203822654866
  (942, 2)	2.9871629938050166
  (942, 1)	3.1291423247016334
  (942, 0)	3.9528689885016073
this is the 183 epoch
rmse loss on training set is 0.9532167986369672
rmse loss on test set is 0.9801429425888022
for this epoch using 108.8891224861145 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9889376967328682
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9414594290569234
  (0, 1678)	2.9963079927931497
  (0, 1677)	2.886610865320694
  (0, 1676)	2.9985046464708005
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9588066430795865
  (0, 1671)	2.949103958365701
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.965331857308888
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.965331857308888
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8976506714516663
  (0, 1662)	2.965331857308888
  (0, 1661)	2.956155197976786
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0139778272738615
  :	:
  (942, 24)	3.8623142152712338
  (942, 23)	3.58689890053696
  (942, 22)	4.296904869645271
  (942, 21)	4.326307323561427
  (942, 20)	2.877272325959438
  (942, 19)	3.561651177262613
  (942, 18)	4.16112817537207
  (942, 17)	3.1315708471576342
  (942, 16)	3.424827476091979
  (942, 15)	3.5265301988113524
  (942, 14)	3.924993013895289
  (942, 13)	4.151780132627419
  (942, 12)	3.5078220036662224
  (942, 11)	4.468386466757184
  (942, 10)	3.8983337938919886
  (942, 9)	3.836330464776121
  (942, 8)	4.061700054164174
  (942, 7)	3.9424713196080097
  (942, 6)	3.8859213576244014
  (942, 5)	3.812819104664927
  (942, 4)	3.430445090155815
  (942, 3)	3.4729285252436797
  (942, 2)	2.9864421547872873
  (942, 1)	3.128637369052949
  (942, 0)	3.9529590674716895
this is the 184 epoch
rmse loss on training set is 0.9531074294198281
rmse loss on test set is 0.9800790881487532
for this epoch using 107.56015872955322 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.98888297204911
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9411452142971197
  (0, 1678)	2.996284461809823
  (0, 1677)	2.886005966784417
  (0, 1676)	2.9985128321112846
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9585415710758527
  (0, 1671)	2.948863775908735
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9651607079270574
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9651607079270574
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8971325139161155
  (0, 1662)	2.9651607079270574
  (0, 1661)	2.9559472132503424
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0140832125955543
  :	:
  (942, 24)	3.862321325700707
  (942, 23)	3.586866538977906
  (942, 22)	4.297717933027738
  (942, 21)	4.326874386041707
  (942, 20)	2.8763029433184815
  (942, 19)	3.5623436655463254
  (942, 18)	4.162642746124673
  (942, 17)	3.1306992198700145
  (942, 16)	3.4250823939940593
  (942, 15)	3.5273017863549656
  (942, 14)	3.924922062272906
  (942, 13)	4.152008224277461
  (942, 12)	3.5079823509876658
  (942, 11)	4.4688823888196225
  (942, 10)	3.8987059678363742
  (942, 9)	3.8372105588400554
  (942, 8)	4.061754142623247
  (942, 7)	3.9428464340192937
  (942, 6)	3.8859612218509154
  (942, 5)	3.814752651494246
  (942, 4)	3.430685801923295
  (942, 3)	3.4733337850913815
  (942, 2)	2.9857254814756202
  (942, 1)	3.1281336932441826
  (942, 0)	3.9530469232526277
this is the 185 epoch
rmse loss on training set is 0.9529991107304475
rmse loss on test set is 0.9800160294383056
for this epoch using 106.40736603736877 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9888283412998304
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9408310499000616
  (0, 1678)	2.9962609000630356
  (0, 1677)	2.885401199737096
  (0, 1676)	2.9985210755857823
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9582767014656897
  (0, 1671)	2.948623866104328
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.964989569462561
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.964989569462561
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8966146089881053
  (0, 1662)	2.964989569462561
  (0, 1661)	2.955739451129886
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.014188596925394
  :	:
  (942, 24)	3.8623267068964084
  (942, 23)	3.5868306085947217
  (942, 22)	4.298522287220388
  (942, 21)	4.327435693035329
  (942, 20)	2.875340604563404
  (942, 19)	3.563029315712868
  (942, 18)	4.164140728142394
  (942, 17)	3.1298262131691974
  (942, 16)	3.4253340915493684
  (942, 15)	3.5280678748403234
  (942, 14)	3.9248500837387468
  (942, 13)	4.152231878958145
  (942, 12)	3.5081400874579955
  (942, 11)	4.469372846286006
  (942, 10)	3.8990730265708162
  (942, 9)	3.8380796222566227
  (942, 8)	4.061805751343648
  (942, 7)	3.943217883308957
  (942, 6)	3.8859990287271065
  (942, 5)	3.816676573715759
  (942, 4)	3.430922881163821
  (942, 3)	3.473736201977145
  (942, 2)	2.985012966850497
  (942, 1)	3.1276313238094207
  (942, 0)	3.953132595885346
this is the 186 epoch
rmse loss on training set is 0.952891827448502
rmse loss on test set is 0.9799537542681652
for this epoch using 107.25297403335571 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988773803791833
  (0, 1680)	3.000000000000002
  (0, 1679)	2.940516935595803
  (0, 1678)	2.996237307351613
  (0, 1677)	2.8847965638399917
  (0, 1676)	2.9985293761638427
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.958012035568684
  (0, 1671)	2.948384227220126
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.964818441823908
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.964818441823908
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8960969550106967
  (0, 1662)	2.964818441823908
  (0, 1661)	2.955531910705152
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0142939786032055
  :	:
  (942, 24)	3.8623304025622254
  (942, 23)	3.586791207482104
  (942, 22)	4.299318093481115
  (942, 21)	4.327991342664033
  (942, 20)	2.874385275821267
  (942, 19)	3.563708210804633
  (942, 18)	4.165622328155381
  (942, 17)	3.1289518760017665
  (942, 16)	3.4255826270376106
  (942, 15)	3.528828527585959
  (942, 14)	3.9247771128373183
  (942, 13)	4.152451205187917
  (942, 12)	3.508295261723671
  (942, 11)	4.469857957313695
  (942, 10)	3.899435077637099
  (942, 9)	3.8389378077925183
  (942, 8)	4.06185494194411
  (942, 7)	3.94358574119631
  (942, 6)	3.8860348220096195
  (942, 5)	3.8185909511864122
  (942, 4)	3.4311563925383766
  (942, 3)	3.474135815154124
  (942, 2)	2.9843046034011182
  (942, 1)	3.1271302864223522
  (942, 0)	3.9532161248330207
this is the 187 epoch
rmse loss on training set is 0.9527855647478504
rmse loss on test set is 0.979892250692237
for this epoch using 106.88430261611938 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9887193588416854
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9402028711193124
  (0, 1678)	2.996213683478153
  (0, 1677)	2.8841920587604712
  (0, 1676)	2.9985377331300427
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9577475746537627
  (0, 1671)	2.9481448575410005
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.964647324923654
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.964647324923654
  (0, 1664)	3.000000000000002
  (0, 1663)	2.895579550345978
  (0, 1662)	2.964647324923654
  (0, 1661)	2.9553245910704113
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.014399356002036
  :	:
  (942, 24)	3.8623324552797547
  (942, 23)	3.5867484314757774
  (942, 22)	4.3001055093024165
  (942, 21)	4.32854143071988
  (942, 20)	2.8734369228778984
  (942, 19)	3.564380432626322
  (942, 18)	4.167087749953118
  (942, 17)	3.1280762562681366
  (942, 16)	3.425828057560752
  (942, 15)	3.529583806839052
  (942, 14)	3.9247031832534365
  (942, 13)	4.1526663085986755
  (942, 12)	3.508447921347056
  (942, 11)	4.47033783670909
  (942, 10)	3.899792226001408
  (942, 9)	3.8397852658663685
  (942, 8)	4.061901774349464
  (942, 7)	3.9439500793579674
  (942, 6)	3.8860686446607438
  (942, 5)	3.820495862630562
  (942, 4)	3.4313863994288614
  (942, 3)	3.4745326629856126
  (942, 2)	2.983600383142248
  (942, 1)	3.1266306059175077
  (942, 0)	3.953297548985063
this is the 188 epoch
rmse loss on training set is 0.952680308089188
rmse loss on test set is 0.9798315070013319
for this epoch using 108.46430110931396 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988665005775593
  (0, 1680)	3.000000000000002
  (0, 1679)	2.939888856210458
  (0, 1678)	2.996190028248956
  (0, 1677)	2.8835876841719603
  (0, 1676)	2.998546145783654
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9574833199405517
  (0, 1671)	2.9479057553688666
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.964476218678243
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.964476218678243
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8950623933748414
  (0, 1662)	2.964476218678243
  (0, 1661)	2.9551174913244656
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.014504727527469
  :	:
  (942, 24)	3.862332906544163
  (942, 23)	3.5867023742040147
  (942, 22)	4.300884688508447
  (942, 21)	4.329086050735514
  (942, 20)	2.872495511196675
  (942, 19)	3.5650460617679673
  (942, 18)	4.168537194433113
  (942, 17)	3.1271994008459867
  (942, 16)	3.426070439069189
  (942, 15)	3.5303337737978273
  (942, 14)	3.92462832783656
  (942, 13)	4.152877292019191
  (942, 12)	3.5085981128351427
  (942, 11)	4.470812596033039
  (942, 10)	3.900144574120955
  (942, 9)	3.8406221445892235
  (942, 8)	4.061946306846837
  (942, 7)	3.944310967491843
  (942, 6)	3.8861005388619985
  (942, 5)	3.8223913856618106
  (942, 4)	3.431612963965858
  (942, 3)	3.4749267829697024
  (942, 2)	2.982900297630525
  (942, 1)	3.1261323063109576
  (942, 0)	3.9533769066611764
this is the 189 epoch
rmse loss on training set is 0.9525760432129469
rmse loss on test set is 0.9797715117171459
for this epoch using 115.91165947914124 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9886107439292724
  (0, 1680)	3.000000000000002
  (0, 1679)	2.939574890613946
  (0, 1678)	2.9961663414740363
  (0, 1677)	2.8829834397538634
  (0, 1676)	2.998554613438224
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.957219272600671
  (0, 1671)	2.9476669190225717
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.964305123007865
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.964305123007865
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8945454824968126
  (0, 1662)	2.964305123007865
  (0, 1661)	2.954910610570651
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0146100916169263
  :	:
  (942, 24)	3.862331796798713
  (942, 23)	3.586653127137816
  (942, 22)	4.301655781349407
  (942, 21)	4.329625294051849
  (942, 20)	2.8715610059366785
  (942, 19)	3.565705177627398
  (942, 18)	4.169970859648611
  (942, 17)	3.126321355613065
  (942, 16)	3.426309826387452
  (942, 15)	3.5310784886335655
  (942, 14)	3.9245525786242696
  (942, 13)	4.153084255555852
  (942, 12)	3.5087458816675015
  (942, 11)	4.471282343702837
  (942, 10)	3.900492222008806
  (942, 9)	3.841448589804265
  (942, 8)	4.061988596139793
  (942, 7)	3.9446684733791764
  (942, 6)	3.886130546027383
  (942, 5)	3.8242775968042766
  (942, 4)	3.431836147055817
  (942, 3)	3.4753182117633004
  (942, 2)	2.982204337980356
  (942, 1)	3.125635410820471
  (942, 0)	3.9534542356156606
this is the 190 epoch
rmse loss on training set is 0.9524727561323589
rmse loss on test set is 0.9797122535864082
for this epoch using 106.41591882705688 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988556572647834
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9392609740793256
  (0, 1678)	2.9961426229670725
  (0, 1677)	2.8823793251915752
  (0, 1676)	2.9985631354212186
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9569554337590382
  (0, 1671)	2.9474283468377203
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9641340378363075
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9641340378363075
  (0, 1664)	3.000000000000002
  (0, 1663)	2.894028816129851
  (0, 1662)	2.9641340378363075
  (0, 1661)	2.954703947916844
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.014715446738992
  :	:
  (942, 24)	3.8623291654678926
  (942, 23)	3.586600779640015
  (942, 22)	4.302418934593305
  (942, 21)	4.330159249883448
  (942, 20)	2.8706333719703085
  (942, 19)	3.5663578584321804
  (942, 18)	4.1713889408553975
  (942, 17)	3.125442165469438
  (942, 16)	3.426546273239082
  (942, 15)	3.5318180105119685
  (942, 14)	3.924475966864692
  (942, 13)	4.153287296670966
  (942, 12)	3.5088912723232384
  (942, 11)	4.4717471850907184
  (942, 10)	3.9008352672968707
  (942, 9)	3.842264745125679
  (942, 8)	4.0620286974003825
  (942, 7)	3.9450226629442517
  (942, 6)	3.886158706816323
  (942, 5)	3.8261545715133845
  (942, 4)	3.432056008407545
  (942, 3)	3.4757069852052895
  (942, 2)	2.981512494879324
  (942, 1)	3.125139941885152
  (942, 0)	3.9535295730417976
this is the 191 epoch
rmse loss on training set is 0.9523704331267973
rmse loss on test set is 0.9796537215752039
for this epoch using 108.05751705169678 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9885024912856455
  (0, 1680)	3.000000000000002
  (0, 1679)	2.938947106360888
  (0, 1678)	2.996118872545404
  (0, 1677)	2.881775340176373
  (0, 1676)	2.9985717110737005
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9566918044950765
  (0, 1671)	2.9471900371665414
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.963962963090843
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.963962963090843
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8935123927101625
  (0, 1662)	2.963962963090843
  (0, 1661)	2.954497502475428
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.014820791392769
  :	:
  (942, 24)	3.8623250509892415
  (942, 23)	3.5865454190131705
  (942, 22)	4.303174291615219
  (942, 21)	4.330688005381539
  (942, 20)	2.869712573900311
  (942, 19)	3.5670041812610633
  (942, 18)	4.172791630557679
  (942, 17)	3.124561874359211
  (942, 16)	3.4267798322709933
  (942, 15)	3.532552397614053
  (942, 14)	3.9243985230383207
  (942, 13)	4.153486510258557
  (942, 12)	3.509034328307203
  (942, 11)	4.4722072226192715
  (942, 10)	3.9011738052971534
  (942, 9)	3.843070751976852
  (942, 8)	4.062066664319293
  (942, 7)	3.9453736003123128
  (942, 6)	3.88618506114642
  (942, 5)	3.8280223841962555
  (942, 4)	3.4322726065580893
  (942, 3)	3.476093138339082
  (942, 2)	2.980824758603153
  (942, 1)	3.124645921184577
  (942, 0)	3.9536029555763865
this is the 192 epoch
rmse loss on training set is 0.9522690607352837
rmse loss on test set is 0.9795959048634835
for this epoch using 107.26431393623352 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9884484992062084
  (0, 1680)	3.000000000000002
  (0, 1679)	2.938633287217685
  (0, 1678)	2.996095090029967
  (0, 1677)	2.881171484405409
  (0, 1676)	2.9985803397499406
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.956428385843994
  (0, 1671)	2.9469519883777333
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.963791898702091
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.963791898702091
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8929962106920057
  (0, 1662)	2.963791898702091
  (0, 1661)	2.9542912733632867
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0149261241072147
  :	:
  (942, 24)	3.8623194908439515
  (942, 23)	3.5864871305463013
  (942, 22)	4.303921992484195
  (942, 21)	4.331211645694789
  (942, 20)	2.8687985760762778
  (942, 19)	3.567644222064924
  (942, 18)	4.174179118553118
  (942, 17)	3.123680525291748
  (942, 16)	3.427010555077185
  (942, 15)	3.533281707156499
  (942, 14)	3.9243202768788965
  (942, 13)	4.153681988717926
  (942, 12)	3.50917509217537
  (942, 11)	4.472662555853459
  (942, 10)	3.9015079290613204
  (942, 9)	3.8438667496277765
  (942, 8)	4.062102549153955
  (942, 7)	3.9457213478655104
  (942, 6)	3.886209648205964
  (942, 5)	3.8298811082315036
  (942, 4)	3.4324859988979437
  (942, 3)	3.4764767054344454
  (942, 2)	2.980141119030259
  (942, 1)	3.1241533696574413
  (942, 0)	3.953674419304458
this is the 193 epoch
rmse loss on training set is 0.9521686257501796
rmse loss on test set is 0.9795387928397457
for this epoch using 106.95944023132324 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988394595782025
  (0, 1680)	3.000000000000002
  (0, 1679)	2.938319516413477
  (0, 1678)	2.9960712752453196
  (0, 1677)	2.8805677575816317
  (0, 1676)	2.99858902081711
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.956165178797902
  (0, 1671)	2.946714198856319
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9636208446039105
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9636208446039105
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8924802685474997
  (0, 1662)	2.9636208446039105
  (0, 1661)	2.954085259701837
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015031443440545
  :	:
  (942, 24)	3.862312521586336
  (942, 23)	3.586425997560626
  (942, 22)	4.304662174047637
  (942, 21)	4.331730254028083
  (942, 20)	2.867891342610631
  (942, 19)	3.5682780556872413
  (942, 18)	4.17555159197695
  (942, 17)	3.122798160362311
  (942, 16)	3.427238492221864
  (942, 15)	3.5340059954115604
  (942, 14)	3.924241257393507
  (942, 13)	4.153873822024767
  (942, 12)	3.509313605559485
  (942, 11)	4.4731132815898205
  (942, 10)	3.9018377294385775
  (942, 9)	3.8446528752317453
  (942, 8)	4.062136402775017
  (942, 7)	3.946065966296898
  (942, 6)	3.8862325064661802
  (942, 5)	3.8317308159887324
  (942, 4)	3.4326962416957323
  (942, 3)	3.476857720008591
  (942, 2)	2.979461565655878
  (942, 1)	3.1236623075197465
  (942, 0)	3.9537439997640242
this is the 194 epoch
rmse loss on training set is 0.9520691152110945
rmse loss on test set is 0.9794823750958804
for this epoch using 108.41511344909668 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9883407803944926
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9380057937166564
  (0, 1678)	2.9960474280195477
  (0, 1677)	2.8799641594137633
  (0, 1676)	2.9985977536549417
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.955902184307019
  (0, 1671)	2.946476667003474
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.963449800733265
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.963449800733265
  (0, 1664)	3.000000000000002
  (0, 1663)	2.891964564766458
  (0, 1662)	2.963449800733265
  (0, 1661)	2.9538794606169665
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015136747979576
  :	:
  (942, 24)	3.8623041788720545
  (942, 23)	3.586362101454083
  (942, 22)	4.305394970013542
  (942, 21)	4.332243911699067
  (942, 20)	2.8669908373940767
  (942, 19)	3.5689057558841393
  (942, 18)	4.176909235345263
  (942, 17)	3.121914820772285
  (942, 16)	3.4274636932620344
  (942, 15)	3.5347253177264886
  (942, 14)	3.9241614928821487
  (942, 13)	4.154062097800265
  (942, 12)	3.5094499091909315
  (942, 11)	4.473559493942562
  (942, 10)	3.9021632951320204
  (942, 9)	3.8454292638613565
  (942, 8)	4.06216827471097
  (942, 7)	3.9464075146627584
  (942, 6)	3.8862536736932616
  (942, 5)	3.8335715788474487
  (942, 4)	3.4329033901222106
  (942, 3)	3.47723621484677
  (942, 2)	2.9787860876058208
  (942, 1)	3.1231727542824865
  (942, 0)	3.953811731950978
this is the 195 epoch
rmse loss on training set is 0.9519705163989859
rmse loss on test set is 0.9794266414221805
for this epoch using 111.57742023468018 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988287052433745
  (0, 1680)	3.000000000000002
  (0, 1679)	2.937692118900254
  (0, 1678)	2.9960235481842896
  (0, 1677)	2.8793606896162265
  (0, 1676)	2.9986065376554243
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9556394032807836
  (0, 1671)	2.9462393912364178
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9632787670301237
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9632787670301237
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8914490978561673
  (0, 1662)	2.9632787670301237
  (0, 1661)	2.9536738752390836
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015242036339176
  :	:
  (942, 24)	3.862294497485313
  (942, 23)	3.586295521744944
  (942, 22)	4.30612051103037
  (942, 21)	4.332752698192912
  (942, 20)	2.866097024110553
  (942, 19)	3.569527395343968
  (942, 18)	4.178252230597506
  (942, 17)	3.121030546848846
  (942, 16)	3.4276862067694487
  (942, 15)	3.535439728542427
  (942, 14)	3.924081010956365
  (942, 13)	4.15424690137791
  (942, 12)	3.5095840429238763
  (942, 11)	4.474001284426895
  (942, 10)	3.9024847127533224
  (942, 9)	3.846196048543832
  (942, 8)	4.062198213191158
  (942, 7)	3.946746050433121
  (942, 6)	3.886273186960204
  (942, 5)	3.8354034672156425
  (942, 4)	3.4331074982737904
  (942, 3)	3.4776122220221146
  (942, 2)	2.978114673649775
  (942, 1)	3.1226847287689474
  (942, 0)	3.9538776503241784
this is the 196 epoch
rmse loss on training set is 0.9518728168303588
rmse loss on test set is 0.9793715818024891
for this epoch using 110.65706062316895 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9882334112985482
  (0, 1680)	3.000000000000002
  (0, 1679)	2.937378491741877
  (0, 1678)	2.9959996355746807
  (0, 1677)	2.8787573479090773
  (0, 1676)	2.998615372222481
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9553768365889304
  (0, 1671)	2.9460023699882325
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.963107743437339
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.963107743437339
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8909338663412334
  (0, 1662)	2.963107743437339
  (0, 1661)	2.9534685027030663
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015347307161613
  :	:
  (942, 24)	3.8622835113650607
  (942, 23)	3.5862263361143394
  (942, 22)	4.3068389247648655
  (942, 21)	4.33325669121502
  (942, 20)	2.8652098662517815
  (942, 19)	3.570143045706355
  (942, 18)	4.179580757138107
  (942, 17)	3.120145378064199
  (942, 16)	3.4279060803520984
  (942, 15)	3.5361492814129867
  (942, 14)	3.9239998385574513
  (942, 13)	4.15442831586838
  (942, 12)	3.5097160457577554
  (942, 11)	4.474438742039627
  (942, 10)	3.902802066876078
  (942, 9)	3.84695336029557
  (942, 8)	4.062226265187128
  (942, 7)	3.947081629540628
  (942, 6)	3.886291082658472
  (942, 5)	3.8372265505479217
  (942, 4)	3.433308619195436
  (942, 3)	3.4779857729149373
  (942, 2)	2.9774473122143212
  (942, 1)	3.122198249131514
  (942, 0)	3.953941788810479
this is the 197 epoch
rmse loss on training set is 0.9517760042517442
rmse loss on test set is 0.9793171864095223
for this epoch using 111.78303241729736 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9881798563961897
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9370649120236574
  (0, 1678)	2.995975690029325
  (0, 1677)	2.8781541340179944
  (0, 1676)	2.998624256771685
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9551144850625892
  (0, 1671)	2.9457656017077176
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9629367299005853
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9629367299005853
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8904188687634007
  (0, 1662)	2.9629367299005853
  (0, 1661)	2.9532633421482775
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015452559116038
  :	:
  (942, 24)	3.862271253630166
  (942, 23)	3.5861546204477626
  (942, 22)	4.307550335977646
  (942, 21)	4.333755966742042
  (942, 20)	2.8643293271312302
  (942, 19)	3.5707527775810317
  (942, 18)	4.180894991877353
  (942, 17)	3.119259353054342
  (942, 16)	3.428123360675127
  (942, 15)	3.536854029022198
  (942, 14)	3.923918001973864
  (942, 13)	4.154606422222313
  (942, 12)	3.5098459558590527
  (942, 11)	4.4748719533370265
  (942, 10)	3.9031154400875248
  (942, 9)	3.847701328156194
  (942, 8)	4.062252476452509
  (942, 7)	3.9474143064278167
  (942, 6)	3.8863073965093267
  (942, 5)	3.839040897363227
  (942, 4)	3.4335068049030633
  (942, 3)	3.4783568982314548
  (942, 2)	2.9767839913954726
  (942, 1)	3.121713332868103
  (942, 0)	3.95400418080996
this is the 198 epoch
rmse loss on training set is 0.9516800666342686
rmse loss on test set is 0.9792634456002797
for this epoch using 112.31296253204346 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.988126387142305
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9367513795322266
  (0, 1678)	2.9959517113902807
  (0, 1677)	2.8775510476741766
  (0, 1676)	2.998633190729934
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9548523494953143
  (0, 1671)	2.945529084859255
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9627657263682052
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9627657263682052
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8899041036813515
  (0, 1662)	2.9627657263682052
  (0, 1661)	2.9530583927185563
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0155577908978977
  :	:
  (942, 24)	3.862257756603603
  (942, 23)	3.5860804488756943
  (942, 22)	4.308254866596853
  (942, 21)	4.334250599071082
  (942, 20)	2.863455369897738
  (942, 19)	3.5713566605660114
  (942, 18)	4.1821951092715075
  (942, 17)	3.1183725096373727
  (942, 16)	3.428338093481265
  (942, 15)	3.5375540232022433
  (942, 14)	3.9238355268581158
  (942, 13)	4.154781299291275
  (942, 12)	3.5099738105824434
  (942, 11)	4.475301002510238
  (942, 10)	3.903424913038944
  (942, 9)	3.8484400792218
  (942, 8)	4.0622768915613765
  (942, 7)	3.947744134092756
  (942, 6)	3.8863221635751404
  (942, 5)	3.8408465752622027
  (942, 4)	3.433702106405389
  (942, 3)	3.4787256280219165
  (942, 2)	2.9761246989709704
  (942, 1)	3.1212299968381205
  (942, 0)	3.9540648592011953
this is the 199 epoch
rmse loss on training set is 0.9515849921683696
rmse loss on test set is 0.9792103499116719
for this epoch using 114.90385365486145 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9880730029608062
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9364378940586615
  (0, 1678)	2.9959276995029813
  (0, 1677)	2.8769480886143377
  (0, 1676)	2.998642173535226
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9545904306440933
  (0, 1671)	2.9452928179226405
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9625947327911466
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9625947327911466
  (0, 1664)	3.000000000000002
  (0, 1663)	2.889389569670531
  (0, 1662)	2.9625947327911466
  (0, 1661)	2.9528536535622134
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0156630012283743
  :	:
  (942, 24)	3.862243051835783
  (942, 23)	3.586003893813183
  (942, 22)	4.308952635789781
  (942, 21)	4.334740660867211
  (942, 20)	2.8625879575486417
  (942, 19)	3.571954763265589
  (942, 18)	4.1834812813621
  (942, 17)	3.117484884831388
  (942, 16)	3.4285503236107524
  (942, 15)	3.5382493149505985
  (942, 14)	3.923752438243172
  (942, 13)	4.154953023886781
  (942, 12)	3.5100996464913177
  (942, 11)	4.475725971458131
  (942, 10)	3.9037305644946456
  (942, 9)	3.8491697386776655
  (942, 8)	4.06229955394521
  (942, 7)	3.94807116413331
  (942, 6)	3.886335418270387
  (942, 5)	3.842643650944094
  (942, 4)	3.4338945737252717
  (942, 3)	3.479091991698226
  (942, 2)	2.975469422412134
  (942, 1)	3.1207482572781
  (942, 0)	3.954123856346575
this is the 200 epoch
rmse loss on training set is 0.951490769258737
rmse loss on test set is 0.9791578900562049
for this epoch using 117.24867796897888 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9880197032837215
  (0, 1680)	3.000000000000002
  (0, 1679)	2.936124455398431
  (0, 1678)	2.9959036542162694
  (0, 1677)	2.876345256580596
  (0, 1676)	2.998651204636316
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9543287292303373
  (0, 1671)	2.9450567993929626
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.962423749122876
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.962423749122876
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8888752653229917
  (0, 1662)	2.962423749122876
  (0, 1661)	2.952649123831998
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0157681888538677
  :	:
  (942, 24)	3.8622271701270474
  (942, 23)	3.5859250259985767
  (942, 22)	4.309643760032491
  (942, 21)	4.335226223209416
  (942, 20)	2.8617270529424994
  (942, 19)	3.572547153307727
  (942, 18)	4.184753677814584
  (942, 17)	3.1165965148719272
  (942, 16)	3.4287600950207504
  (942, 15)	3.538939954446891
  (942, 14)	3.923668760558102
  (942, 13)	4.155121670837576
  (942, 12)	3.5102234993777075
  (942, 11)	4.476146939857824
  (942, 10)	3.9040324713796717
  (942, 9)	3.849890429830348
  (942, 8)	4.062320505928492
  (942, 7)	3.948395446789804
  (942, 6)	3.8863471943725134
  (942, 5)	3.8444321902233685
  (942, 4)	3.4340842559205713
  (942, 3)	3.4794560180510197
  (942, 2)	2.974818148895458
  (942, 1)	3.120268129816896
  (942, 0)	3.954181204097805
this is the 201 epoch
rmse loss on training set is 0.9513973865192994
rmse loss on test set is 0.9791060569178367
for this epoch using 115.73070907592773 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9879664875510854
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9358110633513825
  (0, 1678)	2.9958795753823058
  (0, 1677)	2.875742551320453
  (0, 1676)	2.998660283492493
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9540672459408652
  (0, 1671)	2.944821027780414
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.962252775319259
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.962252775319259
  (0, 1664)	3.000000000000002
  (0, 1663)	2.888361189247179
  (0, 1662)	2.962252775319259
  (0, 1661)	2.95244480268515
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0158733525454524
  :	:
  (942, 24)	3.8622101415492147
  (942, 23)	3.5858439145313405
  (942, 22)	4.310328353177701
  (942, 21)	4.3357073556349315
  (942, 20)	2.8608726188113995
  (942, 19)	3.57313389736129
  (942, 18)	4.186012465956159
  (942, 17)	3.115707435229027
  (942, 16)	3.4289674508043415
  (942, 15)	3.5396259910692733
  (942, 14)	3.9235845176434347
  (942, 13)	4.1552873130450845
  (942, 12)	3.510345404281569
  (942, 11)	4.476563985232855
  (942, 10)	3.9043307088261248
  (942, 9)	3.8506022741390473
  (942, 8)	4.062339788762985
  (942, 7)	3.948717030986448
  (942, 6)	3.886357525032635
  (942, 5)	3.8462122580458935
  (942, 4)	3.434271201104465
  (942, 3)	3.479817735266217
  (942, 2)	2.9741708653138406
  (942, 1)	3.119789629490527
  (942, 0)	3.954236933801254
this is the 202 epoch
rmse loss on training set is 0.9513048327684703
rmse loss on test set is 0.9790548415479245
for this epoch using 111.46893525123596 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9879133552108255
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9354977177216597
  (0, 1678)	2.995855462856589
  (0, 1677)	2.8751399725867337
  (0, 1676)	2.9986694095732975
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.953805981428805
  (0, 1671)	2.944585501610186
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9620818113385243
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9620818113385243
  (0, 1664)	3.000000000000002
  (0, 1663)	2.887847340067789
  (0, 1662)	2.9620818113385243
  (0, 1661)	2.9522406892833257
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.015978491098387
  :	:
  (942, 24)	3.862191995466464
  (942, 23)	3.585760626908946
  (942, 22)	4.311006526520702
  (942, 21)	4.336184126182091
  (942, 20)	2.860024617772861
  (942, 19)	3.573715061152693
  (942, 18)	4.187257810813
  (942, 17)	3.1148176806238643
  (942, 16)	3.4291724332090174
  (942, 15)	3.540307473410515
  (942, 14)	3.923499732765889
  (942, 13)	4.155450021537282
  (942, 12)	3.510465395509588
  (942, 11)	4.476977183019153
  (942, 10)	3.9046253502183106
  (942, 9)	3.8513053912465685
  (942, 8)	4.062357442660831
  (942, 7)	3.949035964371199
  (942, 6)	3.886366442785988
  (942, 5)	3.8479839185048257
  (942, 4)	3.4344554564653746
  (942, 3)	3.4801771709411127
  (942, 2)	2.9735275582874974
  (942, 1)	3.119312770756612
  (942, 0)	3.9542910763035684
this is the 203 epoch
rmse loss on training set is 0.9512130970244128
rmse loss on test set is 0.9790042351613406
for this epoch using 108.96947503089905 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9878603057186277
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9351844183176983
  (0, 1678)	2.9958313164978696
  (0, 1677)	2.8745375201375265
  (0, 1676)	2.998678582358287
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9535449363145654
  (0, 1671)	2.944350219422291
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9619108571411408
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9619108571411408
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8873337164255832
  (0, 1662)	2.9619108571411408
  (0, 1661)	2.9520367827926335
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0160836033315803
  :	:
  (942, 24)	3.862172760555382
  (942, 23)	3.5856752290630065
  (942, 22)	4.31167838886364
  (942, 21)	4.336656601431812
  (942, 20)	2.8591830123413984
  (942, 19)	3.574290709482359
  (942, 18)	4.188489875146683
  (942, 17)	3.1139272850449817
  (942, 16)	3.429375083654789
  (942, 15)	3.540984449293622
  (942, 14)	3.9234144286326256
  (942, 13)	4.155609865520922
  (942, 12)	3.510583506653386
  (942, 11)	4.477386606628859
  (942, 10)	3.9049164672366032
  (942, 9)	3.8519998990095723
  (942, 8)	4.062373506826298
  (942, 7)	3.949352293354558
  (942, 6)	3.8863739795623293
  (942, 5)	3.849747234856071
  (942, 4)	3.4346370682863885
  (942, 3)	3.480534352099989
  (942, 2)	2.9728882141745836
  (942, 1)	3.1188375675085407
  (942, 0)	3.954343661957256
this is the 204 epoch
rmse loss on training set is 0.9511221685005252
rmse loss on test set is 0.9789542291326392
for this epoch using 108.11350798606873 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987807338537829
  (0, 1680)	3.000000000000002
  (0, 1679)	2.934871164952126
  (0, 1678)	2.995807136168144
  (0, 1677)	2.8739351937361106
  (0, 1676)	2.998687801336743
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9532841111866506
  (0, 1671)	2.944115179771424
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9617399126897532
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9617399126897532
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8868203169772215
  (0, 1662)	2.9617399126897532
  (0, 1661)	2.9518330823836187
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0161886880871087
  :	:
  (942, 24)	3.862152464824288
  (942, 23)	3.5855877853944174
  (942, 22)	4.312344046577918
  (942, 21)	4.337124846547541
  (942, 20)	2.858347764939628
  (942, 19)	3.574860906240701
  (942, 18)	4.1897088194900105
  (942, 17)	3.1130362817641966
  (942, 16)	3.4295754427517315
  (942, 15)	3.5416569657871593
  (942, 14)	3.923328627405147
  (942, 13)	4.155766912432041
  (942, 12)	3.5106997706071668
  (942, 11)	4.477792327512022
  (942, 10)	3.905204129900165
  (942, 9)	3.852685913528248
  (942, 8)	4.062388019486534
  (942, 7)	3.9496660631468843
  (942, 6)	3.886380166696086
  (942, 5)	3.8515022695335017
  (942, 4)	3.4348160819641653
  (942, 3)	3.480889305209229
  (942, 2)	2.9722528190814947
  (942, 1)	3.1183640330891533
  (942, 0)	3.954394720626202
this is the 205 epoch
rmse loss on training set is 0.9510320366009898
rmse loss on test set is 0.9789048149923658
for this epoch using 109.32240843772888 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.98775445313928
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9345579574417804
  (0, 1678)	2.9957829217326375
  (0, 1677)	2.873332993150927
  (0, 1676)	2.9986970660075047
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9530235066026185
  (0, 1671)	2.943880381226832
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9615689779491237
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9615689779491237
  (0, 1664)	3.000000000000002
  (0, 1663)	2.886307140395069
  (0, 1662)	2.9615689779491237
  (0, 1661)	2.951629587231232
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.016293744229765
  :	:
  (942, 24)	3.862131135631902
  (942, 23)	3.585498358807902
  (942, 22)	4.3130036036650825
  (942, 21)	4.337588925314045
  (942, 20)	2.857518837909081
  (942, 19)	3.5754257144238406
  (942, 18)	4.19091480218216
  (942, 17)	3.1121447033520595
  (942, 16)	3.429773550317305
  (942, 15)	3.5423250692201886
  (942, 14)	3.9232423507126635
  (942, 13)	4.155921227985158
  (942, 12)	3.510814219584946
  (942, 11)	4.478194415216381
  (942, 10)	3.9054884066085194
  (942, 9)	3.8533635491755307
  (942, 8)	4.06240101792113
  (942, 7)	3.9499773177946462
  (942, 6)	3.8863850349363513
  (942, 5)	3.8532490841637737
  (942, 4)	3.4349925420275538
  (942, 3)	3.4812420561920465
  (942, 2)	2.9716213588729237
  (942, 1)	3.117892180304242
  (942, 0)	3.9544442816914316
this is the 206 epoch
rmse loss on training set is 0.9509426909165177
rmse loss on test set is 0.9788559844234984
for this epoch using 108.36878657341003 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9877016490012456
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9342447956076136
  (0, 1678)	2.9957586730597394
  (0, 1677)	2.872730918155481
  (0, 1676)	2.998706375878661
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.952763123089845
  (0, 1671)	2.9436458223721287
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.961398052886029
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.961398052886029
  (0, 1664)	3.000000000000002
  (0, 1663)	2.885794185367074
  (0, 1662)	2.961398052886029
  (0, 1661)	2.9514262965148745
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.016398770646545
  :	:
  (942, 24)	3.8621087997052816
  (942, 23)	3.585407010745551
  (942, 22)	4.313657161815981
  (942, 21)	4.3380489001747495
  (942, 20)	2.856696193520663
  (942, 19)	3.575985196148968
  (942, 18)	4.192107979403153
  (942, 17)	3.111252581693019
  (942, 16)	3.4299694453930725
  (942, 15)	3.5429888051969063
  (942, 14)	3.923155619665074
  (942, 13)	4.156072876220856
  (942, 12)	3.5109268851371893
  (942, 11)	4.478592937445082
  (942, 10)	3.905769364181949
  (942, 9)	3.8540329186256885
  (942, 8)	4.062412538490621
  (942, 7)	3.9502861002153775
  (942, 6)	3.886388614456782
  (942, 5)	3.8549877395808383
  (942, 4)	3.435166492155649
  (942, 3)	3.4815926304427043
  (942, 2)	2.970993819181585
  (942, 1)	3.1174220214356048
  (942, 0)	3.9544923740566995
this is the 207 epoch
rmse loss on training set is 0.9508541212201335
rmse loss on test set is 0.9788077292579269
for this epoch using 111.49577069282532 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987648925609278
  (0, 1680)	3.000000000000002
  (0, 1679)	2.933931679274656
  (0, 1678)	2.9957343900209805
  (0, 1677)	2.8721289685283384
  (0, 1676)	2.9987157304673566
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.952502961146403
  (0, 1671)	2.9434115018051994
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9612271374692263
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9612271374692263
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8852814505965423
  (0, 1662)	2.9612271374692263
  (0, 1661)	2.9512232094183193
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0165037662462098
  :	:
  (942, 24)	3.862085483157169
  (942, 23)	3.585313801219742
  (942, 22)	4.314304820468394
  (942, 21)	4.338504832267958
  (942, 20)	2.8558797939846885
  (942, 19)	3.5765394126693386
  (942, 18)	4.193288505207749
  (942, 17)	3.1103599480001725
  (942, 16)	3.430163166261167
  (942, 15)	3.5436482186108638
  (942, 14)	3.923068454865542
  (942, 13)	4.1562219195519186
  (942, 12)	3.5110377981670156
  (942, 11)	4.47898796011263
  (942, 10)	3.9060470679008756
  (942, 9)	3.854694132882418
  (942, 8)	4.062422616663939
  (942, 7)	3.9505924522316214
  (942, 6)	3.886390934865241
  (942, 5)	3.8567182958401602
  (942, 4)	3.435337975195469
  (942, 3)	3.48194105284041
  (942, 2)	2.9703701854176883
  (942, 1)	3.1169535682538276
  (942, 0)	3.9545390261542304
this is the 208 epoch
rmse loss on training set is 0.9507663174631347
rmse loss on test set is 0.9787600414730987
for this epoch using 113.85960841178894 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987596282456112
  (0, 1680)	3.000000000000002
  (0, 1679)	2.933618608272008
  (0, 1678)	2.9957100724909975
  (0, 1677)	2.8715271440530157
  (0, 1676)	2.998725129299595
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.952243021241857
  (0, 1671)	2.9431774181380104
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.961056231669364
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.961056231669364
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8847689348020085
  (0, 1662)	2.961056231669364
  (0, 1661)	2.95102032512977
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0166087299588367
  :	:
  (942, 24)	3.86206121150276
  (942, 23)	3.5852187888452076
  (942, 22)	4.314946676863077
  (942, 21)	4.338956781461858
  (942, 20)	2.855069601460713
  (942, 19)	3.5770884243890415
  (942, 18)	4.194456531558661
  (942, 17)	3.109466832829732
  (942, 16)	3.4303547504602516
  (942, 15)	3.544303353658978
  (942, 14)	3.9229808764227756
  (942, 13)	4.156368418808225
  (942, 12)	3.5111469889459515
  (942, 11)	4.47937954739896
  (942, 10)	3.9063215815441583
  (942, 9)	3.8553473013064097
  (942, 8)	4.062431287044978
  (942, 7)	3.9508964146036503
  (942, 6)	3.8863920252133877
  (942, 5)	3.8584408122326344
  (942, 4)	3.435507033179303
  (942, 3)	3.482287347762719
  (942, 2)	2.969750442778177
  (942, 1)	3.1164868320307777
  (942, 0)	3.954584265950417
this is the 209 epoch
rmse loss on training set is 0.9506792697711189
rmse loss on test set is 0.978712913188699
for this epoch using 117.10579442977905 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9875437190415295
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9333055824327277
  (0, 1678)	2.9956857203475122
  (0, 1677)	2.870925444517947
  (0, 1676)	2.9987345719099574
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.951983303818029
  (0, 1671)	2.942943569996492
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9608853354589235
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9608853354589235
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8842566367170828
  (0, 1662)	2.9608853354589235
  (0, 1661)	2.9508176428417983
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.01671366073537
  :	:
  (942, 24)	3.862036009675723
  (942, 23)	3.5851220308703806
  (942, 22)	4.315582826098391
  (942, 21)	4.3394048063883535
  (942, 20)	2.854265578066923
  (942, 19)	3.5776322908773515
  (942, 18)	4.195612208359199
  (942, 17)	3.1085732660951115
  (942, 16)	3.430544234801223
  (942, 15)	3.544954253855082
  (942, 14)	3.9228929039627505
  (942, 13)	4.156512433280123
  (942, 12)	3.5112544871292166
  (942, 11)	4.479767761801773
  (942, 10)	3.9065929674263478
  (942, 9)	3.855992531642372
  (942, 8)	4.062438583398126
  (942, 7)	3.951198027061188
  (942, 6)	3.8863919140060874
  (942, 5)	3.8601553472981567
  (942, 4)	3.4356737073415413
  (942, 3)	3.4826315390985845
  (942, 2)	2.969134576255627
  (942, 1)	3.1160218235517236
  (942, 0)	3.954628120951575
this is the 210 epoch
rmse loss on training set is 0.9505929684401646
rmse loss on test set is 0.978666336663494
for this epoch using 119.17474102973938 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9874912348722678
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9329926015938517
  (0, 1678)	2.995661333471274
  (0, 1677)	2.87032386971642
  (0, 1676)	2.9987440578414644
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.951723809289784
  (0, 1671)	2.94270995602038
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9607144488121477
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9607144488121477
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8837445550902467
  (0, 1662)	2.9607144488121477
  (0, 1661)	2.9506151617513905
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.016818557547192
  :	:
  (942, 24)	3.862009902043946
  (942, 23)	3.5850235832081077
  (942, 22)	4.316213361183424
  (942, 21)	4.339848964475809
  (942, 20)	2.853467685889307
  (942, 19)	3.578171070882852
  (942, 18)	4.196755683485297
  (942, 17)	3.1076792770807162
  (942, 16)	3.430731655382437
  (942, 15)	3.5456009620433497
  (942, 14)	3.9228045566403402
  (942, 13)	4.156654020760659
  (942, 12)	3.5113603217705576
  (942, 11)	4.480152664187248
  (942, 10)	3.906861286434
  (942, 9)	3.856629930045635
  (942, 8)	4.062444538672925
  (942, 7)	3.951497328334125
  (942, 6)	3.886390629210684
  (942, 5)	3.8618619588389693
  (942, 4)	3.4358380381351923
  (942, 3)	3.482973650261085
  (942, 2)	2.9685225706470137
  (942, 1)	3.115558553127255
  (942, 0)	3.954670618209643
this is the 211 epoch
rmse loss on training set is 0.9505074039330734
rmse loss on test set is 0.9786203042921752
for this epoch using 115.74535942077637 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987438829461893
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9326796655962943
  (0, 1678)	2.995636911746062
  (0, 1677)	2.8697224194465334
  (0, 1676)	2.9987535866453254
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.951464538045742
  (0, 1671)	2.9424765748630994
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9605435717050113
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9605435717050113
  (0, 1664)	3.000000000000002
  (0, 1663)	2.883232688684726
  (0, 1662)	2.9605435717050113
  (0, 1661)	2.950412881059881
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.016923419385729
  :	:
  (942, 24)	3.8619829124243883
  (942, 23)	3.584923500465503
  (942, 22)	4.316838373089673
  (942, 21)	4.34028931198071
  (942, 20)	2.8526758869904527
  (942, 19)	3.578704822347262
  (942, 18)	4.19788710281699
  (942, 17)	3.1067848944553718
  (942, 16)	3.4309170476046553
  (942, 15)	3.54624352041122
  (942, 14)	3.9227158531503434
  (942, 13)	4.156793237586444
  (942, 12)	3.511464521336703
  (942, 11)	4.480534313838965
  (942, 10)	3.907126598060957
  (942, 9)	3.8572596011081655
  (942, 8)	4.062449185027827
  (942, 7)	3.9517943561821576
  (942, 6)	3.8863881982660606
  (942, 5)	3.8635607039326323
  (942, 4)	3.4360000652480447
  (942, 3)	3.4833137041996305
  (942, 2)	2.9679144105621154
  (942, 1)	3.1150970306048134
  (942, 0)	3.954711784327941
this is the 212 epoch
rmse loss on training set is 0.9504225668757346
rmse loss on test set is 0.9785748086023813
for this epoch using 113.35741591453552 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987386502330704
  (0, 1680)	3.000000000000002
  (0, 1679)	2.93236677428484
  (0, 1678)	2.9956124550585876
  (0, 1677)	2.869121093511096
  (0, 1676)	2.998763157880748
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9512054904490643
  (0, 1671)	2.942243425191575
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9603727041151386
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9603727041151386
  (0, 1664)	3.000000000000002
  (0, 1663)	2.882721036278298
  (0, 1662)	2.9603727041151386
  (0, 1661)	2.95021079997298
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0170282452619785
  :	:
  (942, 24)	3.8619550640977454
  (942, 23)	3.5848218359732793
  (942, 22)	4.317457950801446
  (942, 21)	4.340725904018373
  (942, 20)	2.8518901434180917
  (942, 19)	3.5792336024189466
  (942, 18)	4.199006610269309
  (942, 17)	3.1058901462855006
  (942, 16)	3.4311004461855994
  (942, 15)	3.546881970502191
  (942, 14)	3.9226268117383847
  (942, 13)	4.156930138677418
  (942, 12)	3.5115671137213647
  (942, 11)	4.4809127685054015
  (942, 10)	3.907388960442754
  (942, 9)	3.857881647884166
  (942, 8)	4.062452553853201
  (942, 7)	3.9520891474235973
  (942, 6)	3.8863846480916773
  (942, 5)	3.865251638944867
  (942, 4)	3.4361598276183805
  (942, 3)	3.483651723411984
  (942, 2)	2.967310080431768
  (942, 1)	3.11463726538004
  (942, 0)	3.9547516454669527
this is the 213 epoch
rmse loss on training set is 0.9503384480535967
rmse loss on test set is 0.9785298422517379
for this epoch using 109.8006443977356 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987334253005614
  (0, 1680)	3.000000000000002
  (0, 1679)	2.932053927508077
  (0, 1678)	2.99558796329853
  (0, 1677)	2.8685198917176207
  (0, 1676)	2.99877277111477
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9509466668380817
  (0, 1671)	2.9420105056861363
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9602018460217496
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9602018460217496
  (0, 1664)	3.000000000000002
  (0, 1663)	2.882209596663186
  (0, 1662)	2.9602018460217496
  (0, 1661)	2.9500089177007642
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.017133034206187
  :	:
  (942, 24)	3.8619263798223895
  (942, 23)	3.5847186418143058
  (942, 22)	4.318072181364865
  (942, 21)	4.341158794592605
  (942, 20)	2.851110417213352
  (942, 19)	3.5797574674661665
  (942, 18)	4.200114347822575
  (942, 17)	3.1049950600479543
  (942, 16)	3.4312818851741866
  (942, 15)	3.5475163532282132
  (942, 14)	3.9225374502114327
  (942, 13)	4.157064777575288
  (942, 12)	3.511668126258938
  (942, 11)	4.481288084445872
  (942, 10)	3.9076484303900654
  (942, 9)	3.8584961719152098
  (942, 8)	4.062454675793425
  (942, 7)	3.9523817379631447
  (942, 6)	3.886380005096357
  (942, 5)	3.8669348195419717
  (942, 4)	3.436317363450441
  (942, 3)	3.48398772995584
  (942, 2)	2.966709564515855
  (942, 1)	3.114179266407801
  (942, 0)	3.954790227350008
this is the 214 epoch
rmse loss on training set is 0.9502550384081981
rmse loss on test set is 0.9784853980249998
for this epoch using 109.61279368400574 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987282081020037
  (0, 1680)	3.000000000000002
  (0, 1679)	2.931741125118353
  (0, 1678)	2.9955634363584664
  (0, 1677)	2.8679188138782394
  (0, 1676)	2.998782425922035
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.950688067527037
  (0, 1671)	2.941777815040349
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9600309974056276
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9600309974056276
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8816983686458313
  (0, 1662)	2.9600309974056276
  (0, 1661)	2.9498072334576513
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.017237785267384
  :	:
  (942, 24)	3.861896881847965
  (942, 23)	3.584613968851523
  (942, 22)	4.318681149935616
  (942, 21)	4.341588036624413
  (942, 20)	2.850336670418698
  (942, 19)	3.5802764730900107
  (942, 18)	4.201210455552242
  (942, 17)	3.104099662642557
  (942, 16)	3.431461397964419
  (942, 15)	3.5481467088819048
  (942, 14)	3.9224477859479925
  (942, 13)	4.157197206480977
  (942, 12)	3.5117675857376778
  (942, 11)	4.481660316474879
  (942, 10)	3.907905063421291
  (942, 9)	3.859103273254866
  (942, 8)	4.062455580768272
  (942, 7)	3.9526721628187884
  (942, 6)	3.8863742951870517
  (942, 5)	3.8686103007030668
  (942, 4)	3.436472710229421
  (942, 3)	3.4843217454600954
  (942, 2)	2.9661128469110416
  (942, 1)	3.1137230422129143
  (942, 0)	3.9548275552690124
this is the 215 epoch
rmse loss on training set is 0.9501723290338352
rmse loss on test set is 0.9784414688312759
for this epoch using 109.31848883628845 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9872299859137805
  (0, 1680)	3.000000000000002
  (0, 1679)	2.931428366971733
  (0, 1678)	2.9955388741338336
  (0, 1677)	2.8673178598096323
  (0, 1676)	2.9987921218846307
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.95042969280676
  (0, 1671)	2.9415453519609005
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9598601582490445
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9598601582490445
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8811873510468105
  (0, 1662)	2.9598601582490445
  (0, 1661)	2.949605746462391
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.017342497513042
  :	:
  (942, 24)	3.86186659192846
  (942, 23)	3.5845078667552825
  (942, 22)	4.319284939825408
  (942, 21)	4.342013681979845
  (942, 20)	2.849568865085709
  (942, 19)	3.5807906741372117
  (942, 18)	4.202295071658122
  (942, 17)	3.1032039804044063
  (942, 16)	3.4316390173090023
  (942, 15)	3.548773077148421
  (942, 14)	3.922357835908087
  (942, 13)	4.1573274762909165
  (942, 12)	3.5118655184126433
  (942, 11)	4.482029518005209
  (942, 10)	3.9081589137942974
  (942, 9)	3.8597030504929255
  (942, 8)	4.062455297993562
  (942, 7)	3.9529604561479688
  (942, 6)	3.8863675437773817
  (942, 5)	3.8702781367320624
  (942, 4)	3.4366259047362573
  (942, 3)	3.4846537911357993
  (942, 2)	2.9655199115584066
  (942, 1)	3.1132686009007466
  (942, 0)	3.95486365409024
this is the 216 epoch
rmse loss on training set is 0.9500903111742823
rmse loss on test set is 0.9783980477013331
for this epoch using 108.29234433174133 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9871779672329626
  (0, 1680)	3.000000000000002
  (0, 1679)	2.931115652927955
  (0, 1678)	2.995514276522899
  (0, 1677)	2.8667170293330044
  (0, 1676)	2.9988018585919303
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9501715429452773
  (0, 1671)	2.9413131151674263
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9596893285357213
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9596893285357213
  (0, 1664)	3.000000000000002
  (0, 1663)	2.880676542700618
  (0, 1662)	2.9596893285357213
  (0, 1661)	2.949404455938069
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.017447170028677
  :	:
  (942, 24)	3.8618355313348838
  (942, 23)	3.5844003840300007
  (942, 22)	4.319883632547189
  (942, 21)	4.342435781496916
  (942, 20)	2.8488069632824082
  (942, 19)	3.5813001247124503
  (942, 18)	4.203368332493105
  (942, 17)	3.102308039115799
  (942, 16)	3.431814775332564
  (942, 15)	3.549395497117058
  (942, 14)	3.922267616642876
  (942, 13)	4.1574556366321955
  (942, 12)	3.511961950018193
  (942, 11)	4.482395741089546
  (942, 10)	3.908410034537258
  (942, 9)	3.86029560077918
  (942, 8)	4.062453856001053
  (942, 7)	3.953246651272652
  (942, 6)	3.8863597757961004
  (942, 5)	3.871938381269353
  (942, 4)	3.4367769830619794
  (942, 3)	3.484983887786751
  (942, 2)	2.964930742250689
  (942, 1)	3.112815950167369
  (942, 0)	3.954898548259962
this is the 217 epoch
rmse loss on training set is 0.9500089762196049
rmse loss on test set is 0.9783551277849417
for this epoch using 107.86703848838806 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987126024529885
  (0, 1680)	3.000000000000002
  (0, 1679)	2.930802982850376
  (0, 1678)	2.9954896434267515
  (0, 1677)	2.8661163222739976
  (0, 1676)	2.99881163564036
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9499136181884817
  (0, 1671)	2.941081103392396
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9595185082508038
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9595185082508038
  (0, 1664)	3.000000000000002
  (0, 1663)	2.880165942455556
  (0, 1662)	2.9595185082508038
  (0, 1661)	2.9492033611120867
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0175518019175156
  :	:
  (942, 24)	3.8618037208675124
  (942, 23)	3.5842915680401934
  (942, 22)	4.320477307859269
  (942, 21)	4.342854385011691
  (942, 20)	2.848050927100549
  (942, 19)	3.5818048781906806
  (942, 18)	4.204430372591414
  (942, 17)	3.101411864017978
  (942, 16)	3.431988703544649
  (942, 15)	3.5500140072926727
  (942, 14)	3.9221771443040736
  (942, 13)	4.157581735896826
  (942, 12)	3.5120569057802027
  (942, 11)	4.482759036460699
  (942, 10)	3.9086584774787663
  (942, 9)	3.860881019846697
  (942, 8)	4.062451282657674
  (942, 7)	3.9535307807038405
  (942, 6)	3.886351015695361
  (942, 5)	3.873591087303318
  (942, 4)	3.4369259806217762
  (942, 3)	3.485312055819894
  (942, 2)	2.964345322639494
  (942, 1)	3.1123650973096617
  (942, 0)	3.9549322618101397
this is the 218 epoch
rmse loss on training set is 0.9499283157030881
rmse loss on test set is 0.9783127023483529
for this epoch using 107.98143982887268 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9870741573629385
  (0, 1680)	3.000000000000002
  (0, 1679)	2.930490356605932
  (0, 1678)	2.9954649747492232
  (0, 1677)	2.8655157384626464
  (0, 1676)	2.9988214526333046
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9496559187607536
  (0, 1671)	2.9408493153809814
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.959347697380777
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.959347697380777
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8796555491735694
  (0, 1662)	2.959347697380777
  (0, 1661)	2.9490024612161663
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0176563923000717
  :	:
  (942, 24)	3.8617711808677453
  (942, 23)	3.5841814650360466
  (942, 22)	4.321066043808155
  (942, 21)	4.343269541383555
  (942, 20)	2.847300718662504
  (942, 19)	3.582304987229001
  (942, 18)	4.205481324696308
  (942, 17)	3.100515479822574
  (942, 16)	3.4321608328523983
  (942, 15)	3.550628645606785
  (942, 14)	3.9220864346531035
  (942, 13)	4.1577058212748765
  (942, 12)	3.512150410427895
  (942, 11)	4.483119453570758
  (942, 10)	3.908904293277106
  (942, 9)	3.8614594020348223
  (942, 8)	4.062447605184126
  (942, 7)	3.9538128761652307
  (942, 6)	3.8863412874589707
  (942, 5)	3.8752363071815386
  (942, 4)	3.437072932168788
  (942, 3)	3.4856383152552852
  (942, 2)	2.963763636242194
  (942, 1)	3.1119160492350573
  (942, 0)	3.954964818364144
this is the 219 epoch
rmse loss on training set is 0.9498483212981834
rmse loss on test set is 0.9782707647717581
for this epoch using 107.97683811187744 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.987022365296488
  (0, 1680)	3.000000000000002
  (0, 1679)	2.930177774065101
  (0, 1678)	2.995440270396887
  (0, 1677)	2.864915277733315
  (0, 1676)	2.998831309180883
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.949398444865564
  (0, 1671)	2.940617749890893
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.959176895913451
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.959176895913451
  (0, 1664)	3.000000000000002
  (0, 1663)	2.879145361730078
  (0, 1662)	2.959176895913451
  (0, 1661)	2.948801755486309
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0177609403138588
  :	:
  (942, 24)	3.8617379312295026
  (942, 23)	3.5840701201782017
  (942, 22)	4.3216499167703954
  (942, 21)	4.343681298519743
  (942, 20)	2.84655630012798
  (942, 19)	3.582800503778411
  (942, 18)	4.20652131978731
  (942, 17)	3.0996189107227745
  (942, 16)	3.432331193572869
  (942, 15)	3.5512394494284543
  (942, 14)	3.9219955030699514
  (942, 13)	4.157827938786691
  (942, 12)	3.5122424882053638
  (942, 11)	4.483477040628743
  (942, 10)	3.909147531448776
  (942, 9)	3.8620308403116033
  (942, 8)	4.062442850172789
  (942, 7)	3.9540929686160364
  (942, 6)	3.886330614610407
  (942, 5)	3.876874092621786
  (942, 4)	3.4372178718075057
  (942, 3)	3.485962685735912
  (942, 2)	2.9631856664486924
  (942, 1)	3.1114688124710703
  (942, 0)	3.954996241142329
this is the 220 epoch
rmse loss on training set is 0.9497689848155964
rmse loss on test set is 0.9782293085469109
for this epoch using 110.19488000869751 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986970647900805
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9298652351018357
  (0, 1678)	2.995415530279015
  (0, 1677)	2.864314939924652
  (0, 1676)	2.9988412048998323
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9491411966860395
  (0, 1671)	2.940386405692274
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.959006103837908
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.959006103837908
  (0, 1664)	3.000000000000002
  (0, 1663)	2.878635379013856
  (0, 1662)	2.959006103837908
  (0, 1661)	2.948601243162813
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.017865445113022
  :	:
  (942, 24)	3.8617039914103968
  (942, 23)	3.5839575775621584
  (942, 22)	4.322229001493192
  (942, 21)	4.344089703398995
  (942, 20)	2.8458176337005012
  (942, 19)	3.583291479095258
  (942, 18)	4.207550487106946
  (942, 17)	3.098722180404278
  (942, 16)	3.432499815445202
  (942, 15)	3.5518464555748914
  (942, 14)	3.9219043645618736
  (942, 13)	4.157948133314184
  (942, 12)	3.5123331628827974
  (942, 11)	4.483831844637213
  (942, 10)	3.9093882403962565
  (942, 9)	3.8625954262958953
  (942, 8)	4.062437043605076
  (942, 7)	3.954371088273169
  (942, 6)	3.8863190202207645
  (942, 5)	3.87850449472284
  (942, 4)	3.4373608330069465
  (942, 3)	3.4862851865370907
  (942, 2)	2.962611396527966
  (942, 1)	3.1110233931746683
  (942, 0)	3.955026552967711
this is the 221 epoch
rmse loss on training set is 0.9496902982004076
rmse loss on test set is 0.9781883272747272
for this epoch using 108.53588938713074 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9869190047519365
  (0, 1680)	3.000000000000002
  (0, 1679)	2.929552739593529
  (0, 1678)	2.995390754307531
  (0, 1677)	2.8637147248795327
  (0, 1676)	2.9988511394133166
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9488841743855745
  (0, 1671)	2.9401552815675474
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.958835321144471
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.958835321144471
  (0, 1664)	3.000000000000002
  (0, 1663)	2.878125599926882
  (0, 1662)	2.958835321144471
  (0, 1661)	2.9484009234902477
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0179699058679788
  :	:
  (942, 24)	3.861669380442428
  (942, 23)	3.583843880242045
  (942, 22)	4.322803371134089
  (942, 21)	4.344494802094598
  (942, 20)	2.8450846816336233
  (942, 19)	3.583777963752509
  (942, 18)	4.208568954187062
  (942, 17)	3.097825312055942
  (942, 16)	3.432666727642419
  (942, 15)	3.5524497003219153
  (942, 14)	3.921813033771793
  (942, 13)	4.158066448631213
  (942, 12)	3.5124224577673897
  (942, 11)	4.484183911427709
  (942, 10)	3.9096264674350403
  (942, 9)	3.8631532502790624
  (942, 8)	4.062430210868163
  (942, 7)	3.9546472646327437
  (942, 6)	3.8863065269166333
  (942, 5)	3.880127563974955
  (942, 4)	3.437501848613534
  (942, 3)	3.4866058365757384
  (942, 2)	2.962040809634447
  (942, 1)	3.110579797141331
  (942, 0)	3.955055776271488
this is the 222 epoch
rmse loss on training set is 0.9496122535293047
rmse loss on test set is 0.9781478146630033
for this epoch using 110.0854983329773 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986867435431619
  (0, 1680)	3.000000000000002
  (0, 1679)	2.929240287420987
  (0, 1678)	2.9953659423970063
  (0, 1677)	2.8631146324449666
  (0, 1676)	2.9988611123508075
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9486273781083687
  (0, 1671)	2.9399243763113003
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.958664547824654
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.958664547824654
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8776160233841606
  (0, 1662)	2.958664547824654
  (0, 1661)	2.9482007957174545
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018074321765128
  :	:
  (942, 24)	3.8616341169423523
  (942, 23)	3.5837290702538556
  (942, 22)	4.323373097299533
  (942, 21)	4.34489663979665
  (942, 20)	2.8443574062369934
  (942, 19)	3.5842600076507574
  (942, 18)	4.209576846874603
  (942, 17)	3.096928328380268
  (942, 16)	3.4328319587829443
  (942, 15)	3.5530492194141123
  (942, 14)	3.9217215249865336
  (942, 13)	4.1581829274330016
  (942, 12)	3.5125103957139396
  (942, 11)	4.484533285694998
  (942, 10)	3.909862258820017
  (942, 9)	3.8637044012463093
  (942, 8)	4.062422376771174
  (942, 7)	3.954921526490772
  (942, 6)	3.8862931568876835
  (942, 5)	3.8817433502702943
  (942, 4)	3.437640950863629
  (942, 3)	3.486924654419264
  (942, 2)	2.9614738888141963
  (942, 1)	3.110138029813945
  (942, 0)	3.9550839330986634
this is the 223 epoch
rmse loss on training set is 0.9495348430078198
rmse loss on test set is 0.9781077645241779
for this epoch using 109.02238297462463 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9868159395272027
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9289278784683477
  (0, 1678)	2.9953410944646044
  (0, 1677)	2.8625146624720927
  (0, 1676)	2.998871123347912
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.948370807979962
  (0, 1671)	2.939693688730133
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9584937838711407
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9584937838711407
  (0, 1664)	3.000000000000002
  (0, 1663)	2.877106648313621
  (0, 1662)	2.9584937838711407
  (0, 1661)	2.948000859097522
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018178692006496
  :	:
  (942, 24)	3.8615982191218023
  (942, 23)	3.5836131886381524
  (942, 22)	4.32393825008244
  (942, 21)	4.345295260833698
  (942, 20)	2.843635769882164
  (942, 19)	3.584737660029052
  (942, 18)	4.2105742893570035
  (942, 17)	3.096031251603582
  (942, 16)	3.432995536941956
  (942, 15)	3.5536450480747814
  (942, 14)	3.921629852144809
  (942, 13)	4.158297611364901
  (942, 12)	3.5125969991351975
  (942, 11)	4.484880011030261
  (942, 10)	3.9100956597711
  (942, 9)	3.8642489668975655
  (942, 8)	4.062413565560856
  (942, 7)	3.955193901963351
  (942, 6)	3.886278931894366
  (942, 5)	3.883351902912979
  (942, 4)	3.437778171395855
  (942, 3)	3.4872416582943107
  (942, 2)	2.9609106170109363
  (942, 1)	3.1096980962915066
  (942, 0)	3.955111045113475
this is the 224 epoch
rmse loss on training set is 0.9494580589677296
rmse loss on test set is 0.9780681707731389
for this epoch using 108.4073257446289 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986764516631507
  (0, 1680)	3.000000000000002
  (0, 1679)	2.928615512623069
  (0, 1678)	2.9953162104300386
  (0, 1677)	2.8619148148161018
  (0, 1676)	2.998881172046253
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.948114464107809
  (0, 1671)	2.9394632176425333
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.958323029277721
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.958323029277721
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8765974736559468
  (0, 1662)	2.958323029277721
  (0, 1661)	2.9478011128877863
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018283015809457
  :	:
  (942, 24)	3.8615617047970328
  (942, 23)	3.5834962754622883
  (942, 22)	4.324498898098828
  (942, 21)	4.345690708693667
  (942, 20)	2.842919735008171
  (942, 19)	3.5852109694754657
  (942, 18)	4.211561404187078
  (942, 17)	3.095134103486015
  (942, 16)	3.43315748966239
  (942, 15)	3.5542372210156663
  (942, 14)	3.9215380288449846
  (942, 13)	4.158410541050038
  (942, 12)	3.512682290011913
  (942, 11)	4.485224129953286
  (942, 10)	3.9103267144982463
  (942, 9)	3.8647870336680326
  (942, 8)	4.062403800936636
  (942, 7)	3.955464418506137
  (942, 6)	3.886263873275281
  (942, 5)	3.884953270629037
  (942, 4)	3.4379135412631063
  (942, 3)	3.4875568660952045
  (942, 2)	2.9603509770719016
  (942, 1)	3.1092600013375775
  (942, 0)	3.9551371336049757
this is the 225 epoch
rmse loss on training set is 0.9493818938644258
rmse loss on test set is 0.9780290274251023
for this epoch using 109.94872212409973 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986713166342781
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9283031897758667
  (0, 1678)	2.9952912902155644
  (0, 1677)	2.861315089336166
  (0, 1676)	2.9988912580933165
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9478583465817403
  (0, 1671)	2.9392329618787443
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.958152284039298
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.958152284039298
  (0, 1664)	3.000000000000002
  (0, 1663)	2.876088498364448
  (0, 1662)	2.958152284039298
  (0, 1661)	2.9476015563498033
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018387292406377
  :	:
  (942, 24)	3.8615245913983256
  (942, 23)	3.5833783698421215
  (942, 22)	4.325055108523458
  (942, 21)	4.3460830260441945
  (942, 20)	2.8422092641269776
  (942, 19)	3.5856799839375078
  (942, 18)	4.212538312307551
  (942, 17)	3.0942369053312753
  (942, 16)	3.433317843965746
  (942, 15)	3.55482577244654
  (942, 14)	3.921446068352724
  (942, 13)	4.158521756116439
  (942, 12)	3.5127662899025975
  (942, 11)	4.485565683943553
  (942, 10)	3.910555466225758
  (942, 9)	3.86531868674838
  (942, 8)	4.062393106065292
  (942, 7)	3.9557331029331775
  (942, 6)	3.886248001954602
  (942, 5)	3.886547501576115
  (942, 4)	3.4380470909443357
  (942, 3)	3.4878702953921095
  (942, 2)	2.95979495175352
  (942, 1)	3.1088237493885744
  (942, 0)	3.9551622194924128
this is the 226 epoch
rmse loss on training set is 0.949306340274428
rmse loss on test set is 0.9779903285935455
for this epoch using 109.45914149284363 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986661888264591
  (0, 1680)	3.000000000000002
  (0, 1679)	2.927990909820677
  (0, 1678)	2.995266333745938
  (0, 1677)	2.860715485895416
  (0, 1676)	2.998901381142314
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.947602455474518
  (0, 1671)	2.939002920280644
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.957981548151805
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.957981548151805
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8755797214049066
  (0, 1662)	2.957981548151805
  (0, 1661)	2.9474021887493524
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0184915210443637
  :	:
  (942, 24)	3.8614868959792226
  (942, 23)	3.5832595099631877
  (942, 22)	4.325606947124577
  (942, 21)	4.346472254752307
  (942, 20)	2.8415043198286742
  (942, 19)	3.586144750732306
  (942, 18)	4.213505133075126
  (942, 17)	3.09333967799615
  (942, 16)	3.4334766263626686
  (942, 15)	3.5554107360844727
  (942, 14)	3.9213539836083773
  (942, 13)	4.1586312952232305
  (942, 12)	3.5128490199530806
  (942, 11)	4.4859047134704095
  (942, 10)	3.910781957216057
  (942, 9)	3.865844010104591
  (942, 8)	4.062381503595085
  (942, 7)	3.9559999814352382
  (942, 6)	3.886231338449275
  (942, 5)	3.8881346433530015
  (942, 4)	3.438178850356039
  (942, 3)	3.4881819634391125
  (942, 2)	2.9592425237269446
  (942, 1)	3.108389344561852
  (942, 0)	3.955186323330656
this is the 227 epoch
rmse loss on training set is 0.949231390892917
rmse loss on test set is 0.9779520684881765
for this epoch using 109.6729462146759 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9866106820057174
  (0, 1680)	3.000000000000002
  (0, 1679)	2.927678672654605
  (0, 1678)	2.9952413409483585
  (0, 1677)	2.8601160043608482
  (0, 1676)	2.9989115408520544
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.947346790842283
  (0, 1671)	2.9387730917016293
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9578108216122185
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9578108216122185
  (0, 1664)	3.000000000000002
  (0, 1663)	2.875071141755447
  (0, 1662)	2.9578108216122185
  (0, 1661)	2.9472030093564188
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018595700984933
  :	:
  (942, 24)	3.8614486352253445
  (942, 23)	3.5831397331014765
  (942, 22)	4.326154478297721
  (942, 21)	4.346858435903612
  (942, 20)	2.8408048647865054
  (942, 19)	3.5866053165565623
  (942, 18)	4.214461984284085
  (942, 17)	3.092442441899894
  (942, 16)	3.4336338628632164
  (942, 15)	3.555992145162984
  (942, 14)	3.921261787234158
  (942, 13)	4.158739196086112
  (942, 12)	3.512930500905775
  (942, 11)	4.486241258022281
  (942, 10)	3.911006228792767
  (942, 9)	3.8663630864973917
  (942, 8)	4.062369015669411
  (942, 7)	3.9562650795975194
  (942, 6)	3.886213902876156
  (942, 5)	3.889714743008921
  (942, 4)	3.4383088488634965
  (942, 3)	3.4884918871818766
  (942, 2)	2.9586936755833975
  (942, 1)	3.1079567906635797
  (942, 0)	3.955209465315562
this is the 228 epoch
rmse loss on training set is 0.9491570385313319
rmse loss on test set is 0.9779142414129797
for this epoch using 111.44714975357056 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9865595471800783
  (0, 1680)	3.000000000000002
  (0, 1679)	2.927366478177899
  (0, 1678)	2.9952163117524857
  (0, 1677)	2.85951664460331
  (0, 1676)	2.998921736886821
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.947091352725078
  (0, 1671)	2.9385434750064405
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.957640104418517
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.957640104418517
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8745627584064066
  (0, 1662)	2.957640104418517
  (0, 1661)	2.9470040174451855
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.01869983150375
  :	:
  (942, 24)	3.8614098254630527
  (942, 23)	3.5830190756436946
  (942, 22)	4.326697765098726
  (942, 21)	4.347241609820786
  (942, 20)	2.8401108617617097
  (942, 19)	3.5870617274963936
  (942, 18)	4.215408982189589
  (942, 17)	3.0915452170332687
  (942, 16)	3.433789578986974
  (942, 15)	3.556570032441004
  (942, 14)	3.921169491541266
  (942, 13)	4.158845495502078
  (942, 12)	3.513010753108679
  (942, 11)	4.486575356134929
  (942, 10)	3.911228321363218
  (942, 9)	3.866875997501391
  (942, 8)	4.062355663940064
  (942, 7)	3.9565284224167168
  (942, 6)	3.8861957149590127
  (942, 5)	3.8912878470527272
  (942, 4)	3.438437115291821
  (942, 3)	3.488800083265286
  (942, 2)	2.958148389839438
  (942, 1)	3.107526091196494
  (942, 0)	3.955231665289289
this is the 229 epoch
rmse loss on training set is 0.9490832761150341
rmse loss on test set is 0.9778768417642992
for this epoch using 110.05632781982422 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986508483406654
  (0, 1680)	3.000000000000002
  (0, 1679)	2.927054326293877
  (0, 1678)	2.995191246090356
  (0, 1677)	2.8589174064974023
  (0, 1676)	2.998931968916237
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.946836141147272
  (0, 1671)	2.9383140690710947
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.957469396569616
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.957469396569616
  (0, 1664)	3.000000000000002
  (0, 1663)	2.874054570360183
  (0, 1662)	2.957469396569616
  (0, 1661)	2.946805212294002
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0188039118903114
  :	:
  (942, 24)	3.8613704826678097
  (942, 23)	3.582897573107052
  (942, 22)	4.327236869275769
  (942, 21)	4.347621816081632
  (942, 20)	2.8394222736082115
  (942, 19)	3.587514029036941
  (942, 18)	4.216346241530472
  (942, 17)	3.090648022967508
  (942, 16)	3.4339437997729543
  (942, 15)	3.55714443021155
  (942, 14)	3.921077108536668
  (942, 13)	4.158950229373536
  (942, 12)	3.5130897965241874
  (942, 11)	4.486907045418854
  (942, 10)	3.9114482744404144
  (942, 9)	3.8673828235238847
  (942, 8)	4.062341469580001
  (942, 7)	3.9567900343177693
  (942, 6)	3.8861767940354506
  (942, 5)	3.8928540014618194
  (942, 4)	3.4385636779366906
  (942, 3)	3.4891065680407176
  (942, 2)	2.957606648941978
  (942, 1)	3.107097249367403
  (942, 0)	3.9552529427455854
this is the 230 epoch
rmse loss on training set is 0.9490100966810534
rmse loss on test set is 0.9778398640289482
for this epoch using 109.08213686943054 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986457490309376
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9267422169089246
  (0, 1678)	2.9951661438963813
  (0, 1677)	2.8583182899214705
  (0, 1676)	2.998942236615167
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.946581156118039
  (0, 1671)	2.9380848727827087
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.957298698065382
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.957298698065382
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8735465766311026
  (0, 1662)	2.957298698065382
  (0, 1661)	2.9466065931853986
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.018907941447722
  :	:
  (942, 24)	3.861330622472253
  (942, 23)	3.582775260158644
  (942, 22)	4.327771851300693
  (942, 21)	4.347999093536507
  (942, 20)	2.838739063277074
  (942, 19)	3.587962266071719
  (942, 18)	4.2172738755517365
  (942, 17)	3.0897508788630303
  (942, 16)	3.434096549789184
  (942, 15)	3.5577153703103277
  (942, 14)	3.9209846499298497
  (942, 13)	4.159053432731557
  (942, 12)	3.513167650737659
  (942, 11)	4.487236362585841
  (942, 10)	3.9116661266643598
  (942, 9)	3.8678836438233097
  (942, 8)	4.062326453295767
  (942, 7)	3.9570499391698375
  (942, 6)	3.886157159063691
  (942, 5)	3.894413251690908
  (942, 4)	3.4386885645748926
  (942, 3)	3.4894113575732293
  (942, 2)	2.9570684352732353
  (942, 1)	3.1066702680945526
  (942, 0)	3.9552733168349787
this is the 231 epoch
rmse loss on training set is 0.9489374933758167
rmse loss on test set is 0.9778033027824097
for this epoch using 105.78114652633667 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9864065675170677
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9264301499324135
  (0, 1678)	2.9951410051073006
  (0, 1677)	2.8577192947575263
  (0, 1676)	2.9989525396635646
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.946326397631815
  (0, 1671)	2.9378558850394145
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9571280089066025
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9571280089066025
  (0, 1664)	3.000000000000002
  (0, 1663)	2.873038776245303
  (0, 1662)	2.9571280089066025
  (0, 1661)	2.9464081594060567
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019011919492363
  :	:
  (942, 24)	3.861290260174122
  (942, 23)	3.5826521706344043
  (942, 22)	4.3283027703994605
  (942, 21)	4.348373480325323
  (942, 20)	2.8380611938208604
  (942, 19)	3.588406482911942
  (942, 18)	4.218191996026597
  (942, 17)	3.088853803477942
  (942, 16)	3.4342478531421867
  (942, 15)	3.558282884124096
  (942, 14)	3.9208921271393473
  (942, 13)	4.159155139758706
  (942, 12)	3.5132443349657323
  (942, 11)	4.48756334347467
  (942, 10)	3.911881915822923
  (942, 9)	3.8683785365273278
  (942, 8)	4.062310635339466
  (942, 7)	3.9573081603019933
  (942, 6)	3.8861368286292777
  (942, 5)	3.89596564268066
  (942, 4)	3.43881180247463
  (942, 3)	3.489714467648431
  (942, 2)	2.9565337311555346
  (942, 1)	3.1062451500148285
  (942, 0)	3.9552928063700312
this is the 232 epoch
rmse loss on training set is 0.9488654594530277
rmse loss on test set is 0.9777671526870503
for this epoch using 110.0097279548645 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986355714663324
  (0, 1680)	3.000000000000002
  (0, 1679)	2.926118125276678
  (0, 1678)	2.995115829662172
  (0, 1677)	2.8571204208911887
  (0, 1676)	2.9989628777463766
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9460718656686864
  (0, 1671)	2.937627104750222
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.956957329094932
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.956957329094932
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8725311682405845
  (0, 1662)	2.956957329094932
  (0, 1661)	2.9462099102467905
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019115845353681
  :	:
  (942, 24)	3.8612494107438557
  (942, 23)	3.5825283375575907
  (942, 22)	4.328829684581843
  (942, 21)	4.348745013894033
  (942, 20)	2.83738862839776
  (942, 19)	3.588846723295536
  (942, 18)	4.219100713278151
  (942, 17)	3.087956815176374
  (942, 16)	3.4343977334861946
  (942, 15)	3.5588470025988515
  (942, 14)	3.920799551299092
  (942, 13)	4.159255383810968
  (942, 12)	3.513319868064448
  (942, 11)	4.487888023076004
  (942, 10)	3.912095678872117
  (942, 9)	3.8688675786507085
  (942, 8)	4.062294035520343
  (942, 7)	3.957564720518248
  (942, 6)	3.8861158209516176
  (942, 5)	3.89751121886608
  (942, 4)	3.4389334184056226
  (942, 3)	3.490015913779316
  (942, 2)	2.956002518855925
  (942, 1)	3.1058218974907503
  (942, 0)	3.955311429830405
this is the 233 epoch
rmse loss on training set is 0.9487939882715294
rmse loss on test set is 0.97773140849038
for this epoch using 109.07707810401917 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9863049313864747
  (0, 1680)	3.000000000000002
  (0, 1679)	2.925806142856979
  (0, 1678)	2.9950906175023144
  (0, 1677)	2.856521668211644
  (0, 1676)	2.9989732505534334
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9458175601948517
  (0, 1671)	2.93739853083488
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9567866586328746
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9567866586328746
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8720237516662843
  (0, 1662)	2.9567866586328746
  (0, 1661)	2.9460118450025545
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0192197183739053
  :	:
  (942, 24)	3.8612080888320532
  (942, 23)	3.582403793156891
  (942, 22)	4.329352650670376
  (942, 21)	4.349113731010591
  (942, 20)	2.836721330275658
  (942, 19)	3.589283030396079
  (942, 18)	4.220000136200764
  (942, 17)	3.0870599319366683
  (942, 16)	3.434546214032203
  (942, 15)	3.5594077562478614
  (942, 14)	3.920706933264701
  (942, 13)	4.159354197439273
  (942, 12)	3.513394268537207
  (942, 11)	4.488210435556541
  (942, 10)	3.912307451955878
  (942, 9)	3.8693508461128
  (942, 8)	4.062276673216083
  (942, 7)	3.9578196421122818
  (942, 6)	3.8860941538905456
  (942, 5)	3.899050024184829
  (942, 4)	3.4390534386489597
  (942, 3)	3.4903157112127707
  (942, 2)	2.9554747805907295
  (942, 1)	3.105400512617378
  (942, 0)	3.955329205367986
this is the 234 epoch
rmse loss on training set is 0.9487230732932084
rmse loss on test set is 0.9776960650233745
for this epoch using 108.91882753372192 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9862542173294755
  (0, 1680)	3.000000000000002
  (0, 1679)	2.925494202591444
  (0, 1678)	2.995065368571281
  (0, 1677)	2.855923036611611
  (0, 1676)	2.998983657779332
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.94556348116301
  (0, 1671)	2.9371701622237962
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9566159975237776
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9566159975237776
  (0, 1664)	3.000000000000002
  (0, 1663)	2.871516525583136
  (0, 1662)	2.9566159975237776
  (0, 1661)	2.9458139629723985
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019323537907798
  :	:
  (942, 24)	3.861166308776665
  (942, 23)	3.582278568884106
  (942, 22)	4.329871724328483
  (942, 21)	4.349479667780541
  (942, 20)	2.8360592628359
  (942, 19)	3.589715446831488
  (942, 18)	4.220890372280975
  (942, 17)	3.0861631713592947
  (942, 16)	3.434693317556748
  (942, 15)	3.5599651751595296
  (942, 14)	3.920614283619522
  (942, 13)	4.159451612410287
  (942, 12)	3.513467554542406
  (942, 11)	4.488530614282383
  (942, 10)	3.9125172704253637
  (942, 9)	3.8698284137547123
  (942, 8)	4.062258567383643
  (942, 7)	3.958072946881573
  (942, 6)	3.886071844952687
  (942, 5)	3.90058210208526
  (942, 4)	3.439171889006731
  (942, 3)	3.4906138749359643
  (942, 2)	2.9549504985299397
  (942, 1)	3.1049809972289784
  (942, 0)	3.955346150811923
this is the 235 epoch
rmse loss on training set is 0.9486527080810547
rmse loss on test set is 0.9776611171988112
for this epoch using 109.24336051940918 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986203572139832
  (0, 1680)	3.000000000000002
  (0, 1679)	2.925182304401042
  (0, 1678)	2.995040082814839
  (0, 1677)	2.8553245259872444
  (0, 1676)	2.9989940991233404
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.945309628512762
  (0, 1671)	2.9369419978578764
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9564453457718023
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9564453457718023
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8710094890631734
  (0, 1662)	2.9564453457718023
  (0, 1661)	2.9456162634594825
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019427303322409
  :	:
  (942, 24)	3.8611240846100294
  (942, 23)	3.5821526954314273
  (942, 22)	4.330386960088026
  (942, 21)	4.349842859662065
  (942, 20)	2.835402389577075
  (942, 19)	3.590144014672591
  (942, 18)	4.221771527618118
  (942, 17)	3.085266550674693
  (942, 16)	3.4348390664106017
  (942, 15)	3.5605192890050468
  (942, 14)	3.9205216126805666
  (942, 13)	4.159547659726613
  (942, 12)	3.5135397439010077
  (942, 11)	4.488848591841682
  (942, 10)	3.9127251688576794
  (942, 9)	3.870300355356203
  (942, 8)	4.062239736569826
  (942, 7)	3.958324656141161
  (942, 6)	3.886048911297629
  (942, 5)	3.9021074955343917
  (942, 4)	3.4392887948115187
  (942, 3)	3.4909104196825473
  (942, 2)	2.9544296548014177
  (942, 1)	3.1045633529055943
  (942, 0)	3.955362283673564
this is the 236 epoch
rmse loss on training set is 0.9485828862970971
rmse loss on test set is 0.9776265600096775
for this epoch using 110.91387295722961 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9861529954695225
  (0, 1680)	3.000000000000002
  (0, 1679)	2.924870448209534
  (0, 1678)	2.9950147601809523
  (0, 1677)	2.854726136238119
  (0, 1676)	2.999004574289275
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9450560021710244
  (0, 1671)	2.9367140366884183
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9562747033818875
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9562747033818875
  (0, 1664)	3.000000000000002
  (0, 1663)	2.870502641189556
  (0, 1662)	2.9562747033818875
  (0, 1661)	2.945418745771056
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0195310139968363
  :	:
  (942, 24)	3.8610814300656284
  (942, 23)	3.582026202748384
  (942, 22)	4.330898411375994
  (942, 21)	4.350203341480689
  (942, 20)	2.8347506741185198
  (942, 19)	3.5905687754515414
  (942, 18)	4.222643706944601
  (942, 17)	3.0843700867508876
  (942, 16)	3.434983482527198
  (942, 15)	3.5610701270459604
  (942, 14)	3.9204289305042996
  (942, 13)	4.159642369646549
  (942, 12)	3.513610854103823
  (942, 11)	4.489164400066591
  (942, 10)	3.9129311810742533
  (942, 9)	3.8707667436523194
  (942, 8)	4.062220198921486
  (942, 7)	3.9585747907370004
  (942, 6)	3.8860253697442713
  (942, 5)	3.903626247025711
  (942, 4)	3.439404180935618
  (942, 3)	3.4912053599387485
  (942, 2)	2.953912231495141
  (942, 1)	3.104147580979453
  (942, 0)	3.9553776211514333
this is the 237 epoch
rmse loss on training set is 0.9485136017005701
rmse loss on test set is 0.9775923885275637
for this epoch using 109.78555011749268 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9861024869749357
  (0, 1680)	3.000000000000002
  (0, 1679)	2.924558633943445
  (0, 1678)	2.9949894006197004
  (0, 1677)	2.854127867267183
  (0, 1676)	2.9990150829854128
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.944802602052345
  (0, 1671)	2.936486277677012
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.956104070359726
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.956104070359726
  (0, 1664)	3.000000000000002
  (0, 1663)	2.869995981056504
  (0, 1662)	2.956104070359726
  (0, 1661)	2.945221409218438
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019634669321995
  :	:
  (942, 24)	3.861038358584784
  (942, 23)	3.5818991200583343
  (942, 22)	4.331406130540632
  (942, 21)	4.350561147443526
  (942, 20)	2.8341040802037636
  (942, 19)	3.5909897701700513
  (942, 18)	4.2235070136457935
  (942, 17)	3.083473796100929
  (942, 16)	3.435126587430886
  (942, 15)	3.561617718141519
  (942, 14)	3.920336246892359
  (942, 13)	4.159735771703164
  (942, 12)	3.5136809023186704
  (942, 11)	4.489478070054556
  (942, 10)	3.913135340158647
  (942, 9)	3.8712276503496676
  (942, 8)	4.062199972195436
  (942, 7)	3.9588233710588447
  (942, 6)	3.886001236776758
  (942, 5)	3.9051383985867334
  (942, 4)	3.4395180718000757
  (942, 3)	3.4914987099491817
  (942, 2)	2.953398210667115
  (942, 1)	3.103733682541177
  (942, 0)	3.955392180136118
this is the 238 epoch
rmse loss on training set is 0.9484448481459852
rmse loss on test set is 0.9775585979011981
for this epoch using 108.8334527015686 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986052046316769
  (0, 1680)	3.000000000000002
  (0, 1679)	2.924246861531989
  (0, 1678)	2.9949640040833008
  (0, 1677)	2.8535297189806816
  (0, 1676)	2.999025624924416
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.944549428059366
  (0, 1671)	2.936258719795399
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9559334467117653
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9559334467117653
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8694895077691003
  (0, 1662)	2.9559334467117653
  (0, 1661)	2.945024253116986
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019738268700352
  :	:
  (942, 24)	3.860994883323049
  (942, 23)	3.5817714758746617
  (942, 22)	4.331910168876823
  (942, 21)	4.350916311153121
  (942, 20)	2.833462571703804
  (942, 19)	3.5914070393075144
  (942, 18)	4.224361549779633
  (942, 17)	3.0825776948902193
  (942, 16)	3.4352684022450615
  (942, 15)	3.5621620907559253
  (942, 14)	3.920243571397026
  (942, 13)	4.159827894722843
  (942, 12)	3.5137499053972805
  (942, 11)	4.489789632188905
  (942, 10)	3.9133376784739538
  (942, 9)	3.8716831461424963
  (942, 8)	4.062179073768019
  (942, 7)	3.959070417052813
  (942, 6)	3.885976528550544
  (942, 5)	3.9066439917865914
  (942, 4)	3.439630491383581
  (942, 3)	3.4917904837226064
  (942, 2)	2.9528875743433827
  (942, 1)	3.103321658445957
  (942, 0)	3.9554059772150656
this is the 239 epoch
rmse loss on training set is 0.9483766195813044
rmse loss on test set is 0.9775251833548634
for this epoch using 110.2824137210846 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.986001673159973
  (0, 1680)	3.000000000000002
  (0, 1679)	2.92393513090709
  (0, 1678)	2.9949385705260605
  (0, 1677)	2.852931691288122
  (0, 1676)	2.99903619982318
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.944296480083081
  (0, 1671)	2.9360313620253824
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9557628324451652
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9557628324451652
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8689832204432295
  (0, 1662)	2.9557628324451652
  (0, 1661)	2.9448272767861323
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019841811545761
  :	:
  (942, 24)	3.8609510171564603
  (942, 23)	3.5816432980165183
  (942, 22)	4.332410576650902
  (942, 21)	4.3512688656209315
  (942, 20)	2.832826112620241
  (942, 19)	3.5918206228288843
  (942, 18)	4.225207416095895
  (942, 17)	3.0816817989436083
  (942, 16)	3.4354089477000453
  (942, 15)	3.5627032729653503
  (942, 14)	3.9201509133267143
  (942, 13)	4.159918766843449
  (942, 12)	3.513817879882164
  (942, 11)	4.490099116158836
  (942, 10)	3.9135382276797754
  (942, 9)	3.8721333007283527
  (942, 8)	4.062157520644423
  (942, 7)	3.959315948233484
  (942, 6)	3.8859512608983002
  (942, 5)	3.9081430677432514
  (942, 4)	3.4397414632310723
  (942, 3)	3.492080695037452
  (942, 2)	2.952380304523739
  (942, 1)	3.1029115093194743
  (942, 0)	3.9554190286773716
this is the 240 epoch
rmse loss on training set is 0.9483089100461619
rmse loss on test set is 0.9774921401870148
for this epoch using 112.24923300743103 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9859513671736786
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9236234420032754
  (0, 1678)	2.9949130999043323
  (0, 1677)	2.85233378410222
  (0, 1676)	2.9990468074027947
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9440437580032786
  (0, 1671)	2.9358042033586833
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9555922275678053
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9555922275678053
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8684771182054387
  (0, 1662)	2.9555922275678053
  (0, 1661)	2.944630479549308
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.019945297283198
  :	:
  (942, 24)	3.860906772687671
  (942, 23)	3.5815146136243494
  (942, 22)	4.332907403124751
  (942, 21)	4.351618843280411
  (942, 20)	2.8321946670883285
  (942, 19)	3.5922305601925566
  (942, 18)	4.2260447120551
  (942, 17)	3.080786123752421
  (942, 16)	3.435548244140845
  (942, 15)	3.563241292464901
  (942, 14)	3.9200582817511473
  (942, 13)	4.160008415531829
  (942, 12)	3.513884842013115
  (942, 11)	4.490406550978805
  (942, 10)	3.9137370187487486
  (942, 9)	3.872578182823667
  (942, 8)	4.062135329467703
  (942, 7)	3.9595599836956823
  (942, 6)	3.885925449335609
  (942, 5)	3.909635667130805
  (942, 4)	3.4398510104622915
  (942, 3)	3.4923693574472763
  (942, 2)	2.951876383185466
  (942, 1)	3.102503235563764
  (942, 0)	3.9554313505184795
this is the 241 epoch
rmse loss on training set is 0.9482417136701268
rmse loss on test set is 0.9774594637688007
for this epoch using 113.14304542541504 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985901128031106
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9233117947576877
  (0, 1678)	2.994887592176499
  (0, 1677)	2.8517359973388716
  (0, 1676)	2.999057447388428
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.943791261688816
  (0, 1671)	2.93557724279685
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9554216320882234
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9554216320882234
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8679712001927986
  (0, 1662)	2.9554216320882234
  (0, 1661)	2.9444338607339677
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0200487253485355
  :	:
  (942, 24)	3.8608621622518773
  (942, 23)	3.5813854491749235
  (942, 22)	4.333400696579353
  (942, 21)	4.351966275999761
  (942, 20)	2.8315681993798396
  (942, 19)	3.592636890357953
  (942, 18)	4.2268735358471465
  (942, 17)	3.0798906844812626
  (942, 16)	3.4356863115347407
  (942, 15)	3.563776176575374
  (942, 14)	3.9199656855066096
  (942, 13)	4.160096867600903
  (942, 12)	3.5139508077337127
  (942, 11)	4.490711965007227
  (942, 10)	3.913934081982616
  (942, 9)	3.873017860178889
  (942, 8)	4.062112516527522
  (942, 7)	3.959802542125894
  (942, 6)	3.885899109066758
  (942, 5)	3.911121830186465
  (942, 4)	3.43995915578002
  (942, 3)	3.4926564842859893
  (942, 2)	2.951375792286888
  (942, 1)	3.1020968373629056
  (942, 0)	3.9554429584448823
this is the 242 epoch
rmse loss on training set is 0.9481750246709725
rmse loss on test set is 0.9774271495427056
for this epoch using 111.15615940093994 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985850955409529
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9230001891100184
  (0, 1678)	2.9948620473029637
  (0, 1677)	2.8511383309170726
  (0, 1676)	2.9990681195092455
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9435389909979888
  (0, 1671)	2.9353504793511402
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.955251046015637
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.955251046015637
  (0, 1664)	3.000000000000002
  (0, 1663)	2.867465465552795
  (0, 1662)	2.955251046015637
  (0, 1661)	2.9442374196715586
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0201520951883793
  :	:
  (942, 24)	3.860817197922549
  (942, 23)	3.5812558304961546
  (942, 22)	4.333890504337717
  (942, 21)	4.352311195094291
  (942, 20)	2.8309466739058435
  (942, 19)	3.593039651793052
  (942, 18)	4.2276939844096475
  (942, 17)	3.078995495974696
  (942, 16)	3.4358231694786863
  (942, 15)	3.5643079522499184
  (942, 14)	3.9198731332009515
  (942, 13)	4.160184149226296
  (942, 12)	3.514015792697594
  (942, 11)	4.4910153859647455
  (942, 10)	3.9141294470280044
  (942, 9)	3.8734523995934906
  (942, 8)	4.062089097768612
  (942, 7)	3.9600436418133262
  (942, 6)	3.8858722549903124
  (942, 5)	3.912601596717507
  (942, 4)	3.440065921478254
  (942, 3)	3.492942088673014
  (942, 2)	2.9508785137708498
  (942, 1)	3.101692314688591
  (942, 0)	3.9554538678787043
this is the 243 epoch
rmse loss on training set is 0.9481088373530313
rmse loss on test set is 0.9773951930211614
for this epoch using 108.20862674713135 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9858008489901593
  (0, 1680)	3.000000000000002
  (0, 1679)	2.922688625002475
  (0, 1678)	2.994836465246074
  (0, 1677)	2.8505407847588833
  (0, 1676)	2.999078823498304
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9432869457788335
  (0, 1671)	2.9351239120424037
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.95508046935989
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.95508046935989
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8669599134432286
  (0, 1662)	2.95508046935989
  (0, 1661)	2.944041155697523
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0202554062598157
  :	:
  (942, 24)	3.8607718915170954
  (942, 23)	3.581125782781497
  (942, 22)	4.334376872787232
  (942, 21)	4.352653631338496
  (942, 20)	2.8303300552193926
  (942, 19)	3.593438882481809
  (942, 18)	4.228506153445883
  (942, 17)	3.078100572763802
  (942, 16)	3.4359588372066048
  (942, 15)	3.5648366460805216
  (942, 14)	3.919780633218562
  (942, 13)	4.160270285962463
  (942, 12)	3.514079812274578
  (942, 11)	4.491316840951835
  (942, 10)	3.9143231428917304
  (942, 9)	3.873881866930605
  (942, 8)	4.062065088799029
  (942, 7)	3.960283300660622
  (942, 6)	3.8858449017046497
  (942, 5)	3.9140750061080363
  (942, 4)	3.440171329450135
  (942, 3)	3.4932261835182405
  (942, 2)	2.95038452956809
  (942, 1)	3.101289667305585
  (942, 0)	3.955464093962258
this is the 244 epoch
rmse loss on training set is 0.948043146105543
rmse loss on test set is 0.9773635897852432
for this epoch using 108.06835198402405 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985750808458117
  (0, 1680)	3.000000000000002
  (0, 1679)	2.922377102379769
  (0, 1678)	2.994810845970129
  (0, 1677)	2.849943358789408
  (0, 1676)	2.9990895590925146
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9430351258694647
  (0, 1671)	2.934897539900988
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.954909902131482
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.954909902131482
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8664545430320563
  (0, 1662)	2.954909902131482
  (0, 1661)	2.943845068151248
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0203586580302306
  :	:
  (942, 24)	3.8607262546022985
  (942, 23)	3.580995330604121
  (942, 22)	4.334859847401453
  (942, 21)	4.352993614977757
  (942, 20)	2.8297183080180206
  (942, 19)	3.5938346199313522
  (942, 18)	4.229310137442605
  (942, 17)	3.0772059290725458
  (942, 16)	3.4360933335964514
  (942, 15)	3.5653622843043746
  (942, 14)	3.9196881937251504
  (942, 13)	4.160355302758421
  (942, 12)	3.514142881556609
  (942, 11)	4.491616356465904
  (942, 10)	3.9145151979557093
  (942, 9)	3.8743063271315226
  (942, 8)	4.062040504898134
  (942, 7)	3.9605215361942774
  (942, 6)	3.8858170635133584
  (942, 5)	3.915542097325692
  (942, 4)	3.440275401195734
  (942, 3)	3.4935087815268977
  (942, 2)	2.949893821600519
  (942, 1)	3.1008888947770585
  (942, 0)	3.9554736515625675
this is the 245 epoch
rmse loss on training set is 0.9479779454010714
rmse loss on test set is 0.97733233548337
for this epoch using 107.01547646522522 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9857008335023587
  (0, 1680)	3.000000000000002
  (0, 1679)	2.922065621189019
  (0, 1678)	2.9947851894413406
  (0, 1677)	2.8493460529366983
  (0, 1676)	2.9991003260325004
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9427835310983643
  (0, 1671)	2.934671361966606
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9547393443415153
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9547393443415153
  (0, 1664)	3.000000000000002
  (0, 1663)	2.865949353497333
  (0, 1662)	2.9547393443415153
  (0, 1661)	2.9436491563760976
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0204618499771003
  :	:
  (942, 24)	3.8606802984996853
  (942, 23)	3.5808644979306643
  (942, 22)	4.335339472761345
  (942, 21)	4.353331175739728
  (942, 20)	2.8291113971462036
  (942, 19)	3.594226901179097
  (942, 18)	4.230106029687346
  (942, 17)	3.0763115788240643
  (942, 16)	3.436226677177194
  (942, 15)	3.5658848928101134
  (942, 14)	3.919595822672499
  (942, 13)	4.160439223972979
  (942, 12)	3.5142050153636535
  (942, 11)	4.491913958417844
  (942, 10)	3.914705639991505
  (942, 9)	3.874725844229836
  (942, 8)	4.062015361024302
  (942, 7)	3.9607583655747
  (942, 6)	3.8857887544305743
  (942, 5)	3.9170029089280685
  (942, 4)	3.440378157829693
  (942, 3)	3.4937898952042565
  (942, 2)	2.9494063717843795
  (942, 1)	3.1004899964697445
  (942, 0)	3.955482555275735
this is the 246 epoch
rmse loss on training set is 0.9479132297939721
rmse loss on test set is 0.9773014258300156
for this epoch using 109.89794850349426 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9856509238155953
  (0, 1680)	3.000000000000002
  (0, 1679)	2.921754181379787
  (0, 1678)	2.9947594956278194
  (0, 1677)	2.8487488671317545
  (0, 1676)	2.999111124062572
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.942532161284689
  (0, 1671)	2.934445377288248
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.954568796001666
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.954568796001666
  (0, 1664)	3.000000000000002
  (0, 1663)	2.865444344027039
  (0, 1662)	2.954568796001666
  (0, 1661)	2.943453419719339
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.020564981587805
  :	:
  (942, 24)	3.8606340342906873
  (942, 23)	3.5807333081347523
  (942, 22)	4.335815792575987
  (942, 21)	4.353666342845502
  (942, 20)	2.8285092875976656
  (942, 19)	3.5946157627997413
  (942, 18)	4.2308939222856665
  (942, 17)	3.0754175356467686
  (942, 16)	3.436358886135604
  (942, 15)	3.5664044971439606
  (942, 14)	3.9195035278030756
  (942, 13)	4.160522073389677
  (942, 12)	3.514266228249287
  (942, 11)	4.49220967214816
  (942, 10)	3.914894496174619
  (942, 9)	3.875140481365458
  (942, 8)	4.061989671822479
  (942, 7)	3.9609938056060345
  (942, 6)	3.8857599881862765
  (942, 5)	3.918457479069243
  (942, 4)	3.4404796200886896
  (942, 3)	3.4940695368602674
  (942, 2)	2.948922162033352
  (942, 1)	3.1000929715590724
  (942, 0)	3.9554908194314677
this is the 247 epoch
rmse loss on training set is 0.947848993918838
rmse loss on test set is 0.9772708566044995
for this epoch using 107.60890007019043 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985601079094224
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9214427829039806
  (0, 1678)	2.994733764499517
  (0, 1677)	2.848151801308441
  (0, 1676)	2.9991219529306226
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.942281016238562
  (0, 1671)	2.93421958492408
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9543982571242187
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9543982571242187
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8649395138190035
  (0, 1662)	2.9543982571242187
  (0, 1661)	2.943257857532193
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0206680523594556
  :	:
  (942, 24)	3.8605874728216985
  (942, 23)	3.580601784010202
  (942, 22)	4.3362888497027585
  (942, 21)	4.353999145020332
  (942, 20)	2.8279119445176155
  (942, 19)	3.5950012409121492
  (942, 18)	4.231673906177934
  (942, 17)	3.0745238128803423
  (942, 16)	3.4364899783229546
  (942, 15)	3.5669211225156667
  (942, 14)	3.919411316654498
  (942, 13)	4.160603874231123
  (942, 12)	3.5143265345063397
  (942, 11)	4.4925035224424805
  (942, 10)	3.915081793098256
  (942, 9)	3.8755503007982477
  (942, 8)	4.061963451631484
  (942, 7)	3.961227872745599
  (942, 6)	3.8857307782314003
  (942, 5)	3.919905845505949
  (942, 4)	3.4405798083387378
  (942, 3)	3.4943477186140086
  (942, 2)	2.948441174261526
  (942, 1)	3.0996978190341347
  (942, 0)	3.955498458097267
this is the 248 epoch
rmse loss on training set is 0.9477852324890381
rmse loss on test set is 0.9772406236497574
for this epoch using 109.52384376525879 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9855512990382955
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9211314257158465
  (0, 1678)	2.9947079960282137
  (0, 1677)	2.8475548554034815
  (0, 1676)	2.9991328123880407
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9420300957613494
  (0, 1671)	2.9339939839413125
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.954227727722005
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.954227727722005
  (0, 1664)	3.000000000000002
  (0, 1663)	2.864434862080782
  (0, 1662)	2.954227727722005
  (0, 1661)	2.9430624691697553
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.02077106179867
  :	:
  (942, 24)	3.8605406247090466
  (942, 23)	3.5804699477839312
  (942, 22)	4.336758686166984
  (942, 21)	4.354329610504236
  (942, 20)	2.8273193332048567
  (942, 19)	3.5953833711860557
  (942, 18)	4.232446071155918
  (942, 17)	3.0736304235816196
  (942, 16)	3.436619971261525
  (942, 15)	3.5674347938044195
  (942, 14)	3.919319196564036
  (942, 13)	4.160684649173168
  (942, 12)	3.514385948172221
  (942, 11)	4.492795533546741
  (942, 10)	3.915267556786845
  (942, 9)	3.875955363921578
  (942, 8)	4.061936714491087
  (942, 7)	3.9614605831131517
  (942, 6)	3.885701137742963
  (942, 5)	3.9213480456038243
  (942, 4)	3.44067874258232
  (942, 3)	3.4946244523980314
  (942, 2)	2.9479633903863283
  (942, 1)	3.099304537702501
  (942, 0)	3.955505485082816
this is the 249 epoch
rmse loss on training set is 0.9477219402952404
rmse loss on test set is 0.9772107228711749
for this epoch using 108.26481914520264 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985501583351417
  (0, 1680)	3.000000000000002
  (0, 1679)	2.920820109771943
  (0, 1678)	2.994682190187519
  (0, 1677)	2.8469580293563705
  (0, 1676)	2.999143702189677
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9417793996459514
  (0, 1671)	2.9337685734161174
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.954057207808429
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.954057207808429
  (0, 1664)	3.000000000000002
  (0, 1663)	2.863930388029545
  (0, 1662)	2.954057207808429
  (0, 1661)	2.9428672539910274
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0208740094214144
  :	:
  (942, 24)	3.860493500343766
  (942, 23)	3.5803378211285604
  (942, 22)	4.337225343181172
  (942, 21)	4.3546577670621485
  (942, 20)	2.8267314191137753
  (942, 19)	3.595762188848673
  (942, 18)	4.233210505879131
  (942, 17)	3.07273738053029
  (942, 16)	3.43674888215099
  (942, 15)	3.5679455355645655
  (942, 14)	3.9192271746728564
  (942, 13)	4.160764420358493
  (942, 12)	3.5144444830342643
  (942, 11)	4.493085729181791
  (942, 10)	3.9154518127092643
  (942, 9)	3.8763557312755172
  (942, 8)	4.061909474148878
  (942, 7)	3.9616919524997396
  (942, 6)	3.8856710796290104
  (942, 5)	3.9227841163433865
  (942, 4)	3.440776442465454
  (942, 3)	3.494899749962643
  (942, 2)	2.9474887923313093
  (942, 1)	3.098913126195015
  (942, 0)	3.955511913944126
this is the 250 epoch
rmse loss on training set is 0.9476591122040247
rmse loss on test set is 0.9771811502354233
for this epoch using 108.78056478500366 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985451931740714
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9205088350310766
  (0, 1678)	2.9946563469527883
  (0, 1677)	2.8463613231093663
  (0, 1676)	2.999154622093725
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.941528927677071
  (0, 1671)	2.9335433524335204
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9538866973974005
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9538866973974005
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8634260908919633
  (0, 1662)	2.9538866973974005
  (0, 1661)	2.9426722113588717
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0209768947528253
  :	:
  (942, 24)	3.8604461098962877
  (942, 23)	3.580205425174786
  (942, 22)	4.337688861163674
  (942, 21)	4.354983641993975
  (942, 20)	2.826148167856313
  (942, 19)	3.59613772869123
  (942, 18)	4.233967297890834
  (942, 17)	3.071844696234553
  (942, 16)	3.4368767278746475
  (942, 15)	3.56845337203125
  (942, 14)	3.919135257930263
  (942, 13)	4.160843209409947
  (942, 12)	3.514502152634846
  (942, 11)	4.493374132557632
  (942, 10)	3.915634585791604
  (942, 9)	3.8767514625599246
  (942, 8)	4.061881744066965
  (942, 7)	3.9619219963764216
  (942, 6)	3.8856406165335766
  (942, 5)	3.9242140943259782
  (942, 4)	3.4408729272844836
  (942, 3)	3.4951736228799883
  (942, 2)	2.947017362028885
  (942, 1)	3.0985235829704254
  (942, 0)	3.955517757987748
this is the 251 epoch
rmse loss on training set is 0.947596743156457
rmse loss on test set is 0.9771519017693258
for this epoch using 107.38687038421631 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9854023439167707
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9201976014542836
  (0, 1678)	2.9946304663011363
  (0, 1677)	2.845764736607432
  (0, 1676)	2.9991655718616856
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9412786796314467
  (0, 1671)	2.9333183200872965
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.953716196503375
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.953716196503375
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8629219699041193
  (0, 1662)	2.953716196503375
  (0, 1661)	2.9424773406400146
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.021079717327041
  :	:
  (942, 24)	3.8603984633210295
  (942, 23)	3.58007278052343
  (942, 22)	4.338149279756933
  (942, 21)	4.355307262144256
  (942, 20)	2.8255695452037677
  (942, 19)	3.5965100250753483
  (942, 18)	4.23471653363387
  (942, 17)	3.0709523829365692
  (942, 16)	3.437003525005598
  (942, 15)	3.5689583271259533
  (942, 14)	3.9190434530978586
  (942, 13)	4.160921037443482
  (942, 12)	3.5145589702763917
  (942, 11)	4.493660766387158
  (942, 10)	3.915815900429775
  (942, 9)	3.8771426166471987
  (942, 8)	4.0618535374285
  (942, 7)	3.9621507299026484
  (942, 6)	3.885609760841464
  (942, 5)	3.925638015779631
  (942, 4)	3.440968215992852
  (942, 3)	3.4954460825481197
  (942, 2)	2.9465490814229995
  (942, 1)	3.0981359063199045
  (942, 0)	3.9555230302748488
this is the 252 epoch
rmse loss on training set is 0.947534828166783
rmse loss on test set is 0.9771229735587735
for this epoch using 105.87660074234009 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.98535281959354
  (0, 1680)	3.000000000000002
  (0, 1679)	2.919886409004802
  (0, 1678)	2.9946045482114174
  (0, 1677)	2.845168269798191
  (0, 1676)	2.999176551258296
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9410286552781497
  (0, 1671)	2.9330934754798736
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.953545705141309
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.953545705141309
  (0, 1664)	3.000000000000002
  (0, 1663)	2.86241802431136
  (0, 1662)	2.953545705141309
  (0, 1661)	2.9422826412050114
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0211824766870126
  :	:
  (942, 24)	3.860350570360854
  (942, 23)	3.5799399072572444
  (942, 22)	4.338606637845286
  (942, 21)	4.355628653911664
  (942, 20)	2.8249955170885306
  (942, 19)	3.5968791119392614
  (942, 18)	4.235458298466115
  (942, 17)	3.070060452617884
  (942, 16)	3.437129289812701
  (942, 15)	3.569460424461846
  (942, 14)	3.9189517667535427
  (942, 13)	4.160997925080777
  (942, 12)	3.5146149490263383
  (942, 11)	4.493945652899597
  (942, 10)	3.9159957805017016
  (942, 9)	3.877529251594902
  (942, 8)	4.061824867143971
  (942, 7)	3.96237816793441
  (942, 6)	3.885578524683047
  (942, 5)	3.9270559165647048
  (942, 4)	3.441062327207651
  (942, 3)	3.4957171401948544
  (942, 2)	2.946083932471681
  (942, 1)	3.097750094371498
  (942, 0)	3.9555277436253142
this is the 253 epoch
rmse loss on training set is 0.9474733623210367
rmse loss on test set is 0.9770943617476279
for this epoch using 108.27007055282593 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9853033584883244
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9195752576480256
  (0, 1678)	2.9945785926641673
  (0, 1677)	2.8445719226318773
  (0, 1676)	2.9991875600514266
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.940778854378821
  (0, 1671)	2.932868817722218
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.95337522332666
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.95337522332666
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8619142533682385
  (0, 1662)	2.95337522332666
  (0, 1661)	2.942088112428237
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0212851723843577
  :	:
  (942, 24)	3.860302440551405
  (942, 23)	3.5798068249524477
  (942, 22)	4.3390609735722565
  (942, 21)	4.35594784325821
  (942, 20)	2.8244260496057403
  (942, 19)	3.5972450228040533
  (942, 18)	4.236192676675855
  (942, 17)	3.069168917004661
  (942, 16)	3.4372540382664636
  (942, 15)	3.569959687349167
  (942, 14)	3.9188602052955215
  (942, 13)	4.161073892461435
  (942, 12)	3.5146701017218835
  (942, 11)	4.4942288138533995
  (942, 10)	3.9161742493792224
  (942, 9)	3.8779114246581248
  (942, 8)	4.0617957458573395
  (942, 7)	3.962604325032127
  (942, 6)	3.885546919938897
  (942, 5)	3.9284678321795483
  (942, 4)	3.441155279216084
  (942, 3)	3.4959868068816564
  (942, 2)	2.945621897149525
  (942, 1)	3.0973661450944605
  (942, 0)	3.9555319106217017
this is the 254 epoch
rmse loss on training set is 0.9474123407757961
rmse loss on test set is 0.9770660625366749
for this epoch using 105.5954098701477 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9852539603217005
  (0, 1680)	3.000000000000002
  (0, 1679)	2.919264147351463
  (0, 1678)	2.994552599641607
  (0, 1677)	2.8439756950613178
  (0, 1676)	2.999198598012083
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9405292766878848
  (0, 1671)	2.9326443459337654
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9532047510753685
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9532047510753685
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8614106563383794
  (0, 1662)	2.9532047510753685
  (0, 1661)	2.9418937536878813
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.021387803979175
  :	:
  (942, 24)	3.8602540832253673
  (942, 23)	3.5796735526900387
  (942, 22)	4.339512324357496
  (942, 21)	4.356264855718209
  (942, 20)	2.8238611090148544
  (942, 19)	3.597607790779667
  (942, 18)	4.236919751496758
  (942, 17)	3.0682777875728426
  (942, 16)	3.4373777860447756
  (942, 15)	3.5704561388003437
  (942, 14)	3.9187687749461073
  (942, 13)	4.161148959254952
  (942, 12)	3.514724440974675
  (942, 11)	4.494510270548875
  (942, 10)	3.9163513299397485
  (942, 9)	3.8782891923016867
  (942, 8)	4.061766185951977
  (942, 7)	3.962829215468378
  (942, 6)	3.8855149582444075
  (942, 5)	3.9298737977659264
  (942, 4)	3.44124708998173
  (942, 3)	3.496255093507283
  (942, 2)	2.945162957450132
  (942, 1)	3.0969840563034996
  (942, 0)	3.9555355436132076
this is the 255 epoch
rmse loss on training set is 0.9473517587568837
rmse loss on test set is 0.9770380721825866
for this epoch using 106.01592350006104 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985204624817463
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9189530780847206
  (0, 1678)	2.994526569127577
  (0, 1677)	2.843379587041867
  (0, 1676)	2.999209664914286
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.940279921952827
  (0, 1671)	2.9324200592422804
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9530342884038534
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9530342884038534
  (0, 1664)	3.000000000000002
  (0, 1663)	2.860907232494364
  (0, 1662)	2.9530342884038534
  (0, 1661)	2.9416995643659254
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0214903710399126
  :	:
  (942, 24)	3.8602055075166226
  (942, 23)	3.579540109066802
  (942, 22)	4.339960726913267
  (942, 21)	4.356579716407081
  (942, 20)	2.8233006617411385
  (942, 19)	3.5979674485708615
  (942, 18)	4.237639605122684
  (942, 17)	3.0673870755532002
  (942, 16)	3.437500548538546
  (942, 15)	3.5709498015351455
  (942, 14)	3.9186774817556267
  (942, 13)	4.161223144672331
  (942, 12)	3.5147779791753826
  (942, 11)	4.494790043840362
  (942, 10)	3.916527044577606
  (942, 9)	3.8786626102120576
  (942, 8)	4.061736199556542
  (942, 7)	3.9630528532353035
  (942, 6)	3.8854826509943043
  (942, 5)	3.9312738481145133
  (942, 4)	3.44133777715081
  (942, 3)	3.496522010811453
  (942, 2)	2.944707095388434
  (942, 1)	3.096603825662919
  (942, 0)	3.9555386547195606
this is the 256 epoch
rmse loss on training set is 0.9472916115581373
rmse loss on test set is 0.977010386996929
for this epoch using 105.69596529006958 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985155351702569
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9186420498194807
  (0, 1678)	2.9945005011075736
  (0, 1677)	2.8427835985313843
  (0, 1676)	2.999220760535011
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9400307899144047
  (0, 1671)	2.9321959567838025
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.952863835328977
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.952863835328977
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8604039811176647
  (0, 1662)	2.952863835328977
  (0, 1661)	2.9415055438481112
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0215928731431845
  :	:
  (942, 24)	3.8601567223642834
  (942, 23)	3.579406512206113
  (942, 22)	4.34040621726049
  (942, 21)	4.356892450029864
  (942, 20)	2.8227446743770446
  (942, 19)	3.5983240284830793
  (942, 18)	4.238352318722225
  (942, 17)	3.0664967919362467
  (942, 16)	3.4376223408572324
  (942, 15)	3.571440697985628
  (942, 14)	3.918586331605978
  (942, 13)	4.161296467477321
  (942, 12)	3.5148307284981413
  (942, 11)	4.495068154148089
  (942, 10)	3.916701415215075
  (942, 9)	3.879031733309168
  (942, 8)	4.0617057985505465
  (942, 7)	3.963275252051844
  (942, 6)	3.8854500093470525
  (942, 5)	3.932668017670092
  (942, 4)	3.441427358058145
  (942, 3)	3.49678756937834
  (942, 2)	2.9442542930029583
  (942, 1)	3.0962254506906444
  (942, 0)	3.955541255834881
this is the 257 epoch
rmse loss on training set is 0.9472318945402005
rmse loss on test set is 0.9769830033451558
for this epoch using 106.06592130661011 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9851061407070905
  (0, 1680)	3.000000000000002
  (0, 1679)	2.918331062529426
  (0, 1678)	2.9944743955686786
  (0, 1677)	2.8421877294901705
  (0, 1676)	2.999231884654185
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.93978188030686
  (0, 1671)	2.931972037702531
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9526933918680704
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9526933918680704
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8599009014985093
  (0, 1662)	2.9526933918680704
  (0, 1661)	2.94131169152394
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.021695309873638
  :	:
  (942, 24)	3.8601077365166607
  (942, 23)	3.5792727797685067
  (942, 22)	4.340848830744483
  (942, 21)	4.357203080889545
  (942, 20)	2.8221931136835705
  (942, 19)	3.5986775624281324
  (942, 18)	4.239057972453074
  (942, 17)	3.0656069474771046
  (942, 16)	3.4377431778342293
  (942, 15)	3.571928850301021
  (942, 14)	3.918495330214379
  (942, 13)	4.161368945997481
  (942, 12)	3.5148827009049026
  (942, 11)	4.495344621469651
  (942, 10)	3.916874463313237
  (942, 9)	3.8793966157579396
  (942, 8)	4.061674994569907
  (942, 7)	3.96349642537075
  (942, 6)	3.885417044229244
  (942, 5)	3.934056340536786
  (942, 4)	3.4415158497331393
  (942, 3)	3.4970517796399876
  (942, 2)	2.9438045323580284
  (942, 1)	3.095848928762229
  (942, 0)	3.9555433586313735
this is the 258 epoch
rmse loss on training set is 0.9471726031293114
rmse loss on test set is 0.9769559176456613
for this epoch using 107.50703263282776 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.985056991564156
  (0, 1680)	3.000000000000002
  (0, 1679)	2.918020116190243
  (0, 1678)	2.994448252499527
  (0, 1677)	2.8415919798809495
  (0, 1676)	2.9992430370545513
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9395331928581734
  (0, 1671)	2.93174830115072
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.952522958038895
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.952522958038895
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8593979929358033
  (0, 1662)	2.952522958038895
  (0, 1661)	2.941118006786647
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.021797680823794
  :	:
  (942, 24)	3.8600585585351013
  (942, 23)	3.579138928962005
  (942, 22)	4.341288602050181
  (942, 21)	4.357511632895148
  (942, 20)	2.8216459465914814
  (942, 19)	3.599028081929933
  (942, 18)	4.239756645476066
  (942, 17)	3.064717552700191
  (942, 16)	3.43786307403215
  (942, 15)	3.572414280352571
  (942, 14)	3.918404483136877
  (942, 13)	4.16144059813483
  (942, 12)	3.5149339081497195
  (942, 11)	4.495619465391146
  (942, 10)	3.917046209882453
  (942, 9)	3.879757310979709
  (942, 8)	4.061643799012215
  (942, 7)	3.963716386385423
  (942, 6)	3.885383766339912
  (942, 5)	3.9354388504831967
  (942, 4)	3.4416032689055913
  (942, 3)	3.4973146518797003
  (942, 2)	2.9433577955458885
  (942, 1)	3.0954742571146876
  (942, 0)	3.955544974563186
this is the 259 epoch
rmse loss on training set is 0.9471137328161459
rmse loss on test set is 0.9769291263688191
for this epoch using 108.9991295337677 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9850079040099047
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9177092107795786
  (0, 1678)	2.9944220718903343
  (0, 1677)	2.8409963496688198
  (0, 1676)	2.999254217521681
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9392847272902305
  (0, 1671)	2.9315247462885945
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9523525338596532
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9523525338596532
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8588952547370132
  (0, 1662)	2.9523525338596532
  (0, 1661)	2.9409244890331956
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0218999855938997
  :	:
  (942, 24)	3.860009196797769
  (942, 23)	3.579004976552192
  (942, 22)	4.341725565217082
  (942, 21)	4.357818129569681
  (942, 20)	2.821103140202497
  (942, 19)	3.599375618129969
  (942, 18)	4.24044841596909
  (942, 17)	3.0638286179039005
  (942, 16)	3.4379820437480273
  (942, 15)	3.572897009738195
  (942, 14)	3.9183137957718284
  (942, 13)	4.16151144137628
  (942, 12)	3.5149843617828584
  (942, 11)	4.495892705098001
  (942, 10)	3.9172166754926967
  (942, 9)	3.880113871663365
  (942, 8)	4.0616122230419975
  (942, 7)	3.9639351480365
  (942, 6)	3.8853501861547066
  (942, 5)	3.936815580947334
  (942, 4)	3.4416896320113635
  (942, 3)	3.4975761962352583
  (942, 2)	2.9429140646887992
  (942, 1)	3.0951014328502997
  (942, 0)	3.955546114870011
this is the 260 epoch
rmse loss on training set is 0.9470552791547219
rmse loss on test set is 0.9769026260360792
for this epoch using 107.9030020236969 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984958877783412
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9173983462770208
  (0, 1678)	2.9943958537328172
  (0, 1677)	2.8404008388212225
  (0, 1676)	2.999265425843872
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.939036483319059
  (0, 1671)	2.931301372284278
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.952182119348949
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.952182119348949
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8583926862180644
  (0, 1662)	2.952182119348949
  (0, 1661)	2.9407311376642413
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0220022237917923
  :	:
  (942, 24)	3.8599596595033634
  (942, 23)	3.5788709388721043
  (942, 22)	4.34215975365382
  (942, 21)	4.35812259405786
  (942, 20)	2.8205646617904083
  (942, 19)	3.599720201792801
  (942, 18)	4.241133361140737
  (942, 17)	3.062940153165083
  (942, 16)	3.4381001010183696
  (942, 15)	3.573377059787092
  (942, 14)	3.9182232733632865
  (942, 13)	4.161581492803789
  (942, 12)	3.5150340731548826
  (942, 11)	4.496164359385489
  (942, 10)	3.91738588028354
  (942, 9)	3.880466349776411
  (942, 8)	4.061580277595739
  (942, 7)	3.9641527230182483
  (942, 6)	3.8853163139300593
  (942, 5)	3.938186565041589
  (942, 4)	3.4417749551979453
  (942, 3)	3.497836422702116
  (942, 2)	2.942473321940969
  (942, 1)	3.0947304529403143
  (942, 0)	3.955546790580695
this is the 261 epoch
rmse loss on training set is 0.9469972377612151
rmse loss on test set is 0.9768764132190463
for this epoch using 111.59300136566162 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9849099126266916
  (0, 1680)	3.000000000000002
  (0, 1679)	2.917087522664041
  (0, 1678)	2.994369598020197
  (0, 1677)	2.839805447307884
  (0, 1676)	2.999276661812102
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9387884606550427
  (0, 1671)	2.9310781783136615
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.952011714525799
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.952011714525799
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8578902867032703
  (0, 1662)	2.952011714525799
  (0, 1661)	2.940537952084133
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.022104395032758
  :	:
  (942, 24)	3.8599099546746447
  (942, 23)	3.5787368318318973
  (942, 22)	4.342591200152296
  (942, 21)	4.358425049133599
  (942, 20)	2.820030478802097
  (942, 19)	3.6000618633114096
  (942, 18)	4.241811557243704
  (942, 17)	3.0620521683435213
  (942, 16)	3.4382172596241594
  (942, 15)	3.573854451564301
  (942, 14)	3.9181329210043807
  (942, 13)	4.161650769104225
  (942, 12)	3.5150830534205926
  (942, 11)	4.4964344466688475
  (942, 10)	3.9175538439739466
  (942, 9)	3.8808147965757143
  (942, 8)	4.0615479733868245
  (942, 7)	3.9643691237848615
  (942, 6)	3.885282159707253
  (942, 5)	3.939551835557525
  (942, 4)	3.4418592543299313
  (942, 3)	3.498095341136433
  (942, 2)	2.942035549490552
  (942, 1)	3.094361314228557
  (942, 0)	3.9555470125168526
this is the 262 epoch
rmse loss on training set is 0.9469396043129219
rmse loss on test set is 0.9768504845386052
for this epoch using 107.83647108078003 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9848610082845823
  (0, 1680)	3.000000000000002
  (0, 1679)	2.916776739923988
  (0, 1678)	2.9943433047471864
  (0, 1677)	2.8392101751007974
  (0, 1676)	2.9992879252200106
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9385406590030603
  (0, 1671)	2.9308551635603384
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.951841319409637
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.951841319409637
  (0, 1664)	3.000000000000002
  (0, 1663)	2.857388055525216
  (0, 1662)	2.951841319409637
  (0, 1661)	2.9403449317008983
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.022206498939382
  :	:
  (942, 24)	3.859860090162036
  (942, 23)	3.5786026709282663
  (942, 22)	4.343019936901554
  (942, 21)	4.358725517207379
  (942, 20)	2.8195005588585103
  (942, 19)	3.6004006327124585
  (942, 18)	4.242483079588075
  (942, 17)	3.0611646730862603
  (942, 16)	3.4383335330956895
  (942, 15)	3.574329205875054
  (942, 14)	3.9180427436405543
  (942, 13)	4.161719286578979
  (942, 12)	3.515131313542928
  (942, 11)	4.49670298499325
  (942, 10)	3.91772058587187
  (942, 9)	3.881159262618201
  (942, 8)	4.061515320910298
  (942, 7)	3.9645843625564443
  (942, 6)	3.885247733316416
  (942, 5)	3.940911424970656
  (942, 4)	3.4419425449943692
  (942, 3)	3.4983529612581763
  (942, 2)	2.9416007295614883
  (942, 1)	3.0939940134349904
  (942, 0)	3.955546791296366
this is the 263 epoch
rmse loss on training set is 0.9468823745471789
rmse loss on test set is 0.9768248366640638
for this epoch using 108.12479877471924 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9848121645047407
  (0, 1680)	3.000000000000002
  (0, 1679)	2.916465998042052
  (0, 1678)	2.994316973909928
  (0, 1677)	2.8386150221741797
  (0, 1676)	2.999299215863786
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9382930780627627
  (0, 1671)	2.9306323272155264
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9516709340202656
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9516709340202656
  (0, 1664)	3.000000000000002
  (0, 1663)	2.856885992024649
  (0, 1662)	2.9516709340202656
  (0, 1661)	2.940152075926194
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0223085351414265
  :	:
  (942, 24)	3.859810073646995
  (942, 23)	3.5784684712537094
  (942, 22)	4.343445995501256
  (942, 21)	4.359024020333453
  (942, 20)	2.8189748697555745
  (942, 19)	3.6007365396614923
  (942, 18)	4.243148002554277
  (942, 17)	3.0602776768318627
  (942, 16)	3.438448934717386
  (942, 15)	3.5748013432691788
  (942, 14)	3.917952746072734
  (942, 13)	4.161787061153315
  (942, 12)	3.5151788642967037
  (942, 11)	4.496969992043326
  (942, 10)	3.9178861248834846
  (942, 9)	3.8814997977713026
  (942, 8)	4.061482330447558
  (942, 7)	3.964798451324913
  (942, 6)	3.885213044380475
  (942, 5)	3.9422653654450146
  (942, 4)	3.442024842505997
  (942, 3)	3.4986092926539554
  (942, 2)	2.9411688444153077
  (942, 1)	3.093628547159118
  (942, 0)	3.955546137336842
this is the 264 epoch
rmse loss on training set is 0.9468255442603011
rmse loss on test set is 0.9767994663122748
for this epoch using 105.40650796890259 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9847633810375913
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9161552970052247
  (0, 1678)	2.994290605506022
  (0, 1677)	2.838019988504435
  (0, 1676)	2.9993105335421713
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9380457175286683
  (0, 1671)	2.930409668477935
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9515005583778735
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9515005583778735
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8563840955504216
  (0, 1662)	2.9515005583778735
  (0, 1661)	2.939959384175326
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.022410503275697
  :	:
  (942, 24)	3.8597599126454214
  (942, 23)	3.5783342475055337
  (942, 22)	4.343869406974892
  (942, 21)	4.359320580216749
  (942, 20)	2.818453379465025
  (942, 19)	3.6010696134680225
  (942, 18)	4.243806399605948
  (942, 17)	3.0593911888145597
  (942, 16)	3.438563477532413
  (942, 15)	3.575270884045351
  (942, 14)	3.9178629329605026
  (942, 13)	4.161854108385519
  (942, 12)	3.5152257162723473
  (942, 11)	4.497235485152532
  (942, 10)	3.918050479522357
  (942, 9)	3.8818364512232364
  (942, 8)	4.061449012070889
  (942, 7)	3.9650114018596985
  (942, 6)	3.885178102319027
  (942, 5)	3.943613688837798
  (942, 4)	3.44210616191236
  (942, 3)	3.498864344779968
  (942, 2)	2.940739876352903
  (942, 1)	3.093264911883457
  (942, 0)	3.9555450608590443
this is the 265 epoch
rmse loss on training set is 0.9467691093066011
rmse loss on test set is 0.9767743702468548
for this epoch using 106.38191723823547 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9847146576362618
  (0, 1680)	3.000000000000002
  (0, 1679)	2.915844636802279
  (0, 1678)	2.994264199534457
  (0, 1677)	2.8374250740701052
  (0, 1676)	2.9993218780563975
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9377985770904154
  (0, 1671)	2.9301871865537286
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9513301925030353
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9513301925030353
  (0, 1664)	3.000000000000002
  (0, 1663)	2.855882365459352
  (0, 1662)	2.9513301925030353
  (0, 1661)	2.93976685586721
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0225124029859205
  :	:
  (942, 24)	3.8597096145109253
  (942, 23)	3.578200013994743
  (942, 22)	4.344290201782558
  (942, 21)	4.359615218219807
  (942, 20)	2.8179360561351987
  (942, 19)	3.6013998830905156
  (942, 18)	4.24445834330251
  (942, 17)	3.058505218068358
  (942, 16)	3.438677174347315
  (942, 15)	3.5757378482552435
  (942, 14)	3.917773308825145
  (942, 13)	4.16192044347572
  (942, 12)	3.515271879879506
  (942, 11)	4.497499481312152
  (942, 10)	3.9182136679182857
  (942, 9)	3.882169271493141
  (942, 8)	4.061415375647926
  (942, 7)	3.9652232257133107
  (942, 6)	3.885142916352114
  (942, 5)	3.9449564267038064
  (942, 4)	3.442186517998856
  (942, 3)	3.4991181269647047
  (942, 2)	2.940313807716207
  (942, 1)	3.0929031039767474
  (942, 0)	3.9555435718902565
this is the 266 epoch
rmse loss on training set is 0.946713065597325
rmse loss on test set is 0.9767495452773404
for this epoch using 106.56735849380493 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9846659940565563
  (0, 1680)	3.000000000000002
  (0, 1679)	2.915534017423743
  (0, 1678)	2.9942377559956252
  (0, 1677)	2.8368302788518633
  (0, 1676)	2.999333249210112
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9375516564329014
  (0, 1671)	2.929964880656401
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9511598364166707
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9511598364166707
  (0, 1664)	3.000000000000002
  (0, 1663)	2.855380801116187
  (0, 1662)	2.9511598364166707
  (0, 1661)	2.93957449042437
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0226142339225897
  :	:
  (942, 24)	3.859659186438064
  (942, 23)	3.578065784654648
  (942, 22)	4.3447084098335615
  (942, 21)	4.359907955369361
  (942, 20)	2.817422868091738
  (942, 19)	3.6017273771413505
  (942, 18)	4.245103905311607
  (942, 17)	3.057619773430998
  (942, 16)	3.438790037736482
  (942, 15)	3.576202255707605
  (942, 14)	3.9176838780526455
  (942, 13)	4.1619860812746
  (942, 12)	3.515317365350556
  (942, 11)	4.497761997180109
  (942, 10)	3.918375707825966
  (942, 9)	3.8824983064410508
  (942, 8)	4.06138143084589
  (942, 7)	3.9654339342266796
  (942, 6)	3.885107495503991
  (942, 5)	3.9462936102998767
  (942, 4)	3.44226592529367
  (942, 3)	3.499370648411721
  (942, 2)	2.939890620889862
  (942, 1)	3.092543119697273
  (942, 0)	3.955541680267589
this is the 267 epoch
rmse loss on training set is 0.9466574090997291
rmse loss on test set is 0.9767249882583874
for this epoch using 106.31692934036255 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984617390056887
  (0, 1680)	3.000000000000002
  (0, 1679)	2.915223438861856
  (0, 1678)	2.9942112748912715
  (0, 1677)	2.836235602832437
  (0, 1676)	2.9993446468093707
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.937304955236468
  (0, 1671)	2.929742750006725
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9509894901400697
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9509894901400697
  (0, 1664)	3.000000000000002
  (0, 1663)	2.854879401893456
  (0, 1662)	2.9509894901400697
  (0, 1661)	2.939382287272905
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0227159957428875
  :	:
  (942, 24)	3.859608635465418
  (942, 23)	3.577931573049286
  (942, 22)	4.345124060498564
  (942, 21)	4.360198812362893
  (942, 20)	2.8169137838382756
  (942, 19)	3.602052123891641
  (942, 18)	4.245743156421305
  (942, 17)	3.0567348635478955
  (942, 16)	3.4389020800465357
  (942, 15)	3.576664125972328
  (942, 14)	3.91759464489661
  (942, 13)	4.162051036291748
  (942, 12)	3.5153621827440436
  (942, 11)	4.498023049089445
  (942, 10)	3.9185366166334625
  (942, 9)	3.882823603277631
  (942, 8)	4.061347187135883
  (942, 7)	3.965643538534368
  (942, 6)	3.8850718486067413
  (942, 5)	3.9476252705891377
  (942, 4)	3.4423443980725894
  (942, 3)	3.499621918202254
  (942, 2)	2.9394702983027483
  (942, 1)	3.0921849551959593
  (942, 0)	3.9555393956412344
this is the 268 epoch
rmse loss on training set is 0.9466021358361102
rmse loss on test set is 0.9767006960890338
for this epoch using 104.45051550865173 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9845688453982615
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9149129011105503
  (0, 1678)	2.994184756224483
  (0, 1677)	2.835641045996621
  (0, 1676)	2.999356070662558
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.93705847317706
  (0, 1671)	2.9295207938326255
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.950819153694862
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.950819153694862
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8543781671714026
  (0, 1662)	2.950819153694862
  (0, 1661)	2.9391902458424664
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0228176881105226
  :	:
  (942, 24)	3.8595579684787724
  (942, 23)	3.5777973923817705
  (942, 22)	4.345537182621585
  (942, 21)	4.360487809574965
  (942, 20)	2.8164087720570294
  (942, 19)	3.6023741512760012
  (942, 18)	4.246376166552185
  (942, 17)	3.055850496875954
  (942, 16)	3.4390133134006677
  (942, 15)	3.5771234783843426
  (942, 14)	3.9175056134812047
  (942, 13)	4.162115322703924
  (942, 12)	3.5154063419481147
  (942, 11)	4.498282653056634
  (942, 10)	3.9186964113704366
  (942, 9)	3.883145208573891
  (942, 8)	4.061312653796896
  (942, 7)	3.9658520495697185
  (942, 6)	3.8850359843039857
  (942, 5)	3.948951438245317
  (942, 4)	3.4424219503637437
  (942, 3)	3.4998719452977896
  (942, 2)	2.9390528224295727
  (942, 1)	3.0918286065194884
  (942, 0)	3.955536727477703
this is the 269 epoch
rmse loss on training set is 0.9465472418828337
rmse loss on test set is 0.976676665711898
for this epoch using 106.23665308952332 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984520359844221
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9146024041654357
  (0, 1678)	2.9941581999996854
  (0, 1677)	2.8350466083311896
  (0, 1676)	2.999367520580372
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9368122099263934
  (0, 1671)	2.929299011369137
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9506488271030293
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9506488271030293
  (0, 1664)	3.000000000000002
  (0, 1663)	2.853877096337915
  (0, 1662)	2.9506488271030293
  (0, 1661)	2.9389983655662753
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.022919310695626
  :	:
  (942, 24)	3.8595071922140454
  (942, 23)	3.5776632555023062
  (942, 22)	4.345947804531589
  (942, 21)	4.360774967063435
  (942, 20)	2.8159078016093626
  (942, 19)	3.602693486897235
  (942, 18)	4.247003004769077
  (942, 17)	3.054966681687297
  (942, 16)	3.439123749702834
  (942, 15)	3.5775803320474755
  (942, 14)	3.9174167878038832
  (942, 13)	4.162178954362996
  (942, 12)	3.515449852683748
  (942, 11)	4.498540824789604
  (942, 10)	3.91885510871619
  (942, 9)	3.883463168270573
  (942, 8)	4.061277839919876
  (942, 7)	3.966059478069721
  (942, 6)	3.884999911054289
  (942, 5)	3.950272143656886
  (942, 4)	3.4424985959522054
  (942, 3)	3.5001207385426065
  (942, 2)	2.938638175792319
  (942, 1)	3.0914740696133407
  (942, 0)	3.955533685062963
this is the 270 epoch
rmse loss on training set is 0.9464927233694452
rmse loss on test set is 0.9766528941124465
for this epoch using 105.44486594200134 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984471933160781
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9142919480237524
  (0, 1678)	2.9941316062225884
  (0, 1677)	2.834452289824923
  (0, 1676)	2.9993789963757527
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.936566165152123
  (0, 1671)	2.929077401858304
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.950478510386871
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.950478510386871
  (0, 1664)	3.000000000000002
  (0, 1663)	2.853376188788403
  (0, 1662)	2.950478510386871
  (0, 1661)	2.938806645881059
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0230208631746454
  :	:
  (942, 24)	3.859456313260279
  (942, 23)	3.5775291749161444
  (942, 22)	4.3463559540538546
  (942, 21)	4.3610603045755205
  (942, 20)	2.81541084153627
  (942, 19)	3.6030101580309117
  (942, 18)	4.247623739292836
  (942, 17)	3.054083426072968
  (942, 16)	3.4392334006419016
  (942, 15)	3.578034705838268
  (942, 14)	3.917328171738198
  (942, 13)	4.162241944803747
  (942, 12)	3.5154927245079963
  (942, 11)	4.498797579695532
  (942, 10)	3.91901272500757
  (942, 9)	3.8837775276875375
  (942, 8)	4.061242754411593
  (942, 7)	3.9662658345798363
  (942, 6)	3.884963637134775
  (942, 5)	3.9515874169311527
  (942, 4)	3.4425743483846145
  (942, 3)	3.5003683066662177
  (942, 2)	2.9382263409616804
  (942, 1)	3.0911213403247
  (942, 0)	3.955530277505613
this is the 271 epoch
rmse loss on training set is 0.946438576477763
rmse loss on test set is 0.9766293783182713
for this epoch using 107.79903483390808 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9844235651164346
  (0, 1680)	3.000000000000002
  (0, 1679)	2.913981532684347
  (0, 1678)	2.994104974900186
  (0, 1677)	2.833858090468507
  (0, 1676)	2.999390497863869
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9363203385179832
  (0, 1671)	2.92885596454908
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9503082035690227
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9503082035690227
  (0, 1664)	3.000000000000002
  (0, 1663)	2.852875443925725
  (0, 1662)	2.9503082035690227
  (0, 1661)	2.938615086227065
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.023122345230212
  :	:
  (942, 24)	3.8594053380624773
  (942, 23)	3.577395162791313
  (942, 22)	4.346761658521064
  (942, 21)	4.361343841553694
  (942, 20)	2.8149178610588583
  (942, 19)	3.6033241916299477
  (942, 18)	4.24823843751176
  (942, 17)	3.053200737946527
  (942, 16)	3.4393422776957276
  (942, 15)	3.578486618409665
  (942, 14)	3.917239769036512
  (942, 13)	4.162304307251443
  (942, 12)	3.5155349668171594
  (942, 11)	4.4990529328884294
  (942, 10)	3.919169276246627
  (942, 9)	3.8840883315329084
  (942, 8)	4.061207405998422
  (942, 7)	3.966471129458626
  (942, 6)	3.884927170644443
  (942, 5)	3.95289728789827
  (942, 4)	3.442649220973543
  (942, 3)	3.500614658285705
  (942, 2)	2.9378173005584447
  (942, 1)	3.090770414405419
  (942, 0)	3.955526513739854
this is the 272 epoch
rmse loss on training set is 0.9463847974409793
rmse loss on test set is 0.9766061153983716
for this epoch using 106.65175318717957 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984375255482075
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9136711581476478
  (0, 1678)	2.994078306040741
  (0, 1677)	2.833264010254564
  (0, 1676)	2.9994020248620417
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9360747296839294
  (0, 1671)	2.9286346986972793
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9501379066724303
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9501379066724303
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8523748611601154
  (0, 1662)	2.9501379066724303
  (0, 1661)	2.93842368604804
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.023223756551063
  :	:
  (942, 24)	3.8593542729244743
  (942, 23)	3.57726123096615
  (942, 22)	4.347164944784109
  (942, 21)	4.3616255971415265
  (942, 20)	2.814428829578725
  (942, 19)	3.6036356143290136
  (942, 18)	4.248847165992918
  (942, 17)	3.052318625047536
  (942, 16)	3.439450392135085
  (942, 15)	3.578936088194635
  (942, 14)	3.917151583332651
  (942, 13)	4.16236605462925
  (942, 12)	3.515576588849861
  (942, 11)	4.499306899196486
  (942, 10)	3.919324778108084
  (942, 9)	3.884395623912079
  (942, 8)	4.06117180323007
  (942, 7)	3.966675372882306
  (942, 6)	3.8848905195076218
  (942, 5)	3.954201786115186
  (942, 4)	3.4427232268019043
  (942, 3)	3.500859801908128
  (942, 2)	2.9374110372548294
  (942, 1)	3.090421287514762
  (942, 0)	3.955522402528616
this is the 273 epoch
rmse loss on training set is 0.9463313825428212
rmse loss on test set is 0.9765831024624452
for this epoch using 105.94611191749573 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.984327004030966
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9133608244156455
  (0, 1678)	2.994051599653736
  (0, 1677)	2.8326700491775565
  (0, 1676)	2.9994135771897428
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9358293383063168
  (0, 1671)	2.9284136035654833
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.949967619720371
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.949967619720371
  (0, 1664)	3.000000000000002
  (0, 1663)	2.851874439909064
  (0, 1662)	2.949967619720371
  (0, 1661)	2.9382324447911885
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0233250968318766
  :	:
  (942, 24)	3.8593031240116598
  (942, 23)	3.5771273909567483
  (942, 22)	4.347565839222599
  (942, 21)	4.361905590189257
  (942, 20)	2.813943716678313
  (942, 19)	3.603944452448917
  (942, 18)	4.249449990493275
  (942, 17)	3.051437094945044
  (942, 16)	3.439557755027602
  (942, 15)	3.5793831334097956
  (942, 14)	3.9170636181444074
  (942, 13)	4.162427199565367
  (942, 12)	3.515617599689997
  (942, 11)	4.499559493169248
  (942, 10)	3.9194792459466834
  (942, 9)	3.8846994483366273
  (942, 8)	4.06113595448319
  (942, 7)	3.9668785748491038
  (942, 6)	3.88485369147718
  (942, 5)	3.955500940869544
  (942, 4)	3.44279637872724
  (942, 3)	3.501103745932716
  (942, 2)	2.937007533775756
  (942, 1)	3.0900739552222345
  (942, 0)	3.9555179524664643
this is the 274 epoch
rmse loss on training set is 0.9462783281166924
rmse loss on test set is 0.9765603366602167
for this epoch using 106.55871081352234 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9842788105386933
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9130505314918613
  (0, 1678)	2.994024855749894
  (0, 1677)	2.832076207233824
  (0, 1676)	2.9994251546685318
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9355841640380134
  (0, 1671)	2.9281926784229677
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.949797342736395
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.949797342736395
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8513741795972734
  (0, 1662)	2.949797342736395
  (0, 1661)	2.9380413619071826
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0234263657732345
  :	:
  (942, 24)	3.859251897353727
  (942, 23)	3.5769936539641995
  (942, 22)	4.347964367755218
  (942, 21)	4.362183839259392
  (942, 20)	2.8134624921212215
  (942, 19)	3.60425073200097
  (942, 18)	4.250046975970651
  (942, 17)	3.0505561550409497
  (942, 16)	3.4396643772415754
  (942, 15)	3.579827772058875
  (942, 14)	3.9169758768762
  (942, 13)	4.162487754400129
  (942, 12)	3.515658008269806
  (942, 11)	4.499810729084534
  (942, 10)	3.9196326948042857
  (942, 9)	3.8849998477329852
  (942, 8)	4.061099867964881
  (942, 7)	3.967080745183592
  (942, 6)	3.8848166941378492
  (942, 5)	3.956794781183464
  (942, 4)	3.4428686893858598
  (942, 3)	3.501346498653148
  (942, 2)	2.936606772900102
  (942, 1)	3.0897284130102505
  (942, 0)	3.955513171982533
this is the 275 epoch
rmse loss on training set is 0.9462256305448136
rmse loss on test set is 0.9765378151807566
for this epoch using 106.23649048805237 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9842306747831575
  (0, 1680)	3.000000000000002
  (0, 1679)	2.912740279381309
  (0, 1678)	2.9939980743411305
  (0, 1677)	2.8314824844214876
  (0, 1676)	2.999436757122004
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9353392065285537
  (0, 1671)	2.9279719225455985
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9496270757443916
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9496270757443916
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8508740796565424
  (0, 1662)	2.9496270757443916
  (0, 1661)	2.9378504368501317
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0235275630814478
  :	:
  (942, 24)	3.8592005988472735
  (942, 23)	3.5768600308816185
  (942, 22)	4.348360555849729
  (942, 21)	4.362460362632041
  (942, 20)	2.8129851258524776
  (942, 19)	3.6045544786911523
  (942, 18)	4.250638186594527
  (942, 17)	3.049675812573292
  (942, 16)	3.439770269449684
  (942, 15)	3.580270021936173
  (942, 14)	3.9168883628214743
  (942, 13)	4.162547731192732
  (942, 12)	3.5156978233726215
  (942, 11)	4.500060620955177
  (942, 10)	3.9197851394168874
  (942, 9)	3.8852968644510715
  (942, 8)	4.061063551716132
  (942, 7)	3.9672818935407417
  (942, 6)	3.8847795349093546
  (942, 5)	3.9580833358172636
  (942, 4)	3.442940171197017
  (942, 3)	3.501588068259642
  (942, 2)	2.936208737461882
  (942, 1)	3.089384656276772
  (942, 0)	3.9555080693434563
this is the 276 epoch
rmse loss on training set is 0.9461732862574732
rmse loss on test set is 0.9765155352518192
for this epoch using 105.21014094352722 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9841825965444992
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9124300680904933
  (0, 1678)	2.9939712554405267
  (0, 1677)	2.830888880740456
  (0, 1676)	2.999448384375802
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9350944654242612
  (0, 1671)	2.9277513352157962
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.949456818768503
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.949456818768503
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8503741395256985
  (0, 1662)	2.949456818768503
  (0, 1661)	2.937659669077569
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.023628688468512
  :	:
  (942, 24)	3.8591492342584024
  (942, 23)	3.576726532301145
  (942, 22)	4.348754428532791
  (942, 21)	4.3627351783102695
  (942, 20)	2.81251158799874
  (942, 19)	3.604855717924344
  (942, 18)	4.251223685756662
  (942, 17)	3.0487960746195215
  (942, 16)	3.4398754421326907
  (942, 15)	3.5807099006299254
  (942, 14)	3.916801079165212
  (942, 13)	4.162607141727985
  (942, 12)	3.5157370536357906
  (942, 11)	4.500309182535596
  (942, 10)	3.9199365942214124
  (942, 9)	3.8855905402727315
  (942, 8)	4.0610270136152105
  (942, 7)	3.9674820294100686
  (942, 6)	3.8847422210495806
  (942, 5)	3.9593666332732087
  (942, 4)	3.4430108363668683
  (942, 3)	3.5018284628411007
  (942, 2)	2.9358134103514293
  (942, 1)	3.0890426803379185
  (942, 0)	3.9555026526561403
this is the 277 epoch
rmse loss on training set is 0.9461212917321519
rmse loss on test set is 0.9764934941392114
for this epoch using 105.9957504272461 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9841345756050837
  (0, 1680)	3.000000000000002
  (0, 1679)	2.912119897627365
  (0, 1678)	2.9939443990623533
  (0, 1677)	2.8302953961923873
  (0, 1676)	2.9994600362575228
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9348499403683967
  (0, 1671)	2.927530915722428
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.949286571833178
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.949286571833178
  (0, 1664)	3.000000000000002
  (0, 1663)	2.849874358650511
  (0, 1662)	2.949286571833178
  (0, 1661)	2.9374690580504272
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0237297416519766
  :	:
  (942, 24)	3.859097809225266
  (942, 23)	3.576593168520676
  (942, 22)	4.349146010399511
  (942, 21)	4.363008304025159
  (942, 20)	2.8120418488684944
  (942, 19)	3.6051544748083804
  (942, 18)	4.251803536081568
  (942, 17)	3.0479169480996697
  (942, 16)	3.439979905583007
  (942, 15)	3.5811474255255833
  (942, 14)	3.9167140289862177
  (942, 13)	4.162665997522751
  (942, 12)	3.515775707553353
  (942, 11)	4.500556427328166
  (942, 10)	3.9200870733623505
  (942, 9)	3.8858809164200356
  (942, 8)	4.060990261380894
  (942, 7)	3.967681162119491
  (942, 6)	3.884704759657648
  (942, 5)	3.9606447017990263
  (942, 4)	3.443080696892477
  (942, 3)	3.5020676903871446
  (942, 2)	2.935420774516471
  (942, 1)	3.0887024804304772
  (942, 0)	3.955496929870604
this is the 278 epoch
rmse loss on training set is 0.9460696434928253
rmse loss on test set is 0.9764716891461445
for this epoch using 107.30157017707825 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9840866117494476
  (0, 1680)	3.000000000000002
  (0, 1679)	2.911809768001317
  (0, 1678)	2.9939175052219933
  (0, 1677)	2.8297020307806418
  (0, 1676)	2.9994717125967147
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9346056310012743
  (0, 1671)	2.927310663360753
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9491163349631537
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9491163349631537
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8493747364836057
  (0, 1662)	2.9491163349631537
  (0, 1661)	2.937278603233021
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0238307223548517
  :	:
  (942, 24)	3.8590463292605945
  (942, 23)	3.5764599495504936
  (942, 22)	4.349535325622805
  (942, 21)	4.3632797572409165
  (942, 20)	2.8115758789521546
  (942, 19)	3.605450774158126
  (942, 18)	4.252377799436811
  (942, 17)	3.047038439779411
  (942, 16)	3.4400836699082453
  (942, 15)	3.5815826138091262
  (942, 14)	3.916627215259555
  (942, 13)	4.1627243098322655
  (942, 12)	3.5158137934788063
  (942, 11)	4.5008023685893805
  (942, 10)	3.9202365906982273
  (942, 9)	3.8861680335634707
  (942, 8)	4.060953302575722
  (942, 7)	3.9678793008391233
  (942, 6)	3.8846671576768843
  (942, 5)	3.9619175693915474
  (942, 4)	3.4431497645656512
  (942, 3)	3.5023057587901145
  (942, 2)	2.9350308129632166
  (942, 1)	3.0883640517143607
  (942, 0)	3.9554909087826897
this is the 279 epoch
rmse loss on training set is 0.946018338109123
rmse loss on test set is 0.9764501176126272
for this epoch using 105.60029745101929 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9840387047642896
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9114996792231302
  (0, 1678)	2.993890573935974
  (0, 1677)	2.829108784510289
  (0, 1676)	2.9994834132248465
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.934361536960378
  (0, 1671)	2.927090577432316
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9489461081834287
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9489461081834287
  (0, 1664)	3.000000000000002
  (0, 1663)	2.848875272484392
  (0, 1662)	2.9489461081834287
  (0, 1661)	2.937088304093029
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0239316303055044
  :	:
  (942, 24)	3.858994799754044
  (942, 23)	3.5763268851197947
  (942, 22)	4.349922397962499
  (942, 21)	4.363549555159812
  (942, 20)	2.8111136489221997
  (942, 19)	3.6057446404994473
  (942, 18)	4.252946536943152
  (942, 17)	3.0461605562731826
  (942, 16)	3.4401867450346586
  (942, 15)	3.582015482470175
  (942, 14)	3.9165406408587407
  (942, 13)	4.162782089656341
  (942, 12)	3.515851319627705
  (942, 11)	4.501047019335852
  (942, 10)	3.9203851598079975
  (942, 9)	3.8864519318299933
  (942, 8)	4.060916144609067
  (942, 7)	3.9680764545850553
  (942, 6)	3.8846294218978725
  (942, 5)	3.9631852638000855
  (942, 4)	3.443218050976717
  (942, 3)	3.5025426758470215
  (942, 2)	2.9346435087573846
  (942, 1)	3.0880273892750485
  (942, 0)	3.9554845970368278
this is the 280 epoch
rmse loss on training set is 0.9459673721956212
rmse loss on test set is 0.9764287769148423
for this epoch using 106.14317560195923 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983990854438405
  (0, 1680)	3.000000000000002
  (0, 1679)	2.911189631304973
  (0, 1678)	2.993863605221913
  (0, 1677)	2.828515657388036
  (0, 1676)	2.99949513797524
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.934117657880503
  (0, 1671)	2.9268706572449226
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.948775891519267
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.948775891519267
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8483759661189842
  (0, 1662)	2.948775891519267
  (0, 1661)	2.9368981601015034
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.024032465237593
  :	:
  (942, 24)	3.8589432259746634
  (942, 23)	3.5761939846829613
  (942, 22)	4.350307250774233
  (942, 21)	4.363817714726975
  (942, 20)	2.810655129633231
  (942, 19)	3.60603609807309
  (942, 18)	4.253509808984607
  (942, 17)	3.045283304047113
  (942, 16)	3.4402891407105503
  (942, 15)	3.5824460483051763
  (942, 14)	3.9164543085581136
  (942, 13)	4.16283934774533
  (942, 12)	3.5158882940802494
  (942, 11)	4.501290392350229
  (942, 10)	3.9205327939971832
  (942, 9)	3.8867326508109437
  (942, 8)	4.060878794740236
  (942, 7)	3.968272632222898
  (942, 6)	3.8845915589613202
  (942, 5)	3.9644478125299796
  (942, 4)	3.4432855675183003
  (942, 3)	3.5027784492614518
  (942, 2)	2.9342588450252136
  (942, 1)	3.0876924881259256
  (942, 0)	3.955478002128615
this is the 281 epoch
rmse loss on training set is 0.9459167424111115
rmse loss on test set is 0.9764076644645867
for this epoch using 109.99514985084534 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983943060562673
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9108796242603705
  (0, 1678)	2.993836599098526
  (0, 1677)	2.8279226494222147
  (0, 1676)	2.9995068866830827
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9338739933938656
  (0, 1671)	2.926650902112526
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9486056849962043
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9486056849962043
  (0, 1664)	3.000000000000002
  (0, 1663)	2.847876816860118
  (0, 1662)	2.9486056849962043
  (0, 1661)	2.936708170732794
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0241332268899463
  :	:
  (942, 24)	3.858891613073184
  (942, 23)	3.576061257425821
  (942, 22)	4.350689907018129
  (942, 21)	4.364084252635079
  (942, 20)	2.8102002921219356
  (942, 19)	3.6063251708385704
  (942, 18)	4.2540676752182085
  (942, 17)	3.044406689421941
  (942, 16)	3.4403908665095413
  (942, 15)	3.582874327920399
  (942, 14)	3.916368221034871
  (942, 13)	4.162896094606029
  (942, 12)	3.5159247247838157
  (942, 11)	4.501532500186749
  (942, 10)	3.920679506303927
  (942, 9)	3.887010229569832
  (942, 8)	4.060841260081372
  (942, 7)	3.968467842471313
  (942, 6)	3.884553575360888
  (942, 5)	3.965705242845835
  (942, 4)	3.443352325388904
  (942, 3)	3.5030130866454017
  (942, 2)	2.9338768049543824
  (942, 1)	3.0873593432106046
  (942, 0)	3.955471131407501
this is the 282 epoch
rmse loss on training set is 0.9458664454578379
rmse loss on test set is 0.976386777708645
for this epoch using 111.48033499717712 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9838953229300196
  (0, 1680)	3.000000000000002
  (0, 1679)	2.910569658104175
  (0, 1678)	2.9938095555855915
  (0, 1677)	2.827329760622765
  (0, 1676)	2.9995186591853438
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.933630543130206
  (0, 1671)	2.9264313113551728
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9484354886400492
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9484354886400492
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8473778241870713
  (0, 1662)	2.9484354886400492
  (0, 1661)	2.936518335464585
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0242339150064743
  :	:
  (942, 24)	3.8588399660843584
  (942, 23)	3.5759287122717027
  (942, 22)	4.351070389267293
  (942, 21)	4.364349185329027
  (942, 20)	2.8097491076071606
  (942, 19)	3.6066118824779294
  (942, 18)	4.254620194583754
  (942, 17)	3.0435307185759197
  (942, 16)	3.4404919318339107
  (942, 15)	3.5833003377350354
  (942, 14)	3.9162823808713614
  (942, 13)	4.162952340507338
  (942, 12)	3.5159606195554356
  (942, 11)	4.501773355176879
  (942, 10)	3.920825309504922
  (942, 9)	3.8872847066500116
  (942, 8)	4.0608035476004725
  (942, 7)	3.968662093905428
  (942, 6)	3.8845154774461
  (942, 5)	3.966957581774945
  (942, 4)	3.443418335596515
  (942, 3)	3.5032465955211083
  (942, 2)	2.933497371794929
  (942, 1)	3.087027949405185
  (942, 0)	3.955463992079294
this is the 283 epoch
rmse loss on training set is 0.9458164780808409
rmse loss on test set is 0.9763661141282397
for this epoch using 111.59668326377869 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9838476413353745
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9102597328525652
  (0, 1678)	2.9937824747039414
  (0, 1677)	2.826736991001185
  (0, 1676)	2.999530455320783
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.933387306716914
  (0, 1671)	2.9262118842989366
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.948265302476828
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.948265302476828
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8468789875855975
  (0, 1662)	2.948265302476828
  (0, 1661)	2.9363286537778497
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0243345293361017
  :	:
  (942, 24)	3.8587882899291377
  (942, 23)	3.5757963578873624
  (942, 22)	4.351448719716039
  (942, 21)	4.364612529010421
  (942, 20)	2.809301547489773
  (942, 19)	3.6068962563994558
  (942, 18)	4.25516742531335
  (942, 17)	3.0426553975475774
  (942, 16)	3.4405923459176924
  (942, 15)	3.583724093984048
  (942, 14)	3.9161967905571196
  (942, 13)	4.163008095485848
  (942, 12)	3.5159959860841923
  (942, 11)	4.502012969434584
  (942, 10)	3.920970216121117
  (942, 9)	3.8875561200822197
  (942, 8)	4.060765664124126
  (942, 7)	3.9688553949601193
  (942, 6)	3.884477271424985
  (942, 5)	3.9682048561104453
  (942, 4)	3.4434836089620933
  (942, 3)	3.5034789833228106
  (942, 2)	2.9331205288601523
  (942, 1)	3.0866983015204466
  (942, 0)	3.9554565912088107
this is the 284 epoch
rmse loss on training set is 0.9457668370672317
rmse loss on test set is 0.9763456712384723
for this epoch using 108.79666233062744 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983800015575647
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9099498485229804
  (0, 1678)	2.9937553564754347
  (0, 1677)	2.8261443405705333
  (0, 1676)	2.9995422749299174
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.933144283779138
  (0, 1671)	2.925992620275834
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9480951265328477
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9480951265328477
  (0, 1664)	3.000000000000002
  (0, 1663)	2.846380306547824
  (0, 1662)	2.9480951265328477
  (0, 1661)	2.936139125156843
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0244350696326565
  :	:
  (942, 24)	3.8587365894169126
  (942, 23)	3.5756642026888
  (942, 22)	4.351824920188014
  (942, 21)	4.364874299641982
  (942, 20)	2.808857583352647
  (942, 19)	3.607178315741382
  (942, 18)	4.2557094249408225
  (942, 17)	3.0417807322384856
  (942, 16)	3.440692117829886
  (942, 15)	3.5841456127211626
  (942, 14)	3.916111452490906
  (942, 13)	4.1630633693513035
  (942, 12)	3.5160308319336315
  (942, 11)	4.502251354861599
  (942, 10)	3.9211142384234012
  (942, 9)	3.887824507391997
  (942, 8)	4.060727616340375
  (942, 7)	3.969047753933254
  (942, 6)	3.884438963366897
  (942, 5)	3.9694470924145397
  (942, 4)	3.443548156123005
  (942, 3)	3.5037102573984344
  (942, 2)	2.9327462595274474
  (942, 1)	3.086370394304022
  (942, 0)	3.955448935722217
this is the 285 epoch
rmse loss on training set is 0.9457175192455168
rmse loss on test set is 0.9763254465877736
for this epoch using 108.69465327262878 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983752445449683
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9096400051341544
  (0, 1678)	2.9937282009229556
  (0, 1677)	2.8255518093453467
  (0, 1676)	2.999554117854958
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.932901473939875
  (0, 1671)	2.9257735186237537
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9479249608346465
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9479249608346465
  (0, 1664)	3.000000000000002
  (0, 1663)	2.845881780572213
  (0, 1662)	2.9479249608346465
  (0, 1661)	2.9359497490890614
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.024535535654797
  :	:
  (942, 24)	3.858684869247696
  (942, 23)	3.5755322548469413
  (942, 22)	4.352199012144083
  (942, 21)	4.365134512951869
  (942, 20)	2.8084171869605137
  (942, 19)	3.6074580833754752
  (942, 18)	4.256246250311
  (942, 17)	3.0409067284159503
  (942, 16)	3.440791256477476
  (942, 15)	3.584564909821647
  (942, 14)	3.916026368982807
  (942, 13)	4.163118171691816
  (942, 12)	3.5160651645440297
  (942, 11)	4.502488523152475
  (942, 10)	3.9212573884380877
  (942, 9)	3.8880899056070115
  (942, 8)	4.060689410801379
  (942, 7)	3.9692391789888597
  (942, 6)	3.884400559205109
  (942, 5)	3.9706843170216763
  (942, 4)	3.443611987536421
  (942, 3)	3.503940425011322
  (942, 2)	2.9323745472391134
  (942, 1)	3.0860442224425033
  (942, 0)	3.955441032409634
this is the 286 epoch
rmse loss on training set is 0.9456685214849543
rmse loss on test set is 0.9763054377573441
for this epoch using 109.73799204826355 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983704930758257
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9093302027060304
  (0, 1678)	2.9937010080703725
  (0, 1677)	2.824959397341688
  (0, 1676)	2.9995659839398168
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9326588768200845
  (0, 1671)	2.9255545786864174
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.947754805408992
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.947754805408992
  (0, 1664)	3.000000000000002
  (0, 1663)	2.845383409163465
  (0, 1662)	2.947754805408992
  (0, 1661)	2.935760525065262
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.024635927165931
  :	:
  (942, 24)	3.8586331340141724
  (942, 23)	3.575400522293172
  (942, 22)	4.352571016690016
  (942, 21)	4.365393184437933
  (942, 20)	2.807980330259818
  (942, 19)	3.6077355819105823
  (942, 18)	4.2567779575888265
  (942, 17)	3.0400333917156557
  (942, 16)	3.440889770608479
  (942, 15)	3.584982000985136
  (942, 14)	3.9159415422561796
  (942, 13)	4.163172511879122
  (942, 12)	3.5160989912347014
  (942, 11)	4.5027244857995115
  (942, 10)	3.9213996779522637
  (942, 9)	3.8883523512642584
  (942, 8)	4.060651053926103
  (942, 7)	3.9694296781601186
  (942, 6)	3.8843620647394546
  (942, 5)	3.971916556041548
  (942, 4)	3.44367511348256
  (942, 3)	3.504169493341864
  (942, 2)	2.9320053755031252
  (942, 1)	3.0857197805635113
  (942, 0)	3.955432887927418
this is the 287 epoch
rmse loss on training set is 0.9456198406948674
rmse loss on test set is 0.976285642360661
for this epoch using 112.94152736663818 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9836574713040225
  (0, 1680)	3.000000000000002
  (0, 1679)	2.909020441259808
  (0, 1678)	2.993673777942568
  (0, 1677)	2.8243671045770524
  (0, 1676)	2.9995778730300464
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9324164920387994
  (0, 1671)	2.9253357998132725
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9475846602829034
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9475846602829034
  (0, 1664)	3.000000000000002
  (0, 1663)	2.844885191832442
  (0, 1662)	2.9475846602829034
  (0, 1661)	2.9355714525794183
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0247362439341345
  :	:
  (942, 24)	3.8585813882038247
  (942, 23)	3.5752690127248408
  (942, 22)	4.352940954583964
  (942, 21)	4.365650329371792
  (942, 20)	2.807546985378549
  (942, 19)	3.6080108336961465
  (942, 18)	4.257304602268347
  (942, 17)	3.0391607276442074
  (942, 16)	3.440987668814848
  (942, 15)	3.585396901738331
  (942, 14)	3.9158569744495555
  (942, 13)	4.163226399073522
  (942, 12)	3.516132319206205
  (942, 11)	4.502959254097519
  (942, 10)	3.9215411185190607
  (942, 9)	3.888611880417109
  (942, 8)	4.060612552002875
  (942, 7)	3.969619259352358
  (942, 6)	3.884323485638918
  (942, 5)	3.9731438353622073
  (942, 4)	3.4437375440679867
  (942, 3)	3.5043974694890303
  (942, 2)	2.9316387278938834
  (942, 1)	3.085397063237694
  (942, 0)	3.955424508800659
this is the 288 epoch
rmse loss on training set is 0.9455714738240343
rmse loss on test set is 0.9762660580429328
for this epoch using 112.11364412307739 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983610066891475
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9087107208178637
  (0, 1678)	2.993646510565351
  (0, 1677)	2.8237749310703752
  (0, 1676)	2.999589784972849
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9321743192131993
  (0, 1671)	2.9251171813594605
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9474145254836155
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9474145254836155
  (0, 1664)	3.000000000000002
  (0, 1663)	2.844387128096115
  (0, 1662)	2.9474145254836155
  (0, 1661)	2.93538253112872
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0248364857320573
  :	:
  (942, 24)	3.8585296362009243
  (942, 23)	3.5751377336104677
  (942, 22)	4.353308846243873
  (942, 21)	4.365905962802951
  (942, 20)	2.807117124626018
  (942, 19)	3.6082838608256345
  (942, 18)	4.257826239181622
  (942, 17)	3.038288741581713
  (942, 16)	3.4410849595354125
  (942, 15)	3.5858096274377353
  (942, 14)	3.915772667618579
  (942, 13)	4.163279842228844
  (942, 12)	3.516165155542523
  (942, 11)	4.503192839148524
  (942, 10)	3.9216817214627495
  (942, 9)	3.888868528642253
  (942, 8)	4.060573911191965
  (942, 7)	3.9698079303459797
  (942, 6)	3.884284827444096
  (942, 5)	3.974366180652984
  (942, 4)	3.443799289228711
  (942, 3)	3.504624360472014
  (942, 2)	2.931274588052942
  (942, 1)	3.085076064980719
  (942, 0)	3.9554159014253676
this is the 289 epoch
rmse loss on training set is 0.9455234178600377
rmse loss on test set is 0.9762466824805964
for this epoch using 112.47041392326355 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983562717326958
  (0, 1680)	3.000000000000002
  (0, 1679)	2.90840104140375
  (0, 1678)	2.9936192059655045
  (0, 1677)	2.823182876841986
  (0, 1676)	2.999601719616996
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9319323579587215
  (0, 1671)	2.9248987226857257
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.94724440103859
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.94724440103859
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8438892174774626
  (0, 1662)	2.94724440103859
  (0, 1661)	2.935193760213515
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.024936652336853
  :	:
  (942, 24)	3.8584778822885513
  (942, 23)	3.57500669219506
  (942, 22)	4.3536747117545875
  (942, 21)	4.36616009956274
  (942, 20)	2.8066907204926435
  (942, 19)	3.6085546851399415
  (942, 18)	4.258342922507373
  (942, 17)	3.0374174387842308
  (942, 16)	3.441181651058694
  (942, 15)	3.586220193272277
  (942, 14)	3.9156886237378847
  (942, 13)	4.163332850097222
  (942, 12)	3.516197507213192
  (942, 11)	4.503425251866262
  (942, 10)	3.9218214978837813
  (942, 9)	3.8891223310466345
  (942, 8)	4.060535137528022
  (942, 7)	3.969995698799184
  (942, 6)	3.88424609556976
  (942, 5)	3.975583617367424
  (942, 4)	3.4438603587333865
  (942, 3)	3.5048501732317168
  (942, 2)	2.9309129396896623
  (942, 1)	3.0847567802552
  (942, 0)	3.95540707207092
this is the 290 epoch
rmse loss on training set is 0.9454756698286532
rmse loss on test set is 0.9762275133808299
for this epoch using 107.5360426902771 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983515422418577
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9080914030421847
  (0, 1678)	2.993591864170743
  (0, 1677)	2.822590941913624
  (0, 1676)	2.9996136768128716
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9316906078891374
  (0, 1671)	2.924680423158365
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9470742869755155
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9470742869755155
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8433914595054284
  (0, 1662)	2.9470742869755155
  (0, 1661)	2.935005139337348
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025036743530118
  :	:
  (942, 24)	3.858426130650555
  (942, 23)	3.5748758955051314
  (942, 22)	4.354038570874891
  (942, 21)	4.366412754268188
  (942, 20)	2.8062677456496727
  (942, 19)	3.608823328230709
  (942, 18)	4.258854705779668
  (942, 17)	3.036546824386231
  (942, 16)	3.441277751525689
  (942, 15)	3.586628614265862
  (942, 14)	3.9156048447028855
  (942, 13)	4.163385431233737
  (942, 12)	3.51622938107537
  (942, 11)	4.503656502980608
  (942, 10)	3.9219604586636363
  (942, 9)	3.8893733222741416
  (942, 8)	4.060496236922556
  (942, 7)	3.970182572250774
  (942, 6)	3.8842072953072084
  (942, 5)	3.976796170746199
  (942, 4)	3.4439207621862984
  (942, 3)	3.5050749146322095
  (942, 2)	2.9305537665818897
  (942, 1)	3.0844392034725834
  (942, 0)	3.9553980268822464
this is the 291 epoch
rmse loss on training set is 0.9454282267932588
rmse loss on test set is 0.9762085484810425
for this epoch using 108.20043182373047 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983468181976236
  (0, 1680)	3.000000000000002
  (0, 1679)	2.907781805759028
  (0, 1678)	2.9935644852096894
  (0, 1677)	2.8219991263083593
  (0, 1676)	2.9996256564123653
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.931449068616671
  (0, 1671)	2.9244622821491535
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9469041833222915
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9469041833222915
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8428938537148345
  (0, 1662)	2.9469041833222915
  (0, 1661)	2.9348166680069028
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025136759097789
  :	:
  (942, 24)	3.8583743853734465
  (942, 23)	3.5747453503536923
  (942, 22)	4.354400443044309
  (942, 21)	4.366663941325814
  (942, 20)	2.805848172948903
  (942, 19)	3.609089811443627
  (942, 18)	4.259361641896351
  (942, 17)	3.035676903402919
  (942, 16)	3.441373268932592
  (942, 15)	3.587034905279983
  (942, 14)	3.9155213323315685
  (942, 13)	4.163437594001017
  (942, 12)	3.5162607838759192
  (942, 11)	4.503886603041838
  (942, 10)	3.9220986144696317
  (942, 9)	3.889621536512243
  (942, 8)	4.060457215166257
  (942, 7)	3.970368558122823
  (942, 6)	3.8841684318267244
  (942, 5)	3.978003865819896
  (942, 4)	3.443980509030381
  (942, 3)	3.50529859146222
  (942, 2)	2.930197052576557
  (942, 1)	3.084123328994975
  (942, 0)	3.9553887718820175
this is the 292 epoch
rmse loss on training set is 0.9453810858542266
rmse loss on test set is 0.9761897855484256
for this epoch using 112.54119920730591 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9834209958115636
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9074722495812337
  (0, 1678)	2.9935370691118806
  (0, 1677)	2.821407430050587
  (0, 1676)	2.9996376582689224
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.93120773975206
  (0, 1671)	2.924244299035298
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9467340901070376
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9467340901070376
  (0, 1664)	3.000000000000002
  (0, 1663)	2.842396399646318
  (0, 1662)	2.9467340901070376
  (0, 1661)	2.9346283457320084
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.02523669883008
  :	:
  (942, 24)	3.858322650448259
  (942, 23)	3.5746150633451528
  (942, 22)	4.354760347389775
  (942, 21)	4.366913674935383
  (942, 20)	2.8054319754223407
  (942, 19)	3.6093541558816393
  (942, 18)	4.259863783127389
  (942, 17)	3.0348076807326376
  (942, 16)	3.4414682111335075
  (942, 15)	3.5874390810161363
  (942, 14)	3.9154380883662516
  (942, 13)	4.1634893465736065
  (942, 12)	3.5162917222533445
  (942, 11)	4.504115562424805
  (942, 10)	3.9222359757595595
  (942, 9)	3.889867007498543
  (942, 8)	4.060418077931337
  (942, 7)	3.970553663723185
  (942, 6)	3.884129510179847
  (942, 5)	3.979206727411805
  (942, 4)	3.4440396085501295
  (942, 3)	3.5055212104365236
  (942, 2)	2.9298427815902977
  (942, 1)	3.083809151136973
  (942, 0)	3.9553793129729
this is the 293 epoch
rmse loss on training set is 0.9453342441483479
rmse loss on test set is 0.9761712223794453
for this epoch using 113.15442824363708 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9833738637378877
  (0, 1680)	3.000000000000002
  (0, 1679)	2.907162734536883
  (0, 1678)	2.9935096159077355
  (0, 1677)	2.8208158531660272
  (0, 1676)	2.9996496822374636
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9309666209046386
  (0, 1671)	2.9240264731993446
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.94656400735806
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.94656400735806
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8418990968462596
  (0, 1662)	2.94656400735806
  (0, 1661)	2.934440172025591
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025336562521425
  :	:
  (942, 24)	3.858270929772428
  (942, 23)	3.574485040880021
  (942, 22)	4.35511830273218
  (942, 21)	4.3671619690935195
  (942, 20)	2.805019126281897
  (942, 19)	3.6096163824081757
  (942, 18)	4.260361181123116
  (942, 17)	3.0339391611590996
  (942, 16)	3.441562585843011
  (942, 15)	3.587841156018356
  (942, 14)	3.9153551144752927
  (942, 13)	4.163540696942376
  (942, 12)	3.5163222027398295
  (942, 11)	4.504343391333013
  (942, 10)	3.9223725527862894
  (942, 9)	3.8901097685272377
  (942, 8)	4.060378830773813
  (942, 7)	3.970737896248094
  (942, 6)	3.8840905353017656
  (942, 5)	3.9804047801406575
  (942, 4)	3.4440980698745065
  (942, 3)	3.505742778197329
  (942, 2)	2.9294909376099914
  (942, 1)	3.083496664167391
  (942, 0)	3.955369655939679
this is the 294 epoch
rmse loss on training set is 0.9452876988482539
rmse loss on test set is 0.9761528567994121
for this epoch using 110.51384377479553 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.9833267855702448
  (0, 1680)	3.000000000000002
  (0, 1679)	2.906853260655103
  (0, 1678)	2.993482125628533
  (0, 1677)	2.820224395681679
  (0, 1676)	2.999661728174371
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.930725711682432
  (0, 1671)	2.9238088040291603
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9463939351038957
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9463939351038957
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8414019448667056
  (0, 1662)	2.9463939351038957
  (0, 1661)	2.9342521464036824
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025436349970389
  :	:
  (942, 24)	3.8582192271515923
  (942, 23)	3.574355289159636
  (942, 22)	4.355474327592692
  (942, 21)	4.3674088375972575
  (942, 20)	2.804609598918994
  (942, 19)	3.6098765116502345
  (942, 18)	4.260853886922328
  (942, 17)	3.033071349353646
  (942, 16)	3.4416564006388044
  (942, 15)	3.588241144675535
  (942, 14)	3.9152724122547724
  (942, 13)	4.163591652918685
  (942, 12)	3.516352231763074
  (942, 11)	4.504570099802522
  (942, 10)	3.9225083556021496
  (942, 9)	3.8903498524553766
  (942, 8)	4.0603394791357275
  (942, 7)	3.9709212627846004
  (942, 6)	3.884051512013481
  (942, 5)	3.981598048423347
  (942, 4)	3.444155901979746
  (942, 3)	3.5059633013156226
  (942, 2)	2.929141504693324
  (942, 1)	3.0831858623110286
  (942, 0)	3.9553598064513404
this is the 295 epoch
rmse loss on training set is 0.9452414471618794
rmse loss on test set is 0.9761346866620139
for this epoch using 108.50251698493958 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983279761125307
  (0, 1680)	3.000000000000002
  (0, 1679)	2.906543827966105
  (0, 1678)	2.9934545983064282
  (0, 1677)	2.819633057625775
  (0, 1676)	2.999673795937507
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.930485011692248
  (0, 1671)	2.9235912909178228
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9462238733732646
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9462238733732646
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8409049432653273
  (0, 1662)	2.9462238733732646
  (0, 1661)	2.934064268385406
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025536060979595
  :	:
  (942, 24)	3.8581675463013276
  (942, 23)	3.574225814190689
  (942, 22)	4.35582844019902
  (942, 21)	4.367654294047617
  (942, 20)	2.8042033669041975
  (942, 19)	3.61013456400152
  (942, 18)	4.2613419509602855
  (942, 17)	3.0322042498774664
  (942, 16)	3.4417496629641726
  (942, 15)	3.5886390612238466
  (942, 14)	3.9151899832301327
  (942, 13)	4.163642222138558
  (942, 12)	3.516381815648239
  (942, 11)	4.504795697705807
  (942, 10)	3.9226433940633587
  (942, 9)	3.890587291709174
  (942, 8)	4.060300028347291
  (942, 7)	3.971103770312939
  (942, 6)	3.8840124450241262
  (942, 5)	3.9827865564774814
  (942, 4)	3.4442131136921246
  (942, 3)	3.506182786292483
  (942, 2)	2.928794466969296
  (942, 1)	3.082876739750318
  (942, 0)	3.9553497700631763
this is the 296 epoch
rmse loss on training set is 0.9451954863318781
rmse loss on test set is 0.9761167098488606
for this epoch using 109.17198014259338 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983232790221391
  (0, 1680)	3.000000000000002
  (0, 1679)	2.906234436501109
  (0, 1678)	2.993427033974407
  (0, 1677)	2.819041839027812
  (0, 1676)	2.9996858853861204
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9302445205397225
  (0, 1671)	2.923373933263605
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9460538221950885
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9460538221950885
  (0, 1664)	3.000000000000002
  (0, 1663)	2.840408091605329
  (0, 1662)	2.9460538221950885
  (0, 1661)	2.933876537492932
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025635695355677
  :	:
  (942, 24)	3.85811589084894
  (942, 23)	3.574096621789669
  (942, 22)	4.356180658491433
  (942, 21)	4.367898351852922
  (942, 20)	2.8038004039868
  (942, 19)	3.6103905596254218
  (942, 18)	4.261825423076553
  (942, 17)	3.031337867183688
  (942, 16)	3.4418423801304936
  (942, 15)	3.5890349197490408
  (942, 14)	3.9151078288578227
  (942, 13)	4.1636924120666485
  (942, 12)	3.5164109606197442
  (942, 11)	4.505020194755492
  (942, 10)	3.9227776778342203
  (942, 9)	3.8908221182901266
  (942, 8)	4.060260483629047
  (942, 7)	3.9712854257088646
  (942, 6)	3.883973338933067
  (942, 5)	3.983970328324089
  (942, 4)	3.4442697136906792
  (942, 3)	3.506401239560359
  (942, 2)	2.9284498086386868
  (942, 1)	3.082569290626995
  (942, 0)	3.955339552218829
this is the 297 epoch
rmse loss on training set is 0.9451498136351197
rmse loss on test set is 0.9760989242690848
for this epoch using 114.67621397972107 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983185872678427
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9059250862923784
  (0, 1678)	2.9933994326662914
  (0, 1677)	2.8184507399184717
  (0, 1676)	2.9996979963808736
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9300042378294235
  (0, 1671)	2.9231567304698762
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.9458837815984853
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.9458837815984853
  (0, 1664)	3.000000000000002
  (0, 1663)	2.8399113894554
  (0, 1662)	2.9458837815984853
  (0, 1661)	2.933688953251486
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.02573525290919
  :	:
  (942, 24)	3.858064264335109
  (942, 23)	3.5739677175872733
  (942, 22)	4.356531000128699
  (942, 21)	4.368141024232238
  (942, 20)	2.8034006840943686
  (942, 19)	3.61064451845809
  (942, 18)	4.26230435252277
  (942, 17)	3.0304722056195614
  (942, 16)	3.441934559319664
  (942, 15)	3.589428734188703
  (942, 14)	3.9150259505268137
  (942, 13)	4.163742230000223
  (942, 12)	3.5164396728031018
  (942, 11)	4.505243600507974
  (942, 10)	3.922911216391299
  (942, 9)	3.8910543637809925
  (942, 8)	4.060220850093964
  (942, 7)	3.9714662357459183
  (942, 6)	3.8839341982320796
  (942, 5)	3.985149387790071
  (942, 4)	3.444325710509861
  (942, 3)	3.5066186674843243
  (942, 2)	2.928107513974543
  (942, 1)	3.082263509043676
  (942, 0)	3.955329158252298
this is the 298 epoch
rmse loss on training set is 0.9451044263821203
rmse loss on test set is 0.9760813278588668
for this epoch using 112.86882066726685 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
  (0, 1681)	2.983139008317902
  (0, 1680)	3.000000000000002
  (0, 1679)	2.9056157773731677
  (0, 1678)	2.993371794416699
  (0, 1677)	2.8178597603296436
  (0, 1676)	2.999710128783815
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.929764163164898
  (0, 1671)	2.9229396819450653
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.94571375161276
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.94571375161276
  (0, 1664)	3.000000000000002
  (0, 1663)	2.839414836389623
  (0, 1662)	2.94571375161276
  (0, 1661)	2.9335015151893113
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.025834733454567
  :	:
  (942, 24)	3.8580126702156394
  (942, 23)	3.573839107032645
  (942, 22)	4.356879482493924
  (942, 21)	4.368382324218612
  (942, 20)	2.803004181332325
  (942, 19)	3.6108964602113054
  (942, 18)	4.262778787970321
  (942, 17)	3.029607269428469
  (942, 16)	3.4420262075864843
  (942, 15)	3.5898205183345246
  (942, 14)	3.914944349560191
  (942, 13)	4.163791683072947
  (942, 12)	3.5164679582266496
  (942, 11)	4.505465924366985
  (942, 10)	3.923044019027477
  (942, 9)	3.891284059351841
  (942, 8)	4.060181132749423
  (942, 7)	3.97164620709765
  (942, 6)	3.8838950273074615
  (942, 5)	3.9863237585108076
  (942, 4)	3.4443811125421515
  (942, 3)	3.506835076363333
  (942, 2)	2.9277675673226273
  (942, 1)	3.081959389065482
  (942, 0)	3.955318593389916
this is the 299 epoch
rmse loss on training set is 0.9450593219165745
rmse loss on test set is 0.976063918581047
for this epoch using 111.54225182533264 seconds
P updating finished 12.5%!
P updating finished 25.0%!
P updating finished 37.5%!
Q updating finished 12.5%!
Q updating finished 25.0%!
Q updating finished 37.5%!
/root/NMF_sparse/plt.py:7: UserWarning: Creating legend with loc="best" can be slow with large amounts of data.
  plt.savefig(name)
  (0, 1681)	2.983092196962883
  (0, 1680)	3.000000000000002
  (0, 1679)	2.905306509777716
  (0, 1678)	2.993344119261064
  (0, 1677)	2.817268900294367
  (0, 1676)	2.9997222824583263
  (0, 1675)	3.000000000000002
  (0, 1674)	3.000000000000002
  (0, 1673)	3.000000000000002
  (0, 1672)	2.9295242961487724
  (0, 1671)	2.9227227871026167
  (0, 1670)	3.000000000000002
  (0, 1669)	3.000000000000002
  (0, 1668)	2.945543732267393
  (0, 1667)	3.000000000000002
  (0, 1666)	3.000000000000002
  (0, 1665)	2.945543732267393
  (0, 1664)	3.000000000000002
  (0, 1663)	2.838918431987441
  (0, 1662)	2.945543732267393
  (0, 1661)	2.9333142228376614
  (0, 1660)	3.000000000000002
  (0, 1659)	3.000000000000002
  (0, 1658)	3.000000000000002
  (0, 1657)	3.0259341368100263
  :	:
  (942, 24)	3.8579611118630077
  (942, 23)	3.573710795397544
  (942, 22)	4.357226122700133
  (942, 21)	4.368622264662308
  (942, 20)	2.802610869983472
  (942, 19)	3.61114640437543
  (942, 18)	4.26324877751782
  (942, 17)	3.02874306275196
  (942, 16)	3.4421173318609894
  (942, 15)	3.5902102858344835
  (942, 14)	3.914863027216664
  (942, 13)	4.163840778258626
  (942, 12)	3.516495822823334
  (942, 11)	4.505687175586977
  (942, 10)	3.9231760948559247
  (942, 9)	3.891511235765831
  (942, 8)	4.060141336499277
  (942, 7)	3.9718253463397355
  (942, 6)	3.883855830442062
  (942, 5)	3.98749346393255
  (942, 4)	3.444435928040624
  (942, 3)	3.5070504724313327
  (942, 2)	2.9274299531017953
  (942, 1)	3.081656924721539
  (942, 0)	3.955307862752262
this is the 300 epoch
rmse loss on training set is 0.9450144976148039
rmse loss on test set is 0.9760466944246974
for this epoch using 112.55325031280518 seconds
